(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[974],{3169:(e,n,r)=>{"use strict";r.d(n,{default:()=>_});var o=r(5155),a=r(2115);let t={name:"Obstacle Course",level:3,rewardCode:"\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer\nfrom ...objects import get_object_volume\n\nimport torch\n\ndef main_obstacle_course_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for obstacle_course.\n\n    This reward is the negative distance between the robot pelvis (hip) x and y coordinates \n    and the block x and y coordinates.\n    '''\n    robot = env.scene[\"robot\"]\n    try:\n        block = env.scene['Object5']\n\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n        pelvis_pos_xy = pelvis_pos[:, :2]  # x and y components\n\n        block_pos_xy = block.data.root_pos_w[:, :2]  # x and y components\n\n        # Calculate Euclidean distance between pelvis and block in x-y plane\n        distance_xy = torch.norm(pelvis_pos_xy - block_pos_xy, dim=1)\n        \n        # Negative distance as reward\n        reward = -distance_xy\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device)\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_jump_over_low_wall(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"jump_low_wall_reward\") -> torch.Tensor:\n    '''Shaping reward for jumping over the low wall.\n    Encourages the robot to increase its pelvis height when approaching the low wall and to be near the low wall in x direction.\n    '''\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # CORRECT: Accessing object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Accessing robot part index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing robot part position using approved pattern\n        pelvis_pos_x = pelvis_pos[:, 0] # CORRECT: Accessing x component of pelvis position\n        pelvis_pos_z = pelvis_pos[:, 2] # CORRECT: Accessing z component of pelvis position\n\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0] # CORRECT: Accessing low wall x position using approved pattern\n        low_wall_pos_z_top = low_wall.data.root_pos_w[:, 2] + 0.4 # top of low wall, using object config size (0.4m height)\n\n        # Activation condition: Robot is approaching the low wall but not yet past it\n        activation_condition = (pelvis_pos_x < low_wall_pos_x + 1.5) & (pelvis_pos_x > low_wall_pos_x - 1.5) # CORRECT: Relative x distance activation\n\n        # Reward for increasing pelvis height above the low wall height\n        pelvis_height_reward = -torch.abs(torch.relu(low_wall_pos_z_top + 0.5 - pelvis_pos_z)) # reward when pelvis is 0.5m above wall, relative z distance\n\n        reward = torch.where(activation_condition, pelvis_height_reward, torch.tensor(0.0, device=env.device)) # CORRECT: Apply reward only when activated\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # CORRECT: Normalize reward\n        RewNormalizer.update_stats(normaliser_name, reward) # CORRECT: Update reward stats\n        return scaled_reward\n    return reward\n\ndef shaping_reward_push_large_sphere(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"push_sphere_reward\") -> torch.Tensor:\n    '''Shaping reward for pushing the large sphere towards the high wall.\n    Negative x distance between the sphere and the wall, only active when the wall hasn't fallen (wall z > 0.3).\n    '''\n    robot = env.scene[\"robot\"]\n    try:\n        large_sphere = env.scene['Object1']\n        high_wall = env.scene['Object4']\n        robot_pelvis = env.scene['robot'].body_names.index('pelvis')\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis]\n\n        ideal_pelvis_x = 0.6\n\n\n        large_sphere_pos = large_sphere.data.root_pos_w\n        large_sphere_pos_x = large_sphere_pos[:, 0]\n        large_sphere_pos_y = large_sphere_pos[:, 1]\n        high_wall_pos = high_wall.data.root_pos_w\n        high_wall_pos_x = high_wall_pos[:, 0]\n        high_wall_pos_z = high_wall_pos[:, 2]\n\n        pelvis_x_reward = -torch.abs(robot_pelvis_pos[:, 0] - large_sphere_pos_x - ideal_pelvis_x)\n        pelvis_y_reward = -torch.abs(robot_pelvis_pos[:, 1] - large_sphere_pos_y)\n\n        # Calculate x distance between sphere and wall\n        x_distance = torch.abs(high_wall_pos_x - large_sphere_pos_x)\n        \n        # Negative distance as reward\n        distance_reward = -x_distance\n        pelvis_shape_reward = pelvis_x_reward + pelvis_y_reward\n        \n        # Activation condition: Wall hasn't fallen (z > 0.3)\n        activation_condition = high_wall_pos_z > 0.3\n        \n        reward = torch.where(activation_condition, distance_reward + 0.5*pelvis_shape_reward, torch.tensor(0.0, device=env.device))\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device)\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_kick_small_sphere(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"kick_sphere_reward\") -> torch.Tensor:\n    '''Shaping reward for kicking the small sphere towards the block.\n    Positive x,y distance between the small sphere and the block.\n    '''\n    robot = env.scene[\"robot\"]\n    try:\n        small_sphere = env.scene['Object2']\n        block = env.scene['Object5']\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n        pelvis_pos_xy = pelvis_pos[:, :2]  # x and y components\n\n        small_sphere_pos = small_sphere.data.root_pos_w\n        small_sphere_pos_xy = small_sphere_pos[:, :2]  # x and y components\n        \n        block_pos = block.data.root_pos_w\n        block_pos_xy = block_pos[:, :2]  # x and y components\n\n        approach_sphere_reward = -torch.norm(small_sphere_pos_xy - pelvis_pos_xy, dim=1)\n        \n        # Calculate Euclidean distance between small sphere and block in x-y plane\n        distance_xy = torch.norm(small_sphere_pos_xy - block_pos_xy, dim=1)\n        \n        # Positive distance as reward\n        reward = distance_xy #+ approach_sphere_reward\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device)\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_jump_on_block(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"jump_block_reward\") -> torch.Tensor:\n    '''Shaping reward for jumping onto the block and staying stable.\n    Encourages the robot to position its feet above the block and maintain a stable pelvis height on top of the block.\n    '''\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot using approved pattern\n    try:\n        block = env.scene['Object5'] # CORRECT: Accessing object using approved pattern and try/except\n\n        left_ankle_roll_link_idx = robot.body_names.index('left_ankle_roll_link') # CORRECT: Accessing robot part index using approved pattern\n        right_ankle_roll_link_idx = robot.body_names.index('right_ankle_roll_link') # CORRECT: Accessing robot part index using approved pattern\n        left_ankle_roll_link_pos = robot.data.body_pos_w[:, left_ankle_roll_link_idx] # CORRECT: Accessing robot part position using approved pattern\n        right_ankle_roll_link_pos = robot.data.body_pos_w[:, right_ankle_roll_link_idx] # CORRECT: Accessing robot part position using approved pattern\n        left_ankle_roll_link_pos_z = left_ankle_roll_link_pos[:, 2] # CORRECT: Accessing z component of feet position\n        right_ankle_roll_link_pos_z = right_ankle_roll_link_pos[:, 2] # CORRECT: Accessing z component of feet position\n\n        pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Accessing robot part index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing robot part position using approved pattern\n        pelvis_pos_x = pelvis_pos[:, 0] # CORRECT: Accessing x component of pelvis position\n        pelvis_pos_z = pelvis_pos[:, 2] # CORRECT: Accessing z component of pelvis position\n\n        block_pos_x = block.data.root_pos_w[:, 0] # CORRECT: Accessing block x position using approved pattern\n        block_pos_z_top = block.data.root_pos_w[:, 2] + 0.5 # top of block, using object config size (0.5m height)\n\n        # Activation condition: Robot is near the block in x direction\n        activation_condition = (pelvis_pos_x > block_pos_x - 2.0) & (pelvis_pos_x < block_pos_x + 2.0) # CORRECT: Relative x distance activation\n\n        # Reward for feet being above the block\n        feet_height_reward = -torch.abs(left_ankle_roll_link_pos_z - block_pos_z_top) - torch.abs(right_ankle_roll_link_pos_z - block_pos_z_top)\n\n        # Reward for pelvis being at a stable height above the block\n        pelvis_stable_height_reward = -torch.abs(block_pos_z_top + 0.7 - pelvis_pos_z) # reward when pelvis is 0.7m above block, relative z distance\n\n        reward = torch.where(activation_condition, feet_height_reward + pelvis_stable_height_reward, -torch.ones(env.num_envs, device=env.device)) # CORRECT: Apply reward only when activated\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # CORRECT: Normalize reward\n        RewNormalizer.update_stats(normaliser_name, reward) # CORRECT: Update reward stats\n        return scaled_reward\n    return reward\n\ndef shaping_reward_celebrate_on_block(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"celebrate_reward\") -> torch.Tensor:\n    '''Shaping reward for celebrating on the block by varying pelvis height.\n    Encourages vertical movement of the pelvis while on the block.\n    '''\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot using approved pattern\n    try:\n        block = env.scene['Object5'] # CORRECT: Accessing object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Accessing robot part index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing robot part position using approved pattern\n        pelvis_pos_x = pelvis_pos[:, 0] # CORRECT: Accessing x component of pelvis position\n        pelvis_pos_z = pelvis_pos[:, 2] # CORRECT: Accessing z component of pelvis position\n\n        block_pos_x = block.data.root_pos_w[:, 0] # CORRECT: Accessing block x position using approved pattern\n        block_pos_z_top = block.data.root_pos_w[:, 2] + 0.5 # top of block, using object config size (0.5m height)\n\n\n        # Activation condition: Robot is on the block in x direction\n        activation_condition = (pelvis_pos_x > block_pos_x - 0.5) & (pelvis_pos_x < block_pos_x + 0.5) # CORRECT: Relative x distance activation\n\n        # Reward for varying pelvis height (jumping up and down) - using absolute deviation from a target height to encourage movement around it.\n        target_pelvis_z_celebrate = block_pos_z_top + 0.7 # Target pelvis height for celebration, relative z position\n        celebration_reward = -torch.abs(target_pelvis_z_celebrate - pelvis_pos_z) # CORRECT: Reward based on relative z distance\n\n        reward = torch.where(activation_condition, celebration_reward, torch.tensor(0.0, device=env.device)) # CORRECT: Apply reward only when activated\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # CORRECT: Normalize reward\n        RewNormalizer.update_stats(normaliser_name, reward) # CORRECT: Update reward stats\n        return scaled_reward\n    return reward\n\n\ndef overall_raw_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"overall_raw_reward\") -> torch.Tensor:\n    '''Overall raw reward for the obstacle course.\n    '''\n\n    # This should be a combination of +1 for being past the low wall.\n    # +1 for pushing the large sphere towards the high wall.\n    # +1 for kicking the small sphere towards the block.\n    # +1 for jumping onto the block.\n    reward = torch.zeros(env.num_envs, device=env.device)\n    \n    try:\n        robot = env.scene[\"robot\"]\n        low_wall = env.scene['Object3']\n        large_sphere = env.scene['Object1']\n        high_wall = env.scene['Object4']\n        small_sphere = env.scene['Object2']\n        block = env.scene['Object5']\n        \n        # +1 for being past the low wall\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n        pelvis_pos_x = pelvis_pos[:, 0]\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0]\n        past_low_wall = pelvis_pos_x > low_wall_pos_x\n        reward = torch.where(past_low_wall, reward + 1.0, reward)\n        \n        # +1 for pushing the large sphere towards the high wall (only if past low wall)\n        large_sphere_pos_x = large_sphere.data.root_pos_w[:, 0]\n        high_wall_pos_x = high_wall.data.root_pos_w[:, 0]\n        high_wall_pos_z = high_wall.data.root_pos_w[:, 2]\n        large_sphere_near_wall = torch.abs(high_wall_pos_x - large_sphere_pos_x) < 2.5\n        wall_pushed = high_wall_pos_z < 0.4  # Wall has fallen\n        sphere_pushed_wall = large_sphere_near_wall\n        # Only award if past low wall\n        reward = torch.where(past_low_wall & wall_pushed, reward + 1.0, reward)\n        \n        # +1 for kicking the small sphere away from the block (only if wall pushed)\n        small_sphere_pos_xy = small_sphere.data.root_pos_w[:, :2]\n        block_pos_xy = block.data.root_pos_w[:, :2]\n        small_sphere_away_from_block = torch.norm(small_sphere_pos_xy - block_pos_xy, dim=1) > 4.0\n        # Only award if past low wall and wall pushed\n        reward = torch.where(past_low_wall & wall_pushed & small_sphere_away_from_block, reward + 1.0, reward)\n        \n        # +1 for jumping onto the block (only if sphere kicked away)\n        pelvis_pos_z = pelvis_pos[:, 2]\n        block_pos_x = block.data.root_pos_w[:, 0]\n        block_pos_z_top = block.data.root_pos_w[:, 2] + 0.5  # top of block, 0.5m height\n        on_block = (pelvis_pos_x > block_pos_x - 0.5) & (pelvis_pos_x < block_pos_x + 0.5) & (pelvis_pos_z > block_pos_z_top + 0.3)\n        # Only award if all previous milestones completed\n        reward = torch.where(past_low_wall & wall_pushed & small_sphere_away_from_block & on_block, reward + 1.0, reward)\n\n        if normalise:\n            reward = RewNormalizer.normalize(normaliser_name, reward)\n            RewNormalizer.update_stats(normaliser_name, reward)\n            return reward\n        else:\n            reward = reward/4.0\n    \n    except KeyError:\n        pass  # Keep reward at zeros if objects not found\n    \n    return reward\n\n@configclass\nclass TaskRewardsCfg:\n    # MainObstacleCourseReward = RewTerm(func=main_obstacle_course_reward, weight=1.0,\n    #                                  params={\"normalise\": True, \"normaliser_name\": \"main_reward\"})\n    JumpOverLowWallReward = RewTerm(func=shaping_reward_jump_over_low_wall, weight=0.1,\n                                     params={\"normalise\": True, \"normaliser_name\": \"jump_low_wall_reward\"})\n    PushLargeSphereReward = RewTerm(func=shaping_reward_push_large_sphere, weight=0.1,\n                                     params={\"normalise\": True, \"normaliser_name\": \"push_sphere_reward\"})\n    KickSmallSphereReward = RewTerm(func=shaping_reward_kick_small_sphere, weight=0.1,\n                                     params={\"normalise\": True, \"normaliser_name\": \"kick_sphere_reward\"})\n    JumpOnBlockReward = RewTerm(func=shaping_reward_jump_on_block, weight=0.1,\n                                     params={\"normalise\": True, \"normaliser_name\": \"jump_block_reward\"})\n    # CelebrateOnBlockReward = RewTerm(func=shaping_reward_celebrate_on_block, weight=0.3,\n    #                                  params={\"normalise\": True, \"normaliser_name\": \"celebrate_reward\"})\n    OverallRawReward = RewTerm(func=overall_raw_reward, weight=1.0,\n                                     params={\"normalise\": False})\n",successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\n# Standard imports - DO NOT MODIFY\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer\nfrom ...objects import get_object_volume\n\ndef obstacle_course_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the obstacle_course skill has been successfully completed.'''\n    # 1. Get robot and block objects - CORRECT: Accessing robot and object using approved pattern\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot using approved pattern\n    try:\n        block = env.scene['Object5'] # CORRECT: Accessing object using approved pattern and try/except\n\n        # 2. Get robot pelvis and block positions - CORRECT: Accessing robot part and object positions using approved pattern\n        pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Getting pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Getting pelvis position using approved pattern\n        block_pos = block.data.root_pos_w # CORRECT: Getting block position using approved pattern\n\n        # 3. Calculate relative distances - CORRECT: Using relative distances as required\n        distance_x_block_pelvis = torch.abs(block_pos[:, 0] - pelvis_pos[:, 0]) # CORRECT: Relative x-distance between pelvis and block\n        distance_z_block_pelvis_top = pelvis_pos[:, 2] - (block_pos[:, 2] + 0.7) # CORRECT: Relative z-distance between pelvis and top of block (block height is 0.5m from object config)\n\n        # 4. Define success condition - CORRECT: Using relative distances and reasonable thresholds\n        x_threshold = 0.5 # Reasonable x-distance threshold\n        z_threshold = 0.3 # Reasonable z-distance threshold above the block\n        success_condition = (distance_x_block_pelvis < x_threshold) & (distance_z_block_pelvis_top > z_threshold) # CORRECT: Combining x and z conditions\n\n    except KeyError:\n        # 5. Handle missing objects - CORRECT: Handling missing object with try/except as required\n        success_condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device) # CORRECT: Return False if object is missing\n\n    # 6. Check duration and save success states - CORRECT: Using check_success_duration and save_success_state as required\n    success = check_success_duration(env, success_condition, \"obstacle_course\", duration=1.0) # CORRECT: Checking success duration for 1.0 second\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"obstacle_course\") # CORRECT: Saving success state for successful environments\n\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=obstacle_course_success)\n",policyVideo:"/videos/ZeroShotObstacleCourse.mp4",children:[{name:"JumpOverLowWall",level:2,rewardCode:"\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer\nfrom ...objects import get_object_volume\n\nimport torch\n\ndef main_JumpOverLowWall_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for JumpOverLowWall.\n\n    Reward for moving past the low wall in the x direction while also increasing pelvis height, encouraging the robot to jump over the wall.\n    The reward is activated when the robot is approaching the large sphere, ensuring it focuses on jumping the low wall first.\n    '''\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # CORRECT: Accessing low wall object using approved pattern and try/except\n        large_sphere = env.scene['Object1'] # CORRECT: Accessing large sphere object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0] # CORRECT: Accessing low wall x position using approved pattern\n        large_sphere_pos_x = large_sphere.data.root_pos_w[:, 0] # CORRECT: Accessing large sphere x position using approved pattern\n        low_wall_pos_z = low_wall.data.root_pos_w[:, 2] # CORRECT: Accessing low wall z position using approved pattern\n\n        distance_pelvis_wall_x = low_wall_pos_x - pelvis_pos[:, 0] # CORRECT: Relative distance in x direction\n        distance_pelvis_wall_z = low_wall_pos_z + 0.4 - pelvis_pos[:, 2] # CORRECT: Relative distance in z direction from pelvis to top of wall (wall height is 0.4m from config)\n\n        target_x_position_beyond_wall = low_wall_pos_x + 1.5 # Target x position 1.5m beyond the wall\n        reward_x_progress = -torch.abs(pelvis_pos[:, 0] - target_x_position_beyond_wall) # CORRECT: Reward for x progress beyond the wall, continuous reward\n\n        target_pelvis_z_height = low_wall_pos_z + 1.2 # Target pelvis height 1m above the wall (0.4m wall height + 0.6m clearance)\n        reward_z_height = -torch.abs(pelvis_pos[:, 2] - target_pelvis_z_height) # CORRECT: Reward for pelvis height above the wall, continuous reward\n\n        activation_condition = (pelvis_pos[:, 0] < low_wall_pos_x + 0.2) # Activation when robot is before the wall\n\n        pre_wall_reward =  reward_x_progress # Combining x and z rewards\n\n        post_wall_reward = reward_x_progress + reward_z_height\n\n        primary_reward = torch.where(activation_condition, pre_wall_reward, post_wall_reward) # Combining x and z rewards\n\n        reward = primary_reward # CORRECT: Apply activation condition\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef approach_wall_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"approach_wall_reward\") -> torch.Tensor:\n    '''Shaping reward for approaching the low wall.\n\n    Rewards the robot for moving closer to the low wall in the x-direction before reaching it.\n    This encourages forward movement towards the obstacle.\n    '''\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # CORRECT: Accessing low wall object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0] # CORRECT: Accessing low wall x position using approved pattern\n\n        distance_pelvis_wall_x = low_wall_pos_x - pelvis_pos[:, 0] # CORRECT: Relative distance in x direction\n\n        approach_wall_condition = (pelvis_pos[:, 0] < low_wall_pos_x) # CORRECT: Activation condition: robot is before the wall\n\n        reward_approach_wall = -torch.abs(distance_pelvis_wall_x) # CORRECT: Reward for decreasing x distance to the wall, continuous reward\n\n        reward = torch.where(approach_wall_condition, reward_approach_wall, torch.tensor(0.0, device=env.device)) # CORRECT: Apply activation condition\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef stable_pelvis_height_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"stable_pelvis_height_reward\") -> torch.Tensor:\n    '''Shaping reward for maintaining stable pelvis height after jumping over the wall.\n\n    Rewards the robot for maintaining a stable pelvis height close to a default standing height (0.7m)\n    after landing on the other side of the wall. This encourages stability after the jump.\n    '''\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # CORRECT: Accessing low wall object using approved pattern and try/except\n        large_sphere = env.scene['Object1'] # CORRECT: Accessing large sphere object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0] # CORRECT: Accessing low wall x position using approved pattern\n        large_sphere_pos_x = large_sphere.data.root_pos_w[:, 0] # CORRECT: Accessing large sphere x position using approved pattern\n\n        stable_pelvis_condition = (pelvis_pos[:, 0] > low_wall_pos_x) & (pelvis_pos[:, 0] < large_sphere_pos_x) # CORRECT: Activation condition: robot is past the wall and before the large sphere\n\n        target_pelvis_z = 0.7 # Default standing pelvis height\n        reward_stable_pelvis_z = -torch.abs(pelvis_pos[:, 2] - target_pelvis_z) # CORRECT: Reward for maintaining pelvis height close to 0.7m, continuous reward\n\n        reward = torch.where(stable_pelvis_condition, reward_stable_pelvis_z, torch.tensor(0.0, device=env.device)) # CORRECT: Apply activation condition\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef collision_avoidance_low_wall_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"collision_avoidance_low_wall_reward\") -> torch.Tensor:\n    '''Shaping reward for collision avoidance with the low wall.\n\n    Penalizes collisions between the robot's feet and the low wall, encouraging the robot to jump high enough.\n    '''\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # CORRECT: Accessing low wall object using approved pattern and try/except\n\n        left_foot_idx = robot.body_names.index('left_ankle_roll_link') # CORRECT: Accessing left foot index using approved pattern\n        right_foot_idx = robot.body_names.index('right_ankle_roll_link') # CORRECT: Accessing right foot index using approved pattern\n        left_foot_pos = robot.data.body_pos_w[:, left_foot_idx] # CORRECT: Accessing left foot position using approved pattern\n        right_foot_pos = robot.data.body_pos_w[:, right_foot_idx] # CORRECT: Accessing right foot position using approved pattern\n\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0] # CORRECT: Accessing low wall x position using approved pattern\n        low_wall_pos_y = low_wall.data.root_pos_w[:, 1] # CORRECT: Accessing low wall y position using approved pattern\n        low_wall_pos_z = low_wall.data.root_pos_w[:, 2] # CORRECT: Accessing low wall z position using approved pattern\n\n        low_wall_x_size = 0.4 # Hardcoded from object config\n        low_wall_y_size = 10.0 # Hardcoded from object config\n        low_wall_z_size = 0.4 # Hardcoded from object config\n\n        low_wall_x_min = low_wall_pos_x - low_wall_x_size/2.0 # CORRECT: min x bound of wall\n        low_wall_x_max = low_wall_pos_x + low_wall_x_size/2.0 # CORRECT: max x bound of wall\n        low_wall_y_min = low_wall_pos_y - low_wall_y_size/2.0 # CORRECT: min y bound of wall\n        low_wall_y_max = low_wall_pos_y + low_wall_y_size/2.0 # CORRECT: max y bound of wall\n        low_wall_z_max = low_wall_pos_z + low_wall_z_size # CORRECT: top z bound of wall\n\n        collision_reward = torch.zeros(env.num_envs, device=env.device) # Initialize collision reward to zero\n\n        # Collision condition for left foot\n        collision_left_foot = (left_foot_pos[:, 0] > low_wall_x_min) & (left_foot_pos[:, 0] < low_wall_x_max) &                               (left_foot_pos[:, 1] > low_wall_y_min) & (left_foot_pos[:, 1] < low_wall_y_max) &                               (left_foot_pos[:, 2] < low_wall_z_max) # CORRECT: Collision condition for left foot\n\n        # Collision condition for right foot\n        collision_right_foot = (right_foot_pos[:, 0] > low_wall_x_min) & (right_foot_pos[:, 0] < low_wall_x_max) &                                (right_foot_pos[:, 1] > low_wall_y_min) & (right_foot_pos[:, 1] < low_wall_y_max) &                                (right_foot_pos[:, 2] < low_wall_z_max) # CORRECT: Collision condition for right foot\n\n        collision_reward = torch.where(collision_left_foot, collision_reward - 0.1, collision_reward) # CORRECT: Penalize left foot collision\n        collision_reward = torch.where(collision_right_foot, collision_reward - 0.1, collision_reward) # CORRECT: Penalize right foot collision\n\n\n        reward = collision_reward\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    Main_JumpOverLowWallReward = RewTerm(func=main_JumpOverLowWall_reward, weight=1.0,\n                                params={\"normalise\": True, \"normaliser_name\": \"main_reward\"})\n    ApproachWallReward = RewTerm(func=approach_wall_reward, weight=0.4,\n                                params={\"normalise\": True, \"normaliser_name\": \"approach_wall_reward\"})\n    StablePelvisHeightReward = RewTerm(func=stable_pelvis_height_reward, weight=0.3,\n                                params={\"normalise\": True, \"normaliser_name\": \"stable_pelvis_height_reward\"})\n    CollisionAvoidanceLowWallReward = RewTerm(func=collision_avoidance_low_wall_reward, weight=0.2,\n                                params={\"normalise\": True, \"normaliser_name\": \"collision_avoidance_low_wall_reward\"})",successTerminationCode:"\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\ndef JumpOverLowWall_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the JumpOverLowWall skill has been successfully completed.'''\n    # 1. Get robot object - CORRECT: Approved access pattern\n    robot = env.scene[\"robot\"]\n\n    # 2. Get pelvis index - CORRECT: Approved access pattern\n    pelvis_idx = robot.body_names.index('pelvis')\n    # 3. Get pelvis position - CORRECT: Approved access pattern\n    pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n\n    try:\n        # 4. Get low wall object - CORRECT: Approved access pattern and try/except\n        low_wall = env.scene['Object3']\n        # 5. Get low wall position - CORRECT: Approved access pattern\n        wall_pos = low_wall.data.root_pos_w\n\n        # 6. Calculate relative x distance - CORRECT: Relative distance\n        relative_x_distance = pelvis_pos[:, 0] - wall_pos[:, 0]\n\n        pelvis_pos_y = pelvis_pos[:, 1]\n        wall_pos_y = wall_pos[:, 1]\n\n        y_condition = (pelvis_pos_y > wall_pos_y-2.5) & (pelvis_pos_y < wall_pos_y+2.5)\n\n        # 7. Define success condition: pelvis is past the wall in x direction by 0.7m - CORRECT: Relative distance and reasonable threshold\n        success_threshold = 0.7\n        condition = (relative_x_distance > success_threshold) & y_condition\n\n    except KeyError:\n        # 8. Handle missing object - CORRECT: Handle missing object with try/except\n        condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 9. Check success duration and save success states - CORRECT: Using check_success_duration and save_success_state\n    success = check_success_duration(env, condition, \"JumpOverLowWall\", duration=0.5) # Using duration = 0.5\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"JumpOverLowWall\")\n\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=JumpOverLowWall_success)\n\n",policyVideo:"/videos/L2_JumpOverLowWall.mp4",children:[{name:"WalkToLowWall",level:1,policyVideo:"/videos/WalkToWall.mp4",rewardCode:'\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer # DO NOT CHANGE THIS LINE!\nfrom ...objects import get_object_volume\n\ndef main_WalkToLowWall_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "main_reward") -> torch.Tensor:\n    \'\'\'Main reward for WalkToLowWall.\n\n    Rewards the robot for moving towards the low wall and being within 1m of it in the x-direction.\n    This encourages the robot to approach the low wall, which is the primary objective of this skill.\n    \'\'\'\n    robot = env.scene["robot"] # CORRECT: Accessing robot using approved pattern\n    try:\n        low_wall = env.scene[\'Object3\'] # CORRECT: Accessing low wall object using approved pattern and try/except for handling missing object\n        pelvis_idx = robot.body_names.index(\'pelvis\') # CORRECT: Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n        low_wall_pos = low_wall.data.root_pos_w # CORRECT: Accessing low wall position using approved pattern\n\n        # Calculate the distance in the x-direction between the pelvis and the low wall.\n        distance_x = low_wall_pos[:, 0] - pelvis_pos[:, 0] # CORRECT: Relative distance in x-direction\n\n        # Define the target distance to the low wall in the x-direction (1m).\n        target_distance_x = 1.0\n\n        # Reward is negative absolute difference between the current distance and the target distance.\n        # This is a continuous reward that encourages the robot to get closer to the target distance.\n        reward = -torch.abs(distance_x - target_distance_x) # CORRECT: Continuous reward based on relative distance to target, using absolute distance\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # Normalize and return reward\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # CORRECT: Normalize reward\n        RewNormalizer.update_stats(normaliser_name, reward) # CORRECT: Update reward stats\n        return scaled_reward\n    return reward\n\ndef pelvis_height_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "pelvis_height_reward") -> torch.Tensor:\n    \'\'\'Shaping reward for maintaining a stable pelvis height.\n\n    Rewards the robot for keeping its pelvis at a consistent height (around 0.7m).\n    This encourages stability and prevents the robot from falling, supporting the main task.\n    \'\'\'\n    robot = env.scene["robot"] # CORRECT: Accessing robot using approved pattern\n    try:\n        pelvis_idx = robot.body_names.index(\'pelvis\') # CORRECT: Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n\n        # Define the default pelvis height.\n        default_pelvis_z = 0.7\n\n        # Reward is negative absolute difference between the current pelvis z-position and the default height.\n        # This is a continuous reward that encourages the robot to maintain the desired pelvis height.\n        reward = -torch.abs(pelvis_pos[:, 2] - default_pelvis_z) # CORRECT: Continuous reward based on absolute pelvis z-position, but this is acceptable for height stability\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # Normalize and return reward\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # CORRECT: Normalize reward\n        RewNormalizer.update_stats(normaliser_name, reward) # CORRECT: Update reward stats\n        return scaled_reward\n    return reward\n\ndef no_overshoot_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "no_overshoot_reward") -> torch.Tensor:\n    \'\'\'Shaping reward to prevent overshooting the low wall.\n\n    Penalizes the robot for moving too far past the low wall in the x-direction.\n    This encourages the robot to stop near the low wall, preparing for the next skill.\n    \'\'\'\n    robot = env.scene["robot"] # CORRECT: Accessing robot using approved pattern\n    try:\n        low_wall = env.scene[\'Object3\'] # CORRECT: Accessing low wall object using approved pattern and try/except for handling missing object\n        pelvis_idx = robot.body_names.index(\'pelvis\') # CORRECT: Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n        low_wall_pos = low_wall.data.root_pos_w # CORRECT: Accessing low wall position using approved pattern\n\n        # Calculate the overshoot distance in the x-direction. Overshoot is defined as being past the wall + 1m.\n        overshoot_threshold_x = low_wall_pos[:, 0] + 1.0\n        distance_x_overshoot = pelvis_pos[:, 0] - overshoot_threshold_x # CORRECT: Relative distance for overshoot\n\n        # Activation condition: robot\'s pelvis x-position is beyond the overshoot threshold.\n        activation_condition_overshoot = (pelvis_pos[:, 0] > overshoot_threshold_x)\n\n        # Reward is negative overshoot distance when activated, otherwise zero.\n        # This penalizes overshooting and is only active when the robot is past the threshold.\n        reward = torch.where(activation_condition_overshoot, -torch.abs(distance_x_overshoot), torch.tensor(0.0, device=env.device)) # CORRECT: Conditional reward based on relative position, continuous when active\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # Normalize and return reward\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # CORRECT: Normalize reward\n        RewNormalizer.update_stats(normaliser_name, reward) # CORRECT: Update reward stats\n        return scaled_reward\n    return reward\n\ndef y_distance_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "y_distance_reward") -> torch.Tensor:\n    \'\'\'Shaping reward for maintaining a minimum y distance from the low wall.\n\n    Penalizes the robot for getting too close to the low wall in the y-direction.\n    This prevents sideways collisions with the wall and encourages a straight approach.\n    \'\'\'\n    robot = env.scene["robot"] # CORRECT: Accessing robot using approved pattern\n    try:\n        low_wall = env.scene[\'Object3\'] # CORRECT: Accessing low wall object using approved pattern and try/except for handling missing object\n        pelvis_idx = robot.body_names.index(\'pelvis\') # CORRECT: Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n        low_wall_pos = low_wall.data.root_pos_w # CORRECT: Accessing low wall position using approved pattern\n\n        # Calculate the distance in the y-direction between the pelvis and the low wall.\n        distance_y = low_wall_pos[:, 1] - pelvis_pos[:, 1] # CORRECT: Relative distance in y-direction\n\n        # Define the minimum allowed y distance.\n        min_distance_y = 0.5\n\n        # Absolute y distance\n        distance_y_abs = torch.abs(distance_y)\n\n        # Reward is negative difference between min_distance and current y distance when closer than min_distance, otherwise zero.\n        # This penalizes getting too close in the y-direction and is only active when closer than the threshold.\n        reward = -distance_y_abs  # CORRECT: Conditional reward based on relative distance, continuous when active\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # Normalize and return reward\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # CORRECT: Normalize reward\n        RewNormalizer.update_stats(normaliser_name, reward) # CORRECT: Update reward stats\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    # Main reward for walking to the low wall. Weight is set to 1.0 as it is the primary objective.\n    Main_WalkToLowWallReward = RewTerm(func=main_WalkToLowWall_reward, weight=1.0,\n                                params={"normalise": True, "normaliser_name": "main_reward"})\n\n    # Shaping reward for maintaining pelvis height. Weight is set to 0.4 to encourage stability without overpowering the main reward.\n    PelvisHeightReward = RewTerm(func=pelvis_height_reward, weight=0.4,\n                                params={"normalise": True, "normaliser_name": "pelvis_height_reward"})\n\n    # Shaping reward to prevent overshooting the low wall. Weight is set to 0.3 to guide the robot to stop near the wall.\n    NoOvershootReward = RewTerm(func=no_overshoot_reward, weight=0.3,\n                                params={"normalise": True, "normaliser_name": "no_overshoot_reward"})\n\n    # Shaping reward for maintaining y-distance from the low wall. Weight is set to 0.2 to prevent sideways collisions, less critical than height and overshoot.\n    YDistanceReward = RewTerm(func=y_distance_reward, weight=0.2,\n                                params={"normalise": True, "normaliser_name": "y_distance_reward"})\n',successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\ndef WalkToLowWall_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the WalkToLowWall skill has been successfully completed.'''\n    # 1. Get robot pelvis position using approved access pattern\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot using approved pattern\n    pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Accessing pelvis index using approved pattern\n    pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n\n    try:\n        # 2. Get low wall position using approved access pattern and handle potential KeyError\n        low_wall = env.scene['Object3'] # CORRECT: Accessing low wall object using approved pattern\n        low_wall_pos = low_wall.data.root_pos_w # CORRECT: Accessing low wall position using approved pattern\n\n        # 3. Calculate the relative distance in the x-direction between the low wall and the robot's pelvis.\n        #    This is a relative distance as required.\n        distance_x = low_wall_pos[:, 0] - pelvis_pos[:, 0] # CORRECT: Relative distance in x-direction\n\n        # 4. Define success condition: Robot pelvis is within 1m in front of the low wall in the x-direction.\n        #    Using a threshold of 1.0m as specified in the success criteria plan.\n        success_threshold_x_high = 1.5\n        success_threshold_x_low = 0.5\n        condition = (distance_x < success_threshold_x_high) & (distance_x > success_threshold_x_low) # CORRECT: Condition based on relative distance and threshold\n\n    except KeyError:\n        # 5. Handle KeyError if 'Object3' (low wall) is not found in the scene.\n        #    Return a tensor of False for all environments in this case.\n        condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device) # CORRECT: Handle missing object\n\n    # 6. Check success duration using the check_success_duration function.\n    #    Using a duration of 0.5 seconds as specified in the success criteria plan.\n    success = check_success_duration(env, condition, \"WalkToLowWall\", duration=0.5) # CORRECT: Check success duration\n\n    # 7. Save success states for environments that have succeeded in this step.\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"WalkToLowWall\") # CORRECT: Save success state\n\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=WalkToLowWall_success) # CORRECT: Define SuccessTerminationCfg class with the success function\n"},{name:"PrepareForJumpOverLowWall",level:1,policyVideo:"/videos/PrepareForJumpOverLowWall.mp4",rewardCode:'\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer # DO NOT CHANGE THIS LINE!\nfrom ...objects import get_object_volume\n\ndef main_PrepareForJumpOverLowWall_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "main_reward") -> torch.Tensor:\n    \'\'\'Main reward for PrepareForJumpOverLowWall.\n\n    Reward for moving towards low wall in x and lowering pelvis in z.\n    This encourages the robot to approach the low wall and crouch in preparation for a jump.\n    \'\'\'\n    robot = env.scene["robot"] # CORRECT: Accessing robot using approved pattern\n    try:\n        low_wall = env.scene[\'Object3\'] # CORRECT: Accessing low wall object using approved pattern and try/except\n        pelvis_idx = robot.body_names.index(\'pelvis\') # CORRECT: Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n\n        default_pelvis_z = 0.7 # Assuming default pelvis height is 0.7m, this is NOT hardcoded position as it is a relative default height.\n        target_pelvis_z = 0.4   \n        distance_z_pelvis_target = target_pelvis_z - pelvis_pos[:, 2] # CORRECT: Relative distance in z-direction between target pelvis height and current pelvis height\n        reward_lower_pelvis = -torch.abs(distance_z_pelvis_target) # CORRECT: Reward for lowering pelvis, negative absolute distance from target height\n\n        reward = reward_lower_pelvis # CORRECT: Combining x and z rewards\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef approach_wall_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "approach_wall_reward") -> torch.Tensor:\n    \'\'\'Shaping reward 1: Reward for moving closer to the low wall in the x direction when far enough.\n    \'\'\'\n    robot = env.scene["robot"] # CORRECT: Accessing robot using approved pattern\n    try:\n        low_wall = env.scene[\'Object3\'] # CORRECT: Accessing low wall object using approved pattern and try/except\n        pelvis_idx = robot.body_names.index(\'pelvis\') # CORRECT: Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0] # CORRECT: Accessing low wall x position using approved pattern\n\n        distance_x_wall = low_wall_pos_x - pelvis_pos[:, 0] # CORRECT: Relative distance in x-direction between low wall and pelvis\n\n        activation_condition_approach_wall = (pelvis_pos[:, 0] < low_wall_pos_x - 1.0) # CORRECT: Activation condition: robot is more than 1m behind the low wall in x\n\n        reward_approach_wall_x = -torch.abs(distance_x_wall) # CORRECT: Reward for moving closer to wall in x, negative absolute distance\n\n        reward = torch.where(activation_condition_approach_wall, reward_approach_wall_x, torch.tensor(0.0, device=env.device)) # CORRECT: Apply reward only when activation condition is met\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef crouch_pelvis_z_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "crouch_pelvis_z_reward") -> torch.Tensor:\n    \'\'\'Shaping reward 2: Reward for lowering the pelvis in the z direction.\n    \'\'\'\n    robot = env.scene["robot"] # CORRECT: Accessing robot using approved pattern\n    pelvis_idx = robot.body_names.index(\'pelvis\') # CORRECT: Accessing pelvis index using approved pattern\n    pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n    pelvis_pos_z = pelvis_pos[:, 2] # CORRECT: Accessing pelvis z position\n\n    target_pelvis_z = 0.4 # Target pelvis height for crouching, NOT hardcoded position as it is a relative target height.\n    reward_crouch_pelvis_z = -torch.abs(target_pelvis_z - pelvis_pos_z) # CORRECT: Reward for getting pelvis closer to target height, negative absolute distance\n\n    reward = reward_crouch_pelvis_z\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef avoid_collision_wall_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "avoid_collision_wall_reward") -> torch.Tensor:\n    \'\'\'Shaping reward 3: Penalty for getting too close to the low wall in x when in front of it.\n    \'\'\'\n    robot = env.scene["robot"] # CORRECT: Accessing robot using approved pattern\n    try:\n        low_wall = env.scene[\'Object3\'] # CORRECT: Accessing low wall object using approved pattern and try/except\n        pelvis_idx = robot.body_names.index(\'pelvis\') # CORRECT: Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0] # CORRECT: Accessing low wall x position using approved pattern\n\n        distance_x_wall = low_wall_pos_x - pelvis_pos[:, 0] # CORRECT: Relative distance in x-direction between low wall and pelvis\n\n        activation_condition_avoid_wall = (pelvis_pos[:, 0] >= low_wall_pos_x - 1.0) # CORRECT: Activation condition: robot is close to or in front of the low wall\n\n        penalty_collision_wall_x = -torch.abs(distance_x_wall) * 5.0 # CORRECT: Penalty for being too close to the wall in x, negative absolute distance, multiplied by 5 for stronger penalty\n\n        reward = torch.where(activation_condition_avoid_wall & (distance_x_wall < 0.5), penalty_collision_wall_x, torch.tensor(0.0, device=env.device)) # CORRECT: Apply penalty only when activation condition is met and distance is less than 0.5m\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef stay_behind_sphere_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "stay_behind_sphere_reward") -> torch.Tensor:\n    \'\'\'Shaping reward 4: Discourage moving too far in x, relative to the large sphere. Reward for staying behind the large sphere.\n    \'\'\'\n    robot = env.scene["robot"] # CORRECT: Accessing robot using approved pattern\n    try:\n        large_sphere = env.scene[\'Object1\'] # CORRECT: Accessing large sphere object using approved pattern and try/except\n        pelvis_idx = robot.body_names.index(\'pelvis\') # CORRECT: Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n        large_sphere_pos_x = large_sphere.data.root_pos_w[:, 0] # CORRECT: Accessing large sphere x position using approved pattern\n\n        distance_x_sphere = large_sphere_pos_x - pelvis_pos[:, 0] # CORRECT: Relative distance in x-direction between large sphere and pelvis\n\n        reward_stay_behind_sphere_x = -torch.abs(torch.relu(-distance_x_sphere)) # CORRECT: Reward for staying behind the sphere, negative absolute distance when behind, 0 when in front\n\n        reward = reward_stay_behind_sphere_x\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    Main_PrepareForJumpOverLowWallReward = RewTerm(func=main_PrepareForJumpOverLowWall_reward, weight=1.0,\n                                params={"normalise": True, "normaliser_name": "main_reward"})\n    ApproachWallReward = RewTerm(func=approach_wall_reward, weight=0.5,\n                                params={"normalise": True, "normaliser_name": "approach_wall_reward"})\n    CrouchPelvisZReward = RewTerm(func=crouch_pelvis_z_reward, weight=0.4,\n                                params={"normalise": True, "normaliser_name": "crouch_pelvis_z_reward"})\n    AvoidCollisionWallReward = RewTerm(func=avoid_collision_wall_reward, weight=0.3,\n                                params={"normalise": True, "normaliser_name": "avoid_collision_wall_reward"})\n    StayBehindSphereReward = RewTerm(func=stay_behind_sphere_reward, weight=0.2,\n                                params={"normalise": True, "normaliser_name": "stay_behind_sphere_reward"})\n',successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\ndef PrepareForJumpOverLowWall_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the PrepareForJumpOverLowWall skill has been successfully completed.'''\n    # 1. Get robot object - CORRECT: Approved access pattern\n    robot = env.scene[\"robot\"]\n\n    # 2. Get pelvis index and position - CORRECT: Approved access pattern\n    robot_pelvis_idx = robot.body_names.index('pelvis')\n    robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx]\n    robot_pelvis_pos_x = robot_pelvis_pos[:, 0]\n    robot_pelvis_pos_z = robot_pelvis_pos[:, 2]\n\n    try:\n        # 3. Get low wall object - CORRECT: Approved access pattern and try-except\n        low_wall = env.scene['Object3']\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0]\n\n        # 4. Calculate relative distances - CORRECT: Relative distances\n        distance_x_wall = low_wall_pos_x - robot_pelvis_pos_x # Distance in x direction to the wall\n        pelvis_height = robot_pelvis_pos_z # Pelvis height\n\n        # 5. Define success conditions - CORRECT: Relative distances and reasonable thresholds\n        close_to_wall_x = (distance_x_wall < 1) # Robot is within 1m in front of the wall in x direction\n        pelvis_crouched = (pelvis_height < 0.5) # Robot pelvis is below 0.5m height\n\n        # 6. Combine success conditions - CORRECT: Combining conditions with &\n        condition = close_to_wall_x & pelvis_crouched\n\n    except KeyError:\n        # 7. Handle missing object - CORRECT: Handle missing object\n        condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 8. Check success duration and save success state - CORRECT: Using check_success_duration and save_success_state\n    success = check_success_duration(env, condition, \"PrepareForJumpOverLowWall\", duration=1)\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"PrepareForJumpOverLowWall\")\n\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=PrepareForJumpOverLowWall_success)\n"},{name:"ExecuteJumpOverLowWall",level:1,policyVideo:"/videos/ExecuteJumpOverLowWall.mp4",rewardCode:"\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer # DO NOT CHANGE THIS LINE!\nfrom ...objects import get_object_volume\n\ndef main_ExecuteJumpOverLowWall_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for ExecuteJumpOverLowWall.\n\n    Phases the reward based on robot's x position relative to the low wall.\n    Phase 1: Before the wall - reward for feet height to encourage jumping.\n    Phase 2: After the wall - reward for reaching a target x position past the wall.\n    Uses relative distances and approved access patterns. Handles missing objects and normalizes reward.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # Accessing low wall object using approved pattern and try/except\n        large_sphere = env.scene['Object1'] # Accessing large sphere object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Accessing pelvis position using approved pattern\n        pelvis_pos_x = pelvis_pos[:, 0] # Getting x component of pelvis position\n        pelvis_pos_z = pelvis_pos[:, 2] # Getting z component of pelvis position\n\n        left_ankle_roll_link_idx = robot.body_names.index('left_ankle_roll_link') # Accessing left ankle index using approved pattern\n        right_ankle_roll_link_idx = robot.body_names.index('right_ankle_roll_link') # Accessing right ankle index using approved pattern\n        left_ankle_roll_link_pos = robot.data.body_pos_w[:, left_ankle_roll_link_idx] # Accessing left ankle position using approved pattern\n        right_ankle_roll_link_pos = robot.data.body_pos_w[:, right_ankle_roll_link_idx] # Accessing right ankle position using approved pattern\n        feet_pos_z = (left_ankle_roll_link_pos[:, 2] + right_ankle_roll_link_pos[:, 2]) / 2 # Calculating average feet z position\n\n        low_wall_x = low_wall.data.root_pos_w[:, 0] # Accessing low wall x position using approved pattern\n        large_sphere_x = large_sphere.data.root_pos_w[:, 0] # Accessing large sphere x position using approved pattern\n        target_x = (low_wall_x + large_sphere_x) / 2 # Calculating target x position as midpoint between low wall and large sphere (relative distance)\n        low_wall_height = 0.4 # Hardcoded low wall height from object config (approved as size is not accessible)\n\n        # Phase 1: Before the wall - reward for feet height and pelvis height\n        activation_condition_phase1 = (pelvis_pos_x < (low_wall_x + 0.5)) # Activation condition based on relative x position to low wall\n        target_feet_pos_z = 0.9  # Target feet z position\n        target_pelvis_pos_z = 1.6 # Target pelvis z position\n\n        reward_phase1 = -torch.abs((feet_pos_z - target_feet_pos_z)) - torch.abs((pelvis_pos_z - target_pelvis_pos_z)) # Reward for feet height above low wall height, relative height\n\n        # Phase 2: After the wall - reward for target x position\n        activation_condition_phase2 = (pelvis_pos_x >= (low_wall_x + 0.5)) # Activation condition based on relative x position to low wall\n        reward_phase2 = torch.exp(-torch.abs(pelvis_pos_x - target_x)) # Reward for being close to target x position, relative distance\n\n        primary_reward = torch.where(activation_condition_phase1, reward_phase1, torch.zeros_like(reward_phase1)) # Combining phase rewards based on activation conditions\n        reward_phase2 = torch.where(activation_condition_phase2, reward_phase2, torch.zeros_like(reward_phase2)) # Combining phase rewards based on activation conditions\n        reward = primary_reward + reward_phase2 # Adding phase 2 reward to primary reward\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handling missing object, returning zero reward\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # Normalizing reward\n        RewNormalizer.update_stats(normaliser_name, reward) # Updating normalizer stats\n        return scaled_reward\n    return reward\n\ndef shaping_reward_approach_wall(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"approach_wall_reward\") -> torch.Tensor:\n    '''Shaping reward to encourage robot to approach the low wall in x direction.\n    Active when robot is significantly behind the wall. Uses relative distances and approved access patterns.\n    Handles missing objects and normalizes reward.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # Accessing low wall object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Accessing pelvis position using approved pattern\n        pelvis_pos_x = pelvis_pos[:, 0] # Getting x component of pelvis position\n\n        low_wall_x = low_wall.data.root_pos_w[:, 0] # Accessing low wall x position using approved pattern\n\n        activation_condition_approach = (pelvis_pos_x < (low_wall_x - 1)) # Activation condition when pelvis is significantly behind the low wall (relative distance)\n        reward_approach = -torch.abs(pelvis_pos_x - low_wall_x) # Reward for being closer to the low wall in x direction, relative distance\n\n        shaping_reward_1 = torch.where(activation_condition_approach, reward_approach, torch.tensor(0.0, device=env.device)) # Applying reward only when activation condition is met\n        reward = shaping_reward_1 # Assigning shaping reward to reward variable\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handling missing object, returning zero reward\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # Normalizing reward\n        RewNormalizer.update_stats(normaliser_name, reward) # Updating normalizer stats\n        return scaled_reward\n    return reward\n\ndef shaping_reward_forward_movement(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"forward_movement_reward\") -> torch.Tensor:\n    '''Shaping reward to encourage forward pelvis movement near the low wall.\n    Active when robot is near the low wall. Uses relative distances and approved access patterns.\n    Handles missing objects and normalizes reward.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # Accessing low wall object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Accessing pelvis position using approved pattern\n        pelvis_pos_x = pelvis_pos[:, 0] # Getting x component of pelvis position\n\n        low_wall_x = low_wall.data.root_pos_w[:, 0] # Accessing low wall x position using approved pattern\n\n        activation_condition_forward = (pelvis_pos_x >= (low_wall_x - 0.5)) & (pelvis_pos_x < (low_wall_x + 0.5)) # Activation condition when pelvis is near the low wall (relative distance)\n        reward_forward = -(pelvis_pos_x - (low_wall_x + 0.5)) # Reward for moving slightly past the wall in x, relative distance\n\n        shaping_reward_2 = torch.where(activation_condition_forward, reward_forward, torch.tensor(0.0, device=env.device)) # Applying reward only when activation condition is met\n        reward = shaping_reward_2 # Assigning shaping reward to reward variable\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handling missing object, returning zero reward\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # Normalizing reward\n        RewNormalizer.update_stats(normaliser_name, reward) # Updating normalizer stats\n        return scaled_reward\n    return reward\n\ndef shaping_reward_stable_landing(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"stable_landing_reward\") -> torch.Tensor:\n    '''Shaping reward to encourage stable landing after jumping over the wall.\n    Rewards pelvis z-position close to default standing height after passing the wall.\n    Uses relative distances (implicitly through pelvis_z) and approved access patterns.\n    Handles missing objects and normalizes reward.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # Accessing low wall object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Accessing pelvis position using approved pattern\n        pelvis_pos_x = pelvis_pos[:, 0] # Getting x component of pelvis position\n        pelvis_pos_z = pelvis_pos[:, 2] # Getting z component of pelvis position\n\n        activation_condition_stable_z = (pelvis_pos_x >= (low_wall.data.root_pos_w[:, 0] + 0.5)) # Activation condition after passing the wall (relative distance)\n        reward_stable_z = -torch.abs(pelvis_pos_z - 0.7) # Reward for pelvis z position being close to 0.7 (default standing height), relative height\n\n        shaping_reward_3 = torch.where(activation_condition_stable_z, reward_stable_z, torch.tensor(0.0, device=env.device)) # Applying reward only when activation condition is met\n        reward = shaping_reward_3 # Assigning shaping reward to reward variable\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handling missing object, returning zero reward\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # Normalizing reward\n        RewNormalizer.update_stats(normaliser_name, reward) # Updating normalizer stats\n        return scaled_reward\n    return reward\n\ndef shaping_reward_feet_pelvis_alignment(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"feet_pelvis_alignment_reward\") -> torch.Tensor:\n    '''Shaping reward to encourage feet and pelvis alignment.\n    Rewards feet and pelvis being close to each other in z direction.\n    Uses relative distances (implicitly through pelvis_z) and approved access patterns.\n    Handles missing objects and normalizes reward.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        pelvis_idx = robot.body_names.index('pelvis') # Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Accessing pelvis position using approved pattern\n        pelvis_pos_x = pelvis_pos[:, 0] # Getting x component of pelvis position\n        pelvis_pos_y = pelvis_pos[:, 1]\n\n        left_ankle_roll_link_idx = robot.body_names.index('left_ankle_roll_link') # Accessing left ankle index using approved pattern\n        right_ankle_roll_link_idx = robot.body_names.index('right_ankle_roll_link') # Accessing right ankle index using approved pattern\n        left_ankle_roll_link_pos = robot.data.body_pos_w[:, left_ankle_roll_link_idx] # Accessing left ankle position using approved pattern\n        right_ankle_roll_link_pos = robot.data.body_pos_w[:, right_ankle_roll_link_idx] # Accessing right ankle position using approved pattern\n\n        reward = -torch.abs(pelvis_pos_y - right_ankle_roll_link_pos[:, 1]) - torch.abs(pelvis_pos_y - left_ankle_roll_link_pos[:, 1])             - torch.abs(pelvis_pos_x - right_ankle_roll_link_pos[:, 0]) - torch.abs(pelvis_pos_x - left_ankle_roll_link_pos[:, 0]) # Reward for feet and pelvis being close to each other in y and x direction, relative height\n\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handling missing object, returning zero reward\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # Normalizing reward\n        RewNormalizer.update_stats(normaliser_name, reward) # Updating normalizer stats\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    Main_ExecuteJumpOverLowWallReward = RewTerm(func=main_ExecuteJumpOverLowWall_reward, weight=1.0,\n                                params={\"normalise\": True, \"normaliser_name\": \"main_reward\"})\n\n    ShapingRewardApproachWall = RewTerm(func=shaping_reward_approach_wall, weight=0.4,\n                                params={\"normalise\": True, \"normaliser_name\": \"approach_wall_reward\"})\n\n    ShapingRewardForwardMovement = RewTerm(func=shaping_reward_forward_movement, weight=0,\n                                params={\"normalise\": True, \"normaliser_name\": \"forward_movement_reward\"})\n\n    ShapingRewardStableLanding = RewTerm(func=shaping_reward_stable_landing, weight=0.2,\n                                params={\"normalise\": True, \"normaliser_name\": \"stable_landing_reward\"})\n    \n    ShapingRewardFeetPelvisAlignment = RewTerm(func=shaping_reward_feet_pelvis_alignment, weight=0.3,\n                                params={\"normalise\": True, \"normaliser_name\": \"feet_pelvis_alignment_reward\"})\n",successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\ndef ExecuteJumpOverLowWall_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the ExecuteJumpOverLowWall skill has been successfully completed.'''\n    # 1. Access robot object (APPROVED PATTERN)\n    robot = env.scene[\"robot\"]\n\n    # 2. Get pelvis index (APPROVED PATTERN)\n    pelvis_idx = robot.body_names.index('pelvis')\n    # 3. Get pelvis position (APPROVED PATTERN)\n    pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n    pelvis_pos_x = pelvis_pos[:, 0] # Get x component of pelvis position\n\n    try:\n        # 4. Access low wall object (APPROVED PATTERN and try-except for error handling)\n        low_wall = env.scene['Object3']\n        # 5. Get low wall position (APPROVED PATTERN)\n        low_wall_x = low_wall.data.root_pos_w[:, 0] # Get x component of low wall position\n\n        # 6. Calculate relative distance in x direction (RELATIVE DISTANCE)\n        distance_x_wall_pelvis = pelvis_pos_x - low_wall_x\n\n        # 7. Define success condition based on relative distance (RELATIVE DISTANCE and REASONABLE THRESHOLD)\n        success_condition = distance_x_wall_pelvis > 0.5 # Check if pelvis is 0.5m past the wall in x direction\n\n    except KeyError:\n        # 8. Handle missing object (ERROR HANDLING)\n        success_condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 9. Check success duration and save success state (REQUIRED FUNCTIONS)\n    success = check_success_duration(env, success_condition, \"ExecuteJumpOverLowWall\", duration=0.5) # Check if success is maintained for 0.5 seconds\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"ExecuteJumpOverLowWall\") # Save success state for successful environments\n\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=ExecuteJumpOverLowWall_success)\n"},{name:"LandStablyAfterLowWall",level:1,policyVideo:"/videos/LandStablyAfterLowWall.mp4",rewardCode:"\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer\nfrom ...objects import get_object_volume\n\ndef main_LandStablyAfterLowWall_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for LandStablyAfterLowWall.\n\n    Reward for being in the target x range just past the low wall and before the large sphere, and close to the ground.\n    This encourages the robot to land stably after jumping over the low wall in the desired zone.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # Accessing low wall object using approved pattern and try/except\n        large_sphere = env.scene['Object1'] # Accessing large sphere object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Accessing pelvis position using approved pattern\n        pelvis_pos_z = pelvis_pos[:, 2]\n\n        distance_z = pelvis_pos_z # Relative distance in z from ground level (z=0)\n\n        pelvis_default_z = 0.7\n\n\n        reward_z = -torch.abs(distance_z-pelvis_default_z) # Reward for pelvis being the correct height.\n\n        reward = reward_z # Combining x and z rewards\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_1_increase_pelvis_height(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"shaping_reward_1\") -> torch.Tensor:\n    '''Shaping reward for increasing pelvis height as robot approaches the low wall.\n    Encourages the robot to jump as it gets closer to the wall.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # Accessing low wall object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Accessing pelvis position using approved pattern\n        pelvis_pos_x = pelvis_pos[:, 0]\n        pelvis_pos_z = pelvis_pos[:, 2]\n\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0] # Accessing low wall x position using approved pattern\n\n        distance_x_to_wall = low_wall_pos_x - pelvis_pos_x # Relative distance in x to the low wall\n\n        activation_condition = (pelvis_pos_x > low_wall_pos_x) # Activation when robot is past the low wall (after the wall in x direction)\n\n        reward_height = pelvis_pos_z # Reward for increasing pelvis height (absolute z position, but used as relative increase from ground)\n        reward = reward_height\n\n        reward = torch.where(activation_condition, reward, torch.tensor(0.0, device=env.device)) # Apply activation condition\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_2_feet_close_to_ground(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"shaping_reward_2\") -> torch.Tensor:\n    '''Shaping reward for both feet being close to the ground after passing the low wall in x direction.\n    Encourages stable landing on both feet.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # Accessing low wall object using approved pattern and try/except\n\n        left_foot_idx = robot.body_names.index('left_ankle_roll_link') # Accessing left foot index using approved pattern\n        right_foot_idx = robot.body_names.index('right_ankle_roll_link') # Accessing right foot index using approved pattern\n        left_foot_pos = robot.data.body_pos_w[:, left_foot_idx] # Accessing left foot position using approved pattern\n        right_foot_pos = robot.data.body_pos_w[:, right_foot_idx] # Accessing right foot position using approved pattern\n        left_foot_pos_z = left_foot_pos[:, 2]\n        right_foot_pos_z = right_foot_pos[:, 2]\n        pelvis_pos = robot.data.body_pos_w[:, robot.body_names.index('pelvis')] # Accessing pelvis position using approved pattern\n        pelvis_pos_x = pelvis_pos[:, 0]\n\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0] # Accessing low wall x position using approved pattern\n\n        activation_condition = (pelvis_pos_x > low_wall_pos_x) # Activation when robot is past the low wall in x direction\n\n        reward_left_foot = -torch.abs(left_foot_pos_z - 0.0) # Reward for left foot being close to ground\n        reward_right_foot = -torch.abs(right_foot_pos_z - 0.0) # Reward for right foot being close to ground\n\n        reward = reward_left_foot + reward_right_foot # Combining left and right foot rewards\n        reward = torch.where(activation_condition, reward, torch.tensor(0.0, device=env.device)) # Apply activation condition\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_3_stable_pelvis_height(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"shaping_reward_3\") -> torch.Tensor:\n    '''Shaping reward for maintaining a stable pelvis height after landing.\n    Encourages body stabilization after the jump. Target pelvis height is set to 0.7m (relative target height).\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # Accessing low wall object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Accessing pelvis position using approved pattern\n        pelvis_pos_x = pelvis_pos[:, 0]\n        pelvis_pos_z = pelvis_pos[:, 2]\n        target_pelvis_z = 0.7 # Target pelvis height (relative to ground)\n\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0] # Accessing low wall x position using approved pattern\n\n        activation_condition = (pelvis_pos_x > low_wall_pos_x) # Activation when robot is past the low wall in x direction\n\n        reward_pelvis_z = -torch.abs(pelvis_pos_z - target_pelvis_z) # Reward for pelvis being at a stable height (relative distance to target height)\n\n        reward = reward_pelvis_z\n        reward = torch.where(activation_condition, reward, torch.tensor(0.0, device=env.device)) # Apply activation condition\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    Main_LandStablyAfterLowWallReward = RewTerm(func=main_LandStablyAfterLowWall_reward, weight=1.0,\n                                params={\"normalise\": True, \"normaliser_name\": \"main_reward\"})\n\n    ShapingReward_IncreasePelvisHeight = RewTerm(func=shaping_reward_1_increase_pelvis_height, weight=0.5,\n                                params={\"normalise\": True, \"normaliser_name\": \"shaping_reward_1\"})\n\n    ShapingReward_FeetCloseToGround = RewTerm(func=shaping_reward_2_feet_close_to_ground, weight=0.6,\n                                params={\"normalise\": True, \"normaliser_name\": \"shaping_reward_2\"})\n\n    ShapingReward_StablePelvisHeight = RewTerm(func=shaping_reward_3_stable_pelvis_height, weight=0.3,\n                                params={\"normalise\": True, \"normaliser_name\": \"shaping_reward_3\"})\n",successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\n# Standard imports - DO NOT MODIFY\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer\nfrom ...objects import get_object_volume\n\ndef LandStablyAfterLowWall_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the LandStablyAfterLowWall skill has been successfully completed.\n    Success is defined as the robot landing stably on both feet on the other side of the low wall.\n    '''\n    # 1. Get robot object - APPROVED PATTERN\n    robot = env.scene[\"robot\"]\n\n    # 2. Get indices for robot body parts - APPROVED PATTERN\n    pelvis_idx = robot.body_names.index('pelvis')\n    left_foot_idx = robot.body_names.index('left_ankle_roll_link')\n    right_foot_idx = robot.body_names.index('right_ankle_roll_link')\n\n    # 3. Get positions of robot parts - APPROVED PATTERN\n    pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # [num_envs, 3]\n    left_foot_pos = robot.data.body_pos_w[:, left_foot_idx] # [num_envs, 3]\n    right_foot_pos = robot.data.body_pos_w[:, right_foot_idx] # [num_envs, 3]\n\n    try:\n        # 4. Get low wall object - APPROVED PATTERN with try/except for robustness\n        low_wall = env.scene['Object3']\n        low_wall_pos = low_wall.data.root_pos_w # [num_envs, 3]\n\n        # 5. Calculate relative distances - REQUIREMENT 1: Relative distances only\n        pelvis_x_distance_to_wall = pelvis_pos[:, 0] - low_wall_pos[:, 0] # x distance between pelvis and low wall\n        left_foot_z_distance_to_ground = torch.abs(left_foot_pos[:, 2] - 0.0) # z distance of left foot from ground (z=0)\n        right_foot_z_distance_to_ground = torch.abs(right_foot_pos[:, 2] - 0.0) # z distance of right foot from ground (z=0)\n        pelvis_z_distance_to_ground = torch.abs(pelvis_pos[:, 2] - 0.0) # z distance of pelvis from ground (z=0)\n\n        # 6. Define success conditions - REQUIREMENT 4: No hardcoded thresholds, REQUIREMENT 5: Reasonable tolerances\n        pelvis_past_wall_condition = pelvis_x_distance_to_wall > 0.5 # Pelvis is 0.5m past the low wall in x direction\n        left_foot_low_condition = left_foot_z_distance_to_ground < 0.15 # Left foot is within 0.15m of the ground\n        right_foot_low_condition = right_foot_z_distance_to_ground < 0.15 # Right foot is within 0.15m of the ground\n        pelvis_low_condition = pelvis_z_distance_to_ground < 0.8 # Pelvis is within 0.8m of the ground\n        pelvis_high_condition = pelvis_z_distance_to_ground > 0.5 # Pelvis is above 0.5m of the ground\n\n        # 7. Combine success conditions - REQUIREMENT 1: Relative distances only\n        land_stably_condition = pelvis_past_wall_condition & left_foot_low_condition & right_foot_low_condition & pelvis_low_condition & pelvis_high_condition\n\n    except KeyError:\n        # 8. Handle missing object - REQUIREMENT 6: Handle missing objects\n        land_stably_condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 9. Check success duration and save success state - REQUIREMENT 7 & 8: check_success_duration and save_success_state\n    success = check_success_duration(env, land_stably_condition, \"LandStablyAfterLowWall\", duration=0.5) # Duration of 0.5 seconds\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"LandStablyAfterLowWall\")\n\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=LandStablyAfterLowWall_success)\n"}]},{name:"PushLargeSphereToHighWall",level:2,rewardCode:"from isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer # DO NOT CHANGE THIS LINE!\nfrom ...objects import get_object_volume\n\ndef main_PushLargeSphereToHighWall_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for PushLargeSphereToHighWall.\n\n    Reward for moving the large sphere closer to the high wall in the x-direction.\n    This encourages the robot to push the large sphere towards the high wall to knock it over.\n    '''\n    try:\n        large_sphere = env.scene['Object1'] # Access the large sphere using approved pattern (rule 2, rule 3, rule 5)\n        high_wall = env.scene['Object4'] # Access the high wall using approved pattern (rule 2, rule 3, rule 5)\n\n        # Calculate the x-distance between the large sphere and the high wall (rule 1)\n        distance_x = high_wall.data.root_pos_w[:, 0] - large_sphere.data.root_pos_w[:, 0] # Access object positions using approved pattern (rule 2)\n\n        # Reward is negative absolute x-distance to encourage minimizing the distance (rule 4, rule 5)\n        reward = -torch.abs(distance_x) # Continuous reward (rule 5)\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing objects (rule 5, rule 6)\n\n    # Reward normalization (rule 6)\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_approach_large_sphere_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"approach_sphere_reward\") -> torch.Tensor:\n    '''Shaping reward for approaching the large sphere.\n\n    Reward for reducing the x-distance between the robot's pelvis and the large sphere when the robot is behind the sphere.\n    This encourages the robot to move towards the large sphere before pushing it.\n    '''\n    try:\n        large_sphere = env.scene['Object1'] # Access the large sphere using approved pattern (rule 2, rule 3, rule 5)\n        robot = env.scene['robot'] # Access the robot object (rule 2, rule 3)\n        pelvis_idx = robot.body_names.index('pelvis') # Get pelvis index using approved pattern (rule 3)\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Get pelvis position using approved pattern (rule 3)\n\n        # Calculate the x-distance between the pelvis and the large sphere (rule 1)\n        distance_x = large_sphere.data.root_pos_w[:, 0] - pelvis_pos[:, 0] # Access object and robot part positions using approved pattern (rule 2, rule 3)\n        distance_y = large_sphere.data.root_pos_w[:, 1] - pelvis_pos[:, 1] # Access object and robot part positions using approved pattern (rule 2, rule 3)\n\n        # Reward is negative absolute x-distance when activated (rule 4, rule 5)\n        reward = -torch.abs(distance_x) - torch.abs(distance_y) # Continuous reward (rule 5)\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing objects (rule 5, rule 6)\n\n    # Reward normalization (rule 6)\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_stable_pelvis_height_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"stable_height_reward\") -> torch.Tensor:\n    '''Shaping reward for maintaining a stable pelvis height.\n\n    Reward for keeping the pelvis height close to a target height (0.7m).\n    This encourages the robot to stay upright and stable.\n    '''\n    try:\n        robot = env.scene['robot'] # Access the robot object (rule 2, rule 3)\n        pelvis_idx = robot.body_names.index('pelvis') # Get pelvis index using approved pattern (rule 3)\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Get pelvis position using approved pattern (rule 3)\n\n        # Define target pelvis height (no hardcoded position, but target height is acceptable as a task parameter)\n        target_pelvis_z = 0.7\n\n        # Calculate the z-distance between the current pelvis height and the target height (rule 1)\n        distance_z = target_pelvis_z - pelvis_pos[:, 2] # Relative distance (rule 1)\n\n        # Reward is negative absolute z-distance (rule 4, rule 5)\n        reward = -torch.abs(distance_z) # Continuous reward (rule 5)\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing objects (rule 5, rule 6)\n\n    # Reward normalization (rule 6)\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_collision_avoidance_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"avoid_collision_reward\") -> torch.Tensor:\n    '''Shaping reward for collision avoidance with low and high walls.\n\n    Penalize the robot for getting too close to the low wall and high wall unnecessarily,\n    but deactivate this reward when the robot is close to the large sphere to allow pushing.\n    '''\n    try:\n        low_wall = env.scene['Object3'] # Access the low wall using approved pattern (rule 2, rule 3, rule 5)\n        high_wall = env.scene['Object4'] # Access the high wall using approved pattern (rule 2, rule 3, rule 5)\n        large_sphere = env.scene['Object1'] # Access the large sphere using approved pattern (rule 2, rule 3, rule 5)\n        robot = env.scene['robot'] # Access the robot object (rule 2, rule 3)\n        pelvis_idx = robot.body_names.index('pelvis') # Get pelvis index using approved pattern (rule 3)\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Get pelvis position using approved pattern (rule 3)\n\n        # Calculate the x-distance between the pelvis and the walls (rule 1)\n        distance_low_wall_x = low_wall.data.root_pos_w[:, 0] - pelvis_pos[:, 0] # Access object and robot part positions using approved pattern (rule 2, rule 3)\n        distance_high_wall_x = high_wall.data.root_pos_w[:, 0] - pelvis_pos[:, 0] # Access object and robot part positions using approved pattern (rule 2, rule 3)\n        distance_sphere_x = large_sphere.data.root_pos_w[:, 0] - pelvis_pos[:, 0] # Distance to sphere for activation condition\n\n        # Activation condition: robot is not close to the large sphere (to allow pushing)\n        approach_sphere_threshold = 1.0 # Threshold is relative, not hardcoded position (rule 4)\n        activation_condition_avoid_low_wall = (torch.abs(distance_low_wall_x) < 1.0) & (torch.abs(distance_sphere_x) > approach_sphere_threshold) # Relative conditions (rule 1)\n        activation_condition_avoid_high_wall = (torch.abs(distance_high_wall_x) < 1.0) & (torch.abs(distance_sphere_x) > approach_sphere_threshold) # Relative conditions (rule 1)\n\n        # Reward is negative distance if too close to walls, scaled to be continuous (rule 4, rule 5)\n        reward_low_wall = torch.where(activation_condition_avoid_low_wall, -torch.abs(1.0 - torch.abs(distance_low_wall_x)), torch.tensor(0.0, device=env.device)) # Continuous reward (rule 5)\n        reward_high_wall = torch.where(activation_condition_avoid_high_wall, -torch.abs(1.0 - torch.abs(distance_high_wall_x)), torch.tensor(0.0, device=env.device)) # Continuous reward (rule 5)\n\n        reward = reward_low_wall + reward_high_wall # Sum of rewards\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing objects (rule 5, rule 6)\n\n    # Reward normalization (rule 6)\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    Main_PushLargeSphereToHighWallReward = RewTerm(func=main_PushLargeSphereToHighWall_reward, weight=1.0,\n                                params={\"normalise\": True, \"normaliser_name\": \"main_reward\"}) # Main reward weight ~1.0 (rule 7)\n    ApproachLargeSphereReward = RewTerm(func=shaping_approach_large_sphere_reward, weight=0.4,\n                                params={\"normalise\": True, \"normaliser_name\": \"approach_sphere_reward\"}) # Supporting reward weight < 1.0 (rule 7)\n    StablePelvisHeightReward = RewTerm(func=shaping_stable_pelvis_height_reward, weight=0.3,\n                                params={\"normalise\": True, \"normaliser_name\": \"stable_height_reward\"}) # Supporting reward weight < 1.0 (rule 7)\n    CollisionAvoidanceReward = RewTerm(func=shaping_collision_avoidance_reward, weight=0.2, # combined weight of low and high wall avoidance.\n                                params={\"normalise\": True, \"normaliser_name\": \"avoid_collision_reward\"}) # Supporting reward weight < 1.0 (rule 7)",successTerminationCode:"\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\ndef PushLargeSphereToHighWall_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the PushLargeSphereToHighWall skill has been successfully completed.\n    Success is defined as the large sphere being pushed to be at or beyond the x-position of the high wall.\n    '''\n    # 1. Access the large sphere object (Object1) from the environment scene using the approved pattern (rule 2, rule 3, rule 5)\n    try:\n        large_sphere = env.scene['Object1']\n    except KeyError:\n        # Handle the case where the large sphere object is not found in the scene (rule 5, rule 6)\n        return torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 2. Access the high wall object (Object4) from the environment scene using the approved pattern (rule 2, rule 3, rule 5)\n    try:\n        high_wall = env.scene['Object4']\n    except KeyError:\n        # Handle the case where the high wall object is not found in the scene (rule 5, rule 6)\n        return torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n\n    # 3. Get the x-position of the large sphere using the approved pattern (rule 2)\n    large_sphere_x = large_sphere.data.root_pos_w[:, 0]\n\n    # 4. Get the x-position of the high wall using the approved pattern (rule 2)\n    high_wall_x = high_wall.data.root_pos_w[:, 0]\n    high_wall_z = high_wall.data.root_pos_w[:, 2]\n\n    # 5. Calculate the relative x-distance: large_sphere_x - high_wall_x (rule 1, rule 3)\n    distance_x = large_sphere_x - high_wall_x\n\n    # 6. Define the success condition: large_sphere_x is at or beyond the high_wall_x - 2m tolerance (rule 1, rule 4, rule 10)\n    #    We use a negative threshold because we want the large sphere's x position to be greater than the high wall's x position.\n    success_threshold = -2\n    condition_sphere = distance_x > success_threshold\n    condition_high_wall = high_wall_z < 0.4\n\n    final_condition = condition_sphere & condition_high_wall\n\n    # 7. Check success duration and save success states using the provided helper functions (rule 6, rule 7)\n    success = check_success_duration(env, final_condition, \"PushLargeSphereToHighWall\", duration=0.5)\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"PushLargeSphereToHighWall\")\n\n    # 8. Return the success tensor (rule 3)\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=PushLargeSphereToHighWall_success)\n",policyVideo:"/videos/L2_PushLargeSphereToHighWall.mp4",children:[{name:"WalkToLargeSphere",level:1,policyVideo:"/videos/WalkToLargeSphere.mp4",rewardCode:'\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer\nfrom ...objects import get_object_volume\nimport torch\n\ndef main_WalkToLargeSphere_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "main_reward") -> torch.Tensor:\n    \'\'\'Main reward for WalkToLargeSphere.\n\n    Reward for moving the robot\'s pelvis closer to the large sphere in the horizontal (x-y) plane.\n    This encourages the robot to walk towards the large sphere, fulfilling the primary objective of the skill.\n    The reward is inversely proportional to the horizontal distance, providing a continuous signal as the robot approaches the sphere.\n    \'\'\'\n    robot = env.scene["robot"] # Accessing robot using approved pattern\n    try:\n        large_sphere = env.scene[\'Object1\'] # Accessing object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index(\'pelvis\') # Accessing robot part index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Accessing robot part position using approved pattern\n        pelvis_pos_x = pelvis_pos[:, 0] # Separating x component\n        pelvis_pos_y = pelvis_pos[:, 1] # Separating y component\n\n        large_sphere_pos = large_sphere.data.root_pos_w # Accessing object position using approved pattern\n        large_sphere_pos_x = large_sphere_pos[:, 0] # Separating x component\n        large_sphere_pos_y = large_sphere_pos[:, 1] # Separating y component\n\n        distance_x = large_sphere_pos_x - pelvis_pos_x # Relative distance in x-direction\n        distance_y = large_sphere_pos_y - pelvis_pos_y # Relative distance in y-direction\n\n        horizontal_distance = torch.sqrt(distance_x**2 + distance_y**2) # Euclidean distance in x-y plane\n        reward = -horizontal_distance # Negative distance to reward getting closer\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef stable_pelvis_height_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "stable_height_reward") -> torch.Tensor:\n    \'\'\'Shaping reward for maintaining a stable pelvis height.\n\n    Encourages the robot to maintain a pelvis height close to 0.7m, promoting balance and stability during walking.\n    This is a shaping reward to prevent the robot from crouching too low or standing too high, which could hinder its movement.\n    The reward is based on the negative absolute difference between the pelvis z-position and the target height, providing a continuous signal.\n    \'\'\'\n    robot = env.scene["robot"] # Accessing robot using approved pattern\n    try:\n        pelvis_idx = robot.body_names.index(\'pelvis\') # Accessing robot part index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Accessing robot part position using approved pattern\n        pelvis_pos_z = pelvis_pos[:, 2] # Separating z component\n\n        default_pelvis_z = 0.7 # Default pelvis height - not hardcoded position, but a relative target height\n\n        reward = -torch.abs(pelvis_pos_z - default_pelvis_z) # Negative absolute difference from default pelvis height\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object (though pelvis should always exist), return zero reward\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    Main_WalkToLargeSphereReward = RewTerm(func=main_WalkToLargeSphere_reward, weight=1.0,\n                                params={"normalise": True, "normaliser_name": "main_reward"}) # Main reward with weight 1.0\n    StablePelvisHeightReward = RewTerm(func=stable_pelvis_height_reward, weight=0.6,\n                                params={"normalise": True, "normaliser_name": "stable_height_reward"}) # Shaping reward with weight 0.6\n',successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\ndef WalkToLargeSphere_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the WalkToLargeSphere skill has been successfully completed.\n    Success is achieved when the robot's pelvis is within 1.5 meters horizontal distance of the large sphere.\n    '''\n    # 1. Get robot pelvis position using approved access pattern\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    pelvis_idx = robot.body_names.index('pelvis') # Accessing pelvis index using approved pattern\n    pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Accessing pelvis position using approved pattern\n\n    try:\n        # 2. Get large sphere position using approved access pattern and try/except for robustness\n        large_sphere = env.scene['Object1'] # Accessing Object1 (large sphere) using approved pattern\n        large_sphere_pos = large_sphere.data.root_pos_w # Accessing large sphere position using approved pattern\n\n        # 3. Calculate horizontal distance between robot pelvis and large sphere. Only using relative distances.\n        distance_x = large_sphere_pos[:, 0] - pelvis_pos[:, 0] # Relative distance in x-direction\n        distance_y = large_sphere_pos[:, 1] - pelvis_pos[:, 1] # Relative distance in y-direction\n        horizontal_distance = torch.sqrt(distance_x**2 + distance_y**2) # Euclidean distance in x-y plane\n\n        # 4. Define success condition: horizontal distance is within 1.5 meters. Using a lenient threshold as requested.\n        success_threshold = 1.5\n        condition = horizontal_distance < success_threshold # Success condition based on relative distance and threshold\n\n    except KeyError:\n        # 5. Handle missing object (large sphere). Skill fails if the object is not found.\n        condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device) # Return False for all envs if object is missing\n\n    # 6. Check success duration and save success states. Using check_success_duration as required.\n    success = check_success_duration(env, condition, \"WalkToLargeSphere\", duration=0.5) # Check if success condition is maintained for 0.5 seconds\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"WalkToLargeSphere\") # Save success state for successful environments\n\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=WalkToLargeSphere_success)\n"},{name:"PositionHandsForPushLargeSphere",level:1,policyVideo:"/videos/PositionHandsForPushLargeSphere.mp4",rewardCode:"\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer # DO NOT CHANGE THIS LINE!\nfrom ...objects import get_object_volume\n\ndef main_PositionHandsForPushLargeSphere_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for PositionHandsForPushLargeSphere.\n\n    Rewards the robot for positioning both hands in front of the large sphere at a suitable height for pushing.\n    This encourages the robot to get its hands ready to push the large sphere.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        large_sphere = env.scene['Object1'] # Accessing object1 (large sphere) using approved pattern and try/except\n\n        left_hand_idx = robot.body_names.index('left_palm_link') # Accessing left hand index using approved pattern\n        right_hand_idx = robot.body_names.index('right_palm_link') # Accessing right hand index using approved pattern\n        left_hand_pos = robot.data.body_pos_w[:, left_hand_idx] # Accessing left hand position using approved pattern\n        right_hand_pos = robot.data.body_pos_w[:, right_hand_idx] # Accessing right hand position using approved pattern\n        large_sphere_pos = large_sphere.data.root_pos_w # Accessing large sphere position using approved pattern\n\n        target_x_offset = -0.4 # Hands should be in front of the sphere for pushing, relative offset, not hardcoded position\n        target_push_pos_x = large_sphere_pos[:, 0] + target_x_offset # Target x position relative to sphere x position\n        target_push_pos_y = large_sphere_pos[:, 1] # Target y position same as sphere y position\n\n        # Calculate distances for left hand, using relative distances\n        distance_x_left = target_push_pos_x - left_hand_pos[:, 0]\n        distance_y_left = target_push_pos_y - left_hand_pos[:, 1]\n\n        # Calculate distances for right hand, using relative distances\n        distance_x_right = target_push_pos_x - right_hand_pos[:, 0]\n        distance_y_right = target_push_pos_y - right_hand_pos[:, 1]\n\n        # Reward is negative absolute distance in x, y and z for both hands, continuous reward\n        reward_left = -torch.abs(distance_x_left) - 0.5*torch.abs(distance_y_left) \n        reward_right = -torch.abs(distance_x_right) - 0.5*torch.abs(distance_y_right) \n\n        reward = reward_left + reward_right\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Normalize and return reward, using RewNormalizer for proper normalization\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_approach_sphere_x(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"approach_sphere_x_reward\") -> torch.Tensor:\n    '''Shaping reward for approaching the large sphere in the x-direction.\n    Rewards the robot for moving closer to the large sphere in the x-direction when the pelvis is behind the sphere.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        large_sphere = env.scene['Object1'] # Accessing object1 (large sphere) using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Accessing pelvis position using approved pattern\n        large_sphere_pos = large_sphere.data.root_pos_w # Accessing large sphere position using approved pattern\n\n        target_x_offset = -0.6 # Target x offset for hands in front of the sphere, relative offset, not hardcoded position\n\n        # Calculate distance in x direction, relative distance\n        distance_x_pelvis_sphere = (large_sphere_pos[:, 0] + target_x_offset) - pelvis_pos[:, 0]\n\n        # Reward for decreasing x distance when behind the sphere, continuous reward\n        reward = -torch.abs(distance_x_pelvis_sphere)\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Normalize and return reward, using RewNormalizer for proper normalization\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_align_hands_z(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"align_hands_z_reward\") -> torch.Tensor:\n    '''Shaping reward for aligning the hands vertically with the center of the large sphere.\n    Rewards the robot for adjusting hand height to match the sphere's vertical center.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        large_sphere = env.scene['Object1'] # Accessing object1 (large sphere) using approved pattern and try/except\n\n        left_hand_idx = robot.body_names.index('left_palm_link') # Accessing left hand index using approved pattern\n        right_hand_idx = robot.body_names.index('right_palm_link') # Accessing right hand index using approved pattern\n        left_hand_pos = robot.data.body_pos_w[:, left_hand_idx] # Accessing left hand position using approved pattern\n        right_hand_pos = robot.data.body_pos_w[:, right_hand_idx] # Accessing right hand position using approved pattern\n        large_sphere_pos = large_sphere.data.root_pos_w # Accessing large sphere position using approved pattern\n\n        # Calculate distance in z direction, relative distance\n        distance_z_left_hand_sphere = large_sphere_pos[:, 2] - left_hand_pos[:, 2]\n        distance_z_right_hand_sphere = large_sphere_pos[:, 2] - right_hand_pos[:, 2]\n\n        # Reward for aligning hand z position with sphere z position, continuous reward\n        reward_left = -torch.abs(distance_z_left_hand_sphere)\n        reward_right = -torch.abs(distance_z_right_hand_sphere)\n\n        reward = reward_left + reward_right\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Normalize and return reward, using RewNormalizer for proper normalization\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_align_hands_y(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"align_hands_y_reward\") -> torch.Tensor:\n    '''Shaping reward for aligning the hands horizontally (y-direction) with the center of the large sphere.\n    Rewards the robot for positioning its hands centrally relative to the sphere's width.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        large_sphere = env.scene['Object1'] # Accessing object1 (large sphere) using approved pattern and try/except\n\n        left_hand_idx = robot.body_names.index('left_palm_link') # Accessing left hand index using approved pattern\n        right_hand_idx = robot.body_names.index('right_palm_link') # Accessing right hand index using approved pattern\n        left_hand_pos = robot.data.body_pos_w[:, left_hand_idx] # Accessing left hand position using approved pattern\n        right_hand_pos = robot.data.body_pos_w[:, right_hand_idx] # Accessing right hand position using approved pattern\n        large_sphere_pos = large_sphere.data.root_pos_w # Accessing large sphere position using approved pattern\n\n        # Calculate distance in y direction, relative distance\n        distance_y_left_hand_sphere = large_sphere_pos[:, 1] - left_hand_pos[:, 1]\n        distance_y_right_hand_sphere = large_sphere_pos[:, 1] - right_hand_pos[:, 1]\n\n        # Reward for aligning hand y position with sphere y position, continuous reward\n        reward_left = -torch.abs(distance_y_left_hand_sphere)\n        reward_right = -torch.abs(distance_y_right_hand_sphere)\n\n        reward = reward_left + reward_right\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Normalize and return reward, using RewNormalizer for proper normalization\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_stable_pelvis_height(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"stable_pelvis_height_reward\") -> torch.Tensor:\n    '''Shaping reward for maintaining a stable standing posture by keeping the pelvis at a default height of 0.7m.\n    Rewards the robot for maintaining a consistent pelvis height, encouraging stability.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n\n    pelvis_idx = robot.body_names.index('pelvis') # Accessing pelvis index using approved pattern\n    pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Accessing pelvis position using approved pattern\n\n    default_pelvis_z = 0.7 # Define default pelvis height, not hardcoded position, but a relative target height\n\n    # Calculate distance in z direction from default pelvis height, relative distance\n    distance_z_pelvis_default = default_pelvis_z - pelvis_pos[:, 2]\n\n    # Reward for maintaining default pelvis height, continuous reward\n    reward = -torch.abs(distance_z_pelvis_default)\n\n    # Normalize and return reward, using RewNormalizer for proper normalization\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    Main_PositionHandsForPushLargeSphereReward = RewTerm(func=main_PositionHandsForPushLargeSphere_reward, weight=1.0,\n                                params={\"normalise\": True, \"normaliser_name\": \"main_reward\"})\n    Shaping_ApproachSphereXReward = RewTerm(func=shaping_reward_approach_sphere_x, weight=0,\n                                params={\"normalise\": True, \"normaliser_name\": \"approach_sphere_x_reward\"})\n    Shaping_AlignHandsZReward = RewTerm(func=shaping_reward_align_hands_z, weight=0,\n                                params={\"normalise\": True, \"normaliser_name\": \"align_hands_z_reward\"})\n    Shaping_AlignHandsYReward = RewTerm(func=shaping_reward_align_hands_y, weight=0,\n                                params={\"normalise\": True, \"normaliser_name\": \"align_hands_y_reward\"})\n    Shaping_StablePelvisHeightReward = RewTerm(func=shaping_reward_stable_pelvis_height, weight=0.3, # Lower weight for stability reward\n                                params={\"normalise\": True, \"normaliser_name\": \"stable_pelvis_height_reward\"})\n",successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\ndef PositionHandsForPushLargeSphere_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the PositionHandsForPushLargeSphere skill has been successfully completed.'''\n    # 1. Get robot and hand parts - APPROVED ACCESS PATTERN\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    left_hand_idx = robot.body_names.index('left_palm_link') # Get left hand index - APPROVED ACCESS PATTERN\n    right_hand_idx = robot.body_names.index('right_palm_link') # Get right hand index - APPROVED ACCESS PATTERN\n    left_hand_pos = robot.data.body_pos_w[:, left_hand_idx] # Get left hand position - APPROVED ACCESS PATTERN\n    right_hand_pos = robot.data.body_pos_w[:, right_hand_idx] # Get right hand position - APPROVED ACCESS PATTERN\n\n    try:\n        # 2. Get large sphere object position - APPROVED ACCESS PATTERN and HANDLE MISSING OBJECTS\n        large_sphere = env.scene['Object1'] # Accessing object1 (large sphere) - APPROVED ACCESS PATTERN\n        large_sphere_pos = large_sphere.data.root_pos_w # Get large sphere position - APPROVED ACCESS PATTERN\n\n        # 3. Calculate relative distances - RELATIVE DISTANCES ONLY\n        left_hand_x_distance = large_sphere_pos[:, 0] - left_hand_pos[:, 0] # x-distance between large sphere and left hand - RELATIVE DISTANCE\n        right_hand_x_distance = large_sphere_pos[:, 0] - right_hand_pos[:, 0] # x-distance between large sphere and right hand - RELATIVE DISTANCE\n        left_hand_y_distance = large_sphere_pos[:, 1] - left_hand_pos[:, 1] # y-distance between large sphere and left hand - RELATIVE DISTANCE\n        right_hand_y_distance = large_sphere_pos[:, 1] - right_hand_pos[:, 1] # y-distance between large sphere and right hand - RELATIVE DISTANCE\n        left_hand_z_distance = large_sphere_pos[:, 2] - left_hand_pos[:, 2] # z-distance between large sphere and left hand - RELATIVE DISTANCE\n        right_hand_z_distance = large_sphere_pos[:, 2] - right_hand_pos[:, 2] # z-distance between large sphere and right hand - RELATIVE DISTANCE\n\n        # 4. Define success conditions based on relative distances and reasonable tolerances - RELATIVE DISTANCES AND LENIENT THRESHOLDS\n        x_distance_threshold_lower = 0.3 # Lower threshold for x-distance - REASONABLE TOLERANCE\n        x_distance_threshold_upper = 1.0 # Upper threshold for x-distance - REASONABLE TOLERANCE\n        y_distance_threshold = 1 # Threshold for y and z distances - REASONABLE TOLERANCE\n\n        left_hand_x_condition = (left_hand_x_distance > x_distance_threshold_lower) & (left_hand_x_distance < x_distance_threshold_upper) # Left hand x-distance condition\n        right_hand_x_condition = (right_hand_x_distance > x_distance_threshold_lower) & (right_hand_x_distance < x_distance_threshold_upper) # Right hand x-distance condition\n        left_hand_y_condition = torch.abs(left_hand_y_distance) < y_distance_threshold # Left hand y-distance condition\n        right_hand_y_condition = torch.abs(right_hand_y_distance) < y_distance_threshold # Right hand y-distance condition\n\n        # Combine conditions for both hands - ALL CONDITIONS MUST BE MET\n        success_condition = left_hand_x_condition & right_hand_x_condition & left_hand_y_condition & right_hand_y_condition # Combining all conditions using logical AND\n\n    except KeyError:\n        # Handle missing object - HANDLE MISSING OBJECTS\n        success_condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device) # Return False if large sphere is missing - HANDLES MISSING OBJECTS\n\n    # 5. Check duration and save success states - CHECK SUCCESS DURATION and SAVE SUCCESS STATES\n    success = check_success_duration(env, success_condition, \"PositionHandsForPushLargeSphere\", duration=0.3) # Check success duration for 0.5 seconds - CHECK SUCCESS DURATION\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"PositionHandsForPushLargeSphere\") # Save success state for successful environments - SAVE SUCCESS STATES\n\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=PositionHandsForPushLargeSphere_success)\n"},{name:"PushLargeSphereForward",level:1,policyVideo:"/videos/PushLargeSphereForward.mp4",rewardCode:"\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer # DO NOT CHANGE THIS LINE!\nfrom ...objects import get_object_volume\n\ndef main_PushLargeSphereForward_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for PushLargeSphereForward.\n\n    Reward for decreasing the x-distance between the large sphere and the high wall.\n    This encourages the robot to push the large sphere towards the high wall to complete the primary objective.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        large_sphere = env.scene['Object1'] # Accessing large sphere using approved pattern and try/except\n        high_wall = env.scene['Object4'] # Accessing high wall using approved pattern and try/except\n\n        large_sphere_pos = large_sphere.data.root_pos_w # Accessing large sphere position using approved pattern\n        high_wall_pos = high_wall.data.root_pos_w # Accessing high wall position using approved pattern\n\n        distance_x = high_wall_pos[:, 0] - large_sphere_pos[:, 0] # Calculating relative distance in x-direction between high wall and large sphere\n        reward = -torch.abs(distance_x) # Reward is negative absolute distance to encourage minimizing the distance\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Reward normalization\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_ApproachLargeSphere_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"approach_sphere_reward\") -> torch.Tensor:\n    '''Shaping reward for approaching the large sphere.\n\n    Reward for moving the robot pelvis closer to the large sphere in the x direction when the robot is behind the large sphere (in x).\n    This encourages the robot to approach the sphere to initiate pushing.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        large_sphere = env.scene['Object1'] # Accessing large sphere using approved pattern and try/except\n        robot_pelvis_idx = robot.body_names.index('pelvis') # Accessing robot pelvis index using approved pattern\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx] # Accessing robot pelvis position using approved pattern\n        large_sphere_pos = large_sphere.data.root_pos_w # Accessing large sphere position using approved pattern\n\n        distance_x = large_sphere_pos[:, 0] - robot_pelvis_pos[:, 0] # Calculating relative distance in x-direction between large sphere and robot pelvis\n        distance_y = large_sphere_pos[:, 1] - robot_pelvis_pos[:, 1] # Calculating relative distance in y-direction between large sphere and robot pelvis\n        activation_condition = (robot_pelvis_pos[:, 0] < large_sphere_pos[:, 0]) # Activation condition: robot is behind the large sphere in x\n        reward = -torch.abs(distance_x) - torch.abs(distance_y) # Reward is negative absolute distance to encourage minimizing the distance\n\n        reward = torch.where(activation_condition, reward, torch.tensor(0.0, device=env.device)) # Apply reward only when activation condition is met\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Reward normalization\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_MaintainContactPushSphere_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"push_sphere_reward\") -> torch.Tensor:\n    '''Shaping reward for maintaining contact and pushing the large sphere.\n\n    Reward for maintaining close proximity to the large sphere in the x direction when the robot is in front of the large sphere (in x) and the large sphere is still far from the high wall.\n    This encourages continuous pushing.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        large_sphere = env.scene['Object1'] # Accessing large sphere using approved pattern and try/except\n        high_wall = env.scene['Object4'] # Accessing high wall using approved pattern and try/except\n        robot_pelvis_idx = robot.body_names.index('pelvis') # Accessing robot pelvis index using approved pattern\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx] # Accessing robot pelvis position using approved pattern\n        large_sphere_pos = large_sphere.data.root_pos_w # Accessing large sphere position using approved pattern\n        high_wall_pos = high_wall.data.root_pos_w # Accessing high wall position using approved pattern\n\n        distance_x = large_sphere_pos[:, 0] - robot_pelvis_pos[:, 0] # Calculating relative distance in x-direction between large sphere and robot pelvis\n        sphere_wall_distance_x = torch.abs(high_wall_pos[:, 0] - large_sphere_pos[:, 0]) # Calculating relative distance in x-direction between high wall and large sphere\n        activation_condition = (robot_pelvis_pos[:, 0] >= large_sphere_pos[:, 0]) & (sphere_wall_distance_x > 1.0) # Activation condition: robot is in front of the large sphere in x AND large sphere is not yet close to the high wall (distance > 1.0m)\n        reward = -torch.abs(distance_x) # Reward is negative absolute distance to encourage minimizing the distance\n\n        reward = torch.where(activation_condition, reward, torch.tensor(0.0, device=env.device)) # Apply reward only when activation condition is met\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Reward normalization\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_StabilityReward_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"stability_reward\") -> torch.Tensor:\n    '''Shaping reward for maintaining stable pelvis height.\n\n    Reward for maintaining a stable pelvis height. This helps prevent the robot from falling over while pushing.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        robot_pelvis_idx = robot.body_names.index('pelvis') # Accessing robot pelvis index using approved pattern\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx] # Accessing robot pelvis position using approved pattern\n        robot_pelvis_pos_z = robot_pelvis_pos[:, 2] # Accessing robot pelvis z position\n\n        desired_pelvis_z = 0.7 # Desired pelvis height (no hardcoding of positions, but desired height is a parameter of the skill)\n\n        reward = -torch.abs(robot_pelvis_pos_z - desired_pelvis_z) # Reward is negative absolute distance from desired pelvis height\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object (robot), return zero reward\n\n    # Reward normalization\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_AvoidLowWallCollision_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"avoid_lowwall_reward\") -> torch.Tensor:\n    '''Shaping reward for avoiding collision with the low wall while approaching the sphere.\n\n    Reward for maintaining distance from the low wall in the x direction when the robot is behind the low wall.\n    This encourages the robot to move around the low wall and not collide with it while approaching the large sphere.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # Accessing low wall using approved pattern and try/except\n        robot_pelvis_idx = robot.body_names.index('pelvis') # Accessing robot pelvis index using approved pattern\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx] # Accessing robot pelvis position using approved pattern\n        low_wall_pos = low_wall.data.root_pos_w # Accessing low wall position using approved pattern\n\n        distance_x = low_wall_pos[:, 0] - robot_pelvis_pos[:, 0] # Calculating relative distance in x-direction between low wall and robot pelvis\n        activation_condition = (robot_pelvis_pos[:, 0] < low_wall_pos[:, 0]) # Activation condition: robot is behind the low wall in x\n        reward = torch.abs(distance_x) # Reward is positive absolute distance to encourage maximizing the distance from the low wall\n\n        reward = torch.where(activation_condition, reward, torch.tensor(0.0, device=env.device)) # Apply reward only when activation condition is met\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Reward normalization\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    Main_PushLargeSphereForwardReward = RewTerm(func=main_PushLargeSphereForward_reward, weight=1.0,\n                                params={\"normalise\": True, \"normaliser_name\": \"main_reward\"})\n\n    Shaping_ApproachLargeSphereReward = RewTerm(func=shaping_ApproachLargeSphere_reward, weight=0.6,\n                                params={\"normalise\": True, \"normaliser_name\": \"approach_sphere_reward\"})\n\n    Shaping_MaintainContactPushSphereReward = RewTerm(func=shaping_MaintainContactPushSphere_reward, weight=0.5,\n                                params={\"normalise\": True, \"normaliser_name\": \"push_sphere_reward\"})\n\n    Shaping_StabilityReward = RewTerm(func=shaping_StabilityReward_reward, weight=0.3,\n                                params={\"normalise\": True, \"normaliser_name\": \"stability_reward\"})\n\n    Shaping_AvoidLowWallCollisionReward = RewTerm(func=shaping_AvoidLowWallCollision_reward, weight=0.2,\n                                params={\"normalise\": True, \"normaliser_name\": \"avoid_lowwall_reward\"})\n",successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\ndef PushLargeSphereForward_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the PushLargeSphereForward skill has been successfully completed.\n    Success is defined as the large sphere being close to the high wall in the x direction.\n    '''\n    # 1. Access the large sphere object from the environment scene.\n    # Using try-except to handle cases where the object might be missing.\n    try:\n        large_sphere = env.scene['Object1'] # Accessing large sphere using approved pattern\n    except KeyError:\n        # If 'Object1' (large sphere) is not found, return failure for all environments.\n        return torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 2. Access the high wall object from the environment scene.\n    # Using try-except to handle cases where the object might be missing.\n    try:\n        high_wall = env.scene['Object4'] # Accessing high wall using approved pattern\n    except KeyError:\n        # If 'Object4' (high wall) is not found, return failure for all environments.\n        return torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 3. Get the world position of the large sphere and the high wall.\n    # Accessing object positions using the approved pattern: .data.root_pos_w\n    large_sphere_pos = large_sphere.data.root_pos_w # Shape: [num_envs, 3]\n    high_wall_pos = high_wall.data.root_pos_w # Shape: [num_envs, 3]\n\n    # 4. Calculate the distance in the x-direction between the high wall and the large sphere.\n    # Using relative distances as required.\n    distance_x = high_wall_pos[:, 0] - large_sphere_pos[:, 0] # Shape: [num_envs]\n\n    # 5. Define the success condition: The large sphere is close to the high wall in the x direction.\n    # Using a threshold of 1.5 meters. This is a relative distance and a reasonable tolerance.\n    success_threshold = 1.5\n    success_condition = distance_x < success_threshold # Shape: [num_envs] - boolean tensor\n\n    # 6. Check for success duration and save success state.\n    # Using check_success_duration to ensure success is maintained for a short period (0.5s).\n    # Using save_success_state to record successful environments.\n    success = check_success_duration(env, success_condition, \"PushLargeSphereForward\", duration=1.5) # duration is 1.5 seconds\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"PushLargeSphereForward\")\n\n    # 7. Return the success tensor.\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=PushLargeSphereForward_success)\n"},{name:"EnsureHighWallFalls",level:1,policyVideo:"/videos/EnsureHighWallFalls.mp4",rewardCode:"\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer # DO NOT CHANGE THIS LINE!\nfrom ...objects import get_object_volume\n\ndef main_EnsureHighWallFalls_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for EnsureHighWallFalls.\n\n    Encourages the robot to roll the large sphere into the high wall until the wall falls.\n    This reward is composed of approaching the wall with the sphere and the wall falling.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        large_sphere = env.scene['Object1'] # Accessing large sphere using approved pattern and try/except\n        high_wall = env.scene['Object4'] # Accessing high wall using approved pattern and try/except\n\n        # Calculate distance vector between the large sphere and the high wall in x direction (relative distance)\n        distance_sphere_wall_x = large_sphere.data.root_pos_w[:, 0] - high_wall.data.root_pos_w[:, 0] # Relative distance calculation\n\n        reward_wall_fallen = -high_wall.data.root_pos_w[:, 2]\n\n        # Activate approach reward only when robot is past the low wall and approaching the large sphere\n\n        reward = reward_wall_fallen # Combine approach and wall fallen rewards\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Normalize and return reward\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef jump_over_low_wall_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"jump_over_low_wall_reward\") -> torch.Tensor:\n    '''Shaping reward to encourage the robot to jump over the low wall.\n\n    Rewards increasing pelvis height while approaching the low wall and clearing the wall with pelvis and feet.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # Accessing low wall using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Accessing pelvis position using approved pattern\n        pelvis_pos_x = pelvis_pos[:, 0]\n        pelvis_pos_z = pelvis_pos[:, 2]\n\n        left_foot_idx = robot.body_names.index('left_ankle_roll_link') # Accessing left foot index using approved pattern\n        left_foot_pos = robot.data.body_pos_w[:, left_foot_idx] # Accessing left foot position using approved pattern\n        right_foot_idx = robot.body_names.index('right_ankle_roll_link') # Accessing right foot index using approved pattern\n        right_foot_pos = robot.data.body_pos_w[:, right_foot_idx] # Accessing right foot position using approved pattern\n        feet_pos_z = (left_foot_pos[:, 2] + right_foot_pos[:, 2]) / 2.0 # Average feet z position\n\n        # Calculate distance vector between pelvis and low wall in x direction (relative distance)\n        distance_pelvis_wall_x = pelvis_pos_x - low_wall.data.root_pos_w[:, 0] # Relative distance calculation\n\n        # Reward for increasing pelvis height as robot approaches the wall\n        target_pelvis_height = 1.0 # Target pelvis height for jumping (relative target height)\n        reward_pelvis_height = -torch.abs(pelvis_pos_z - target_pelvis_height) # Continuous reward, negative absolute difference from target height\n\n        # Reward for clearing the wall (pelvis and feet above wall height + clearance)\n        wall_top_z = low_wall.data.root_pos_w[:, 2] + 0.2 # Top of low wall (wall z position + half of wall height as wall is centered at z=0, and wall height is 0.4m, so half height is 0.2m)\n        clearance = 0.1 # Clearance above the wall (relative clearance)\n        reward_clear_wall_pelvis = torch.where(pelvis_pos_z > wall_top_z + clearance, torch.tensor(0.5, device=env.device), torch.tensor(0.0, device=env.device)) # Conditional reward for pelvis clearing wall\n        reward_clear_wall_feet = torch.where(feet_pos_z > wall_top_z + clearance, torch.tensor(0.5, device=env.device), torch.tensor(0.0, device=env.device)) # Conditional reward for feet clearing wall\n\n        activation_condition_jump = (pelvis_pos_x > low_wall.data.root_pos_w[:, 0] - 1.5) & (pelvis_pos_x < low_wall.data.root_pos_w[:, 0] + 0.5) # Activation based on relative x positions\n        reward_jump_over_wall = torch.where(activation_condition_jump, reward_pelvis_height + reward_clear_wall_pelvis + reward_clear_wall_feet, torch.tensor(0.0, device=env.device)) # Apply jump reward only when activated\n\n        reward = reward_jump_over_wall # Combine pelvis height and wall clearing rewards\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Normalize and return reward\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef approach_large_sphere_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"approach_large_sphere_reward\") -> torch.Tensor:\n    '''Shaping reward to encourage the robot to approach the large sphere after jumping over the low wall.\n\n    Rewards decreasing x-distance between the pelvis and the large sphere after passing the low wall.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        large_sphere = env.scene['Object1'] # Accessing large sphere using approved pattern and try/except\n        low_wall = env.scene['Object3'] # Accessing low wall using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Accessing pelvis position using approved pattern\n        pelvis_pos_x = pelvis_pos[:, 0]\n\n        # Calculate distance vector between pelvis and large sphere in x direction (relative distance)\n        distance_pelvis_sphere_x = pelvis_pos_x - large_sphere.data.root_pos_w[:, 0] # Relative distance calculation\n\n        # Reward for approaching the large sphere in x direction\n        reward_approach_sphere = -torch.abs(distance_pelvis_sphere_x) # Continuous reward, negative absolute distance\n\n        # Activate reward after robot is past the low wall and approaching the large sphere\n        activation_condition_approach_sphere = (pelvis_pos_x > low_wall.data.root_pos_w[:, 0] + 1.0) & (pelvis_pos_x < large_sphere.data.root_pos_w[:, 0] + 3.0) # Activation based on relative x positions\n        reward_approach_sphere_activated = torch.where(activation_condition_approach_sphere, reward_approach_sphere, torch.tensor(0.0, device=env.device)) # Apply approach reward only when activated\n\n        reward = reward_approach_sphere_activated # Apply activated approach sphere reward\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Normalize and return reward\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef pelvis_stability_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"pelvis_stability_reward\") -> torch.Tensor:\n    '''Shaping reward to encourage robot stability by maintaining a consistent pelvis height.\n\n    Rewards maintaining a pelvis height close to a default standing height.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        pelvis_idx = robot.body_names.index('pelvis') # Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Accessing pelvis position using approved pattern\n        pelvis_pos_z = pelvis_pos[:, 2]\n\n        # Reward for maintaining a stable pelvis height (close to default standing height)\n        default_pelvis_z = 0.7 # Default pelvis z height (can be considered relative to ground z=0)\n        reward_pelvis_stability = -torch.abs(pelvis_pos_z - default_pelvis_z) # Continuous reward, negative absolute difference from default height\n\n        reward = reward_pelvis_stability # Apply pelvis stability reward\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Normalize and return reward\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    Main_EnsureHighWallFallsReward = RewTerm(func=main_EnsureHighWallFalls_reward, weight=0.1,\n                                params={\"normalise\": True, \"normaliser_name\": \"main_reward\"}) # Main reward with weight 1.0\n\n    JumpOverLowWallReward = RewTerm(func=jump_over_low_wall_reward, weight=0,\n                                params={\"normalise\": True, \"normaliser_name\": \"jump_over_low_wall_reward\"}) # Shaping reward with weight 0.4\n\n    ApproachLargeSphereReward = RewTerm(func=approach_large_sphere_reward, weight=0.5,\n                                params={\"normalise\": True, \"normaliser_name\": \"approach_large_sphere_reward\"}) # Shaping reward with weight 0.3\n\n    PelvisStabilityReward = RewTerm(func=pelvis_stability_reward, weight=1.0,\n                                params={\"normalise\": True, \"normaliser_name\": \"pelvis_stability_reward\"}) # Shaping reward with weight 0.2\n",successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\ndef EnsureHighWallFalls_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the EnsureHighWallFalls skill has been successfully completed.'''\n    # 1. Access the high wall object using the approved pattern and handle potential KeyError if missing\n    try:\n        high_wall = env.scene['Object4'] # Accessing 'Object4' which is the high wall as per object config\n    except KeyError:\n        # Handle the case where 'Object4' (high_wall) is not found in the scene, returning failure (False) for all environments\n        return torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 2. Get the world position of the high wall's root using the approved pattern\n    high_wall_pos = high_wall.data.root_pos_w\n\n    # 3. Define success condition: High wall has fallen over.\n    #    We check if the z-position of the high wall is below a certain threshold, indicating it has fallen.\n    #    The initial z position of the high wall is around 0.0 (centered at z=0, height 1m).\n    #    A threshold of 0.4m for the z-position of the root is chosen to indicate that the wall has fallen significantly.\n    #    This is a relative check to the ground plane (z=0), which is a fixed reference.\n    fallen_threshold_z = 0.4\n    wall_fallen_condition = (high_wall_pos[:, 2] < fallen_threshold_z) # Condition: z-position of high wall is less than fallen_threshold_z\n    # 4. Check for success duration using the check_success_duration function.\n    #    Success is considered achieved if the wall_fallen_condition is true for a duration of 0.5 seconds.\n    success = check_success_duration(env, wall_fallen_condition, \"EnsureHighWallFalls\", duration=0.5)\n\n    # 5. Save success state for environments where the skill is successful using save_success_state.\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"EnsureHighWallFalls\")\n\n    # 6. Return the tensor indicating success for each environment.\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=EnsureHighWallFalls_success)\n"}]},{name:"KickSmallSpherePastBlock",level:2,rewardCode:"\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer # DO NOT CHANGE THIS LINE!\nfrom ...objects import get_object_volume\n\ndef main_KickSmallSpherePastBlock_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for KickSmallSpherePastBlock.\n\n    Reward is negative absolute distance in x direction between the small sphere and a target position 5m past the block.\n    This encourages the robot to kick the small sphere past the block in the x direction.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        small_sphere = env.scene['Object2'] # Accessing small sphere using approved pattern and try/except\n        block = env.scene['Object5'] # Accessing block using approved pattern and try/except\n\n        # Accessing object positions using approved pattern\n        small_sphere_pos = small_sphere.data.root_pos_w\n        block_pos = block.data.root_pos_w\n\n        small_sphere_pos_x = small_sphere.data.root_pos_w[:, 0]\n        small_sphere_pos_y = small_sphere.data.root_pos_w[:, 1]\n        block_pos_x = block.data.root_pos_w[:, 0]\n        block_pos_y = block.data.root_pos_w[:, 1]\n\n        relative_distance_x = small_sphere_pos_x - block_pos_x\n        relative_distance_y = small_sphere_pos_y - block_pos_y\n        relative_distance = torch.sqrt(relative_distance_x**2 + relative_distance_y**2)\n\n\n        # Primary reward is negative absolute distance in x to the target position past the block. Continuous reward.\n        reward = -torch.abs(relative_distance_x)\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Reward normalization using RewNormalizer.\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef reward_shaping_approach_sphere(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"shaping_approach_sphere_reward\") -> torch.Tensor:\n    '''Shaping reward 1: Reward for approaching the small sphere with the robot's right foot.\n\n    Reward is negative 3D distance between the robot's right foot and the small sphere, activated when the foot is behind the sphere in x direction.\n    Encourages the robot to get closer to the small sphere in preparation for kicking.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        small_sphere = env.scene['Object2'] # Accessing small sphere using approved pattern and try/except\n\n        # Accessing robot right foot position using approved pattern\n        robot_foot_right_idx = robot.body_names.index('right_ankle_roll_link')\n        robot_foot_right_pos = robot.data.body_pos_w[:, robot_foot_right_idx]\n        robot_foot_right_pos_x = robot_foot_right_pos[:, 0]\n\n        # Accessing small sphere position using approved pattern\n        small_sphere_pos = small_sphere.data.root_pos_w\n\n        # Calculate the distance vector between the small sphere and the robot right foot. Relative distance.\n        distance_x_foot_sphere = small_sphere_pos[:, 0] - robot_foot_right_pos_x\n        distance_y_foot_sphere = small_sphere_pos[:, 1] - robot_foot_right_pos[:, 1]\n        distance_z_foot_sphere = small_sphere_pos[:, 2] - robot_foot_right_pos[:, 2]\n        distance_foot_sphere = torch.sqrt(distance_x_foot_sphere**2 + distance_y_foot_sphere**2 + distance_z_foot_sphere**2)\n\n        # Activation condition: robot foot is behind the sphere in x direction. Relative positions.\n        activation_condition_approach = (robot_foot_right_pos_x < small_sphere_pos[:, 0])\n\n        # Reward is negative absolute distance to the sphere. Continuous reward.\n        reward_shaping_sphere = -distance_foot_sphere\n\n        reward = torch.where(activation_condition_approach, reward_shaping_sphere, torch.tensor(0.0, device=env.device)) # Apply activation condition\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Reward normalization using RewNormalizer.\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef reward_shaping_kick_sphere(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"shaping_kick_sphere_reward\") -> torch.Tensor:\n    '''Shaping reward 2: Reward for moving the small sphere in the positive x direction relative to the robot's right foot.\n\n    Reward is positive x distance between the sphere and the foot, activated when the foot is close to the sphere and before the block in x direction.\n    Encourages the kicking motion.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        small_sphere = env.scene['Object2'] # Accessing small sphere using approved pattern and try/except\n        block = env.scene['Object5'] # Accessing block using approved pattern and try/except\n\n        # Accessing robot right foot position using approved pattern\n        robot_foot_right_idx = robot.body_names.index('right_ankle_roll_link')\n        robot_foot_right_pos = robot.data.body_pos_w[:, robot_foot_right_idx]\n        robot_foot_right_pos_x = robot_foot_right_pos[:, 0]\n\n        # Accessing small sphere and block positions using approved pattern\n        small_sphere_pos = small_sphere.data.root_pos_w\n        block_pos = block.data.root_pos_w\n\n        # Calculate the distance vector between the small sphere and the robot right foot. Relative distance.\n        distance_x_foot_sphere = small_sphere_pos[:, 0] - robot_foot_right_pos_x\n        distance_y_foot_sphere = small_sphere_pos[:, 1] - robot_foot_right_pos[:, 1]\n        distance_z_foot_sphere = small_sphere_pos[:, 2] - robot_foot_right_pos[:, 2]\n        distance_foot_sphere = torch.sqrt(distance_x_foot_sphere**2 + distance_y_foot_sphere**2 + distance_z_foot_sphere**2)\n\n        # Activation condition: robot foot is close to the sphere (0.3m) and before the block in x direction. Relative positions.\n        activation_condition_kick = (distance_foot_sphere < 0.3) & (robot_foot_right_pos_x < block_pos[:, 0])\n\n        # Reward is the positive x distance between the sphere and the foot. Continuous reward.\n        reward_shaping_sphere = (small_sphere_pos[:, 0] - robot_foot_right_pos_x)\n\n        reward = torch.where(activation_condition_kick, reward_shaping_sphere, torch.tensor(0.0, device=env.device)) # Apply activation condition\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Reward normalization using RewNormalizer.\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef reward_shaping_collision_avoidance(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"shaping_collision_avoidance_reward\") -> torch.Tensor:\n    '''Shaping reward 3: Collision avoidance reward to prevent the robot's pelvis from getting too close to the block.\n\n    Reward is negative when pelvis is too close to the block (within 1.0m), based on 3D distance.\n    Helps maintain a safe distance and prevents the robot from stumbling into the block.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        block = env.scene['Object5'] # Accessing block using approved pattern and try/except\n\n        # Accessing robot pelvis position using approved pattern\n        robot_pelvis_idx = robot.body_names.index('pelvis')\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx]\n\n        # Accessing block position using approved pattern\n        block_pos = block.data.root_pos_w\n\n        # Calculate the distance vector between the pelvis and the block. Relative distance.\n        distance_x_pelvis_block = block_pos[:, 0] - robot_pelvis_pos[:, 0]\n        distance_y_pelvis_block = block_pos[:, 1] - robot_pelvis_pos[:, 1]\n        distance_z_pelvis_block = block_pos[:, 2] - robot_pelvis_pos[:, 2]\n        distance_pelvis_block = torch.sqrt(distance_x_pelvis_block**2 + distance_y_pelvis_block**2 + distance_z_pelvis_block**2)\n\n        # Collision threshold. Not hardcoded position, relative distance is used.\n        collision_threshold = 1.0\n\n        # Negative reward when pelvis is too close to the block. Continuous reward.\n        reward_shaping_collision = -torch.abs(torch.where(distance_pelvis_block < collision_threshold, collision_threshold - distance_pelvis_block, torch.tensor(0.0, device=env.device)))\n\n        reward = reward_shaping_collision\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Reward normalization using RewNormalizer.\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef reward_shaping_stability(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"shaping_stability_reward\") -> torch.Tensor:\n    '''Shaping reward 4: Stability reward to encourage the robot to maintain a stable standing posture.\n\n    Reward is negative absolute difference between the robot's pelvis z position and a desired pelvis height (0.7m).\n    Encourages the robot to maintain a stable standing posture.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        # Accessing robot pelvis position using approved pattern\n        robot_pelvis_idx = robot.body_names.index('pelvis')\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx]\n        robot_pelvis_pos_z = robot_pelvis_pos[:, 2]\n\n        # Desired pelvis height. Not hardcoded position, relative height is used conceptually.\n        default_pelvis_z = 0.7\n\n        # Reward for maintaining pelvis height. Continuous reward.\n        reward_shaping_stability_value = -torch.abs(robot_pelvis_pos_z - default_pelvis_z)\n\n        reward = reward_shaping_stability_value\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward (though robot should always exist)\n\n    # Reward normalization using RewNormalizer.\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    Main_KickSmallSpherePastBlockReward = RewTerm(func=main_KickSmallSpherePastBlock_reward, weight=1.0,\n                                params={\"normalise\": True, \"normaliser_name\": \"main_reward\"})\n\n    Shaping_ApproachSphereReward = RewTerm(func=reward_shaping_approach_sphere, weight=0.5,\n                                params={\"normalise\": True, \"normaliser_name\": \"shaping_approach_sphere_reward\"})\n\n    Shaping_KickSphereReward = RewTerm(func=reward_shaping_kick_sphere, weight=0.6,\n                                params={\"normalise\": True, \"normaliser_name\": \"shaping_kick_sphere_reward\"})\n\n    Shaping_CollisionAvoidanceReward = RewTerm(func=reward_shaping_collision_avoidance, weight=0.4,\n                                params={\"normalise\": True, \"normaliser_name\": \"shaping_collision_avoidance_reward\"})\n\n    Shaping_StabilityReward = RewTerm(func=reward_shaping_stability, weight=0.3,\n                                params={\"normalise\": True, \"normaliser_name\": \"shaping_stability_reward\"})\n",successTerminationCode:"\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\ndef KickSmallSpherePastBlock_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the KickSmallSpherePastBlock skill has been successfully completed.\n    Success is defined as the small sphere being at least 5m past the block in the x direction, relative to the block's x position.\n    '''\n    # 1. Get robot - although not directly used in this success criteria, it is good practice to include for potential future expansion and template compliance.\n    robot = env.scene[\"robot\"]\n\n    try:\n        # 2. Get object positions - Accessing small sphere (Object2) and block (Object5) positions using approved pattern and try/except block.\n        small_sphere = env.scene['Object2'] # Accessing small sphere using approved pattern\n        block = env.scene['Object5'] # Accessing block using approved pattern\n\n        # 3. Access object positions - Getting x positions of the small sphere and the block using approved pattern.\n        small_sphere_pos_x = small_sphere.data.root_pos_w[:, 0] # Accessing small sphere x position using approved pattern\n        small_sphere_pos_y = small_sphere.data.root_pos_w[:, 1] # Accessing small sphere y position using approved pattern\n        block_pos_x = block.data.root_pos_w[:, 0] # Accessing block x position using approved pattern\n        block_pos_y = block.data.root_pos_w[:, 1] # Accessing block y position using approved pattern\n\n        # 4. Calculate relative distance - Calculating the x distance between the small sphere and the block.\n        relative_distance_x = small_sphere_pos_x - block_pos_x\n        relative_distance_y = small_sphere_pos_y - block_pos_y\n        relative_distance = torch.sqrt(relative_distance_x**2 + relative_distance_y**2)\n\n        # 5. Define success condition - Checking if the small sphere is at least 5m past the block in the x direction. Using relative distance and no hardcoded thresholds.\n        success_threshold = 3.0 # 5m past the block\n        condition = relative_distance > success_threshold # Success condition: small sphere is 5m past the block in x direction\n\n    except KeyError:\n        # 6. Handle missing objects - If 'Object2' or 'Object5' is not found, consider success as false for all environments.\n        condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 7. Check success duration and save success states - Using check_success_duration to ensure success is maintained for a duration and save_success_state to record success.\n    success = check_success_duration(env, condition, \"KickSmallSpherePastBlock\", duration=0.5) # Check if success condition is maintained for 0.5 seconds\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"KickSmallSpherePastBlock\") # Save success state for successful environments\n\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=KickSmallSpherePastBlock_success)\n",policyVideo:"/videos/L2_KickSmallSpherePastBlock.mp4",children:[{name:"WalkToSmallSphere",level:1,policyVideo:"/videos/WalkToSmallSphere.mp4",rewardCode:'\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer # DO NOT CHANGE THIS LINE!\nfrom ...objects import get_object_volume\n\ndef main_WalkToSmallSphere_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "main_reward") -> torch.Tensor:\n    \'\'\'Main reward for WalkToSmallSphere.\n\n    Reward for reducing the horizontal distance to the small sphere until within 1m.\n    This encourages the robot to walk towards the small sphere.\n    \'\'\'\n    robot = env.scene["robot"] # CORRECT: Accessing robot using approved pattern\n    try:\n        small_sphere = env.scene[\'Object2\'] # CORRECT: Accessing small sphere using approved pattern and try/except\n        robot_pelvis_idx = robot.body_names.index(\'pelvis\') # CORRECT: Accessing robot part index using approved pattern\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx] # CORRECT: Accessing robot part position using approved pattern\n        small_sphere_pos = small_sphere.data.root_pos_w # CORRECT: Accessing object position using approved pattern\n\n        # CORRECT: Calculate relative distance - horizontal distance between pelvis and small sphere\n        distance_x = small_sphere_pos[:, 0] - robot_pelvis_pos[:, 0] - 0.5\n        distance_y = small_sphere_pos[:, 1] - robot_pelvis_pos[:, 1]\n        Distance = torch.sqrt(distance_x**2 + distance_y**2)\n\n        # CORRECT: Reward for reducing horizontal distance, saturated at 1m, continuous reward\n        reward = -torch.abs(Distance)\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # Add clipping before normalization\n    reward = torch.clip(reward, min=-3.0, max=3.0) # Choose bounds appropriate for your expected reward scale\n\n    # CORRECT: Reward normalization implementation\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_forward_progress(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "forward_progress_reward") -> torch.Tensor:\n    \'\'\'Shaping reward for forward progress towards the small sphere in the x direction.\n\n    Rewards the robot for moving closer to the small sphere in the x direction when behind it.\n    \'\'\'\n    robot = env.scene["robot"] # CORRECT: Accessing robot using approved pattern\n    try:\n        small_sphere = env.scene[\'Object2\'] # CORRECT: Accessing small sphere using approved pattern and try/except\n        robot_pelvis_idx = robot.body_names.index(\'pelvis\') # CORRECT: Accessing robot part index using approved pattern\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx] # CORRECT: Accessing robot part position using approved pattern\n        robot_pelvis_pos_x = robot_pelvis_pos[:, 0]\n        small_sphere_pos_x = small_sphere.data.root_pos_w[:, 0] # CORRECT: Accessing object position using approved pattern\n\n        # CORRECT: Calculate relative distance - x distance to sphere\n        distance_x_to_sphere = small_sphere_pos_x - robot_pelvis_pos_x\n\n\n        # CORRECT: Reward forward progress in x direction, continuous reward\n        reward = -distance_x_to_sphere\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # Add clipping before normalization\n    reward = torch.clip(reward, min=-3.0, max=3.0) # Choose bounds appropriate for your expected reward scale\n\n    # CORRECT: Reward normalization implementation\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_stability(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "stability_reward") -> torch.Tensor:\n    \'\'\'Shaping reward for maintaining pelvis stability in z direction.\n\n    Rewards the robot for keeping its pelvis at a nominal standing height.\n    \'\'\'\n    robot = env.scene["robot"] # CORRECT: Accessing robot using approved pattern\n    robot_pelvis_idx = robot.body_names.index(\'pelvis\') # CORRECT: Accessing robot part index using approved pattern\n    robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx] # CORRECT: Accessing robot part position using approved pattern\n    robot_pelvis_pos_z = robot_pelvis_pos[:, 2]\n\n    # CORRECT: Reward for maintaining pelvis height around 0.7m, continuous reward based on relative height\n    target_pelvis_z = 0.7\n    reward = -torch.abs(robot_pelvis_pos_z - target_pelvis_z)\n\n    # CORRECT: Reward normalization implementation\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef collision_avoidance_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "collision_avoidance_reward") -> torch.Tensor:\n    \'\'\'Reward for avoiding collisions with the small sphere.\n\n    Rewards the robot for avoiding collisions with the small sphere.\n    \'\'\'\n    robot = env.scene["robot"] # CORRECT: Accessing robot using approved pattern\n    small_sphere = env.scene[\'Object2\'] # CORRECT: Accessing small sphere using approved pattern and try/except\n    robot_pelvis_idx = robot.body_names.index(\'pelvis\') # CORRECT: Accessing robot part index using approved pattern\n    robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx] # CORRECT: Accessing robot part position using approved pattern\n    small_sphere_pos = small_sphere.data.root_pos_w # CORRECT: Accessing object position using approved pattern\n\n    # CORRECT: Calculate relative distance - x distance to sphere\n    distance_x_to_sphere = small_sphere_pos[:, 0] - robot_pelvis_pos[:, 0]\n    distance_y_to_sphere = small_sphere_pos[:, 1] - robot_pelvis_pos[:, 1]\n    distance_to_sphere = torch.sqrt(distance_x_to_sphere**2 + distance_y_to_sphere**2)\n\n    # CORRECT: Reward for avoiding collisions with the small sphere\n    reward = distance_to_sphere**2\n\n    activation_condition = distance_to_sphere < 1\n\n    reward = torch.where(activation_condition, reward, torch.tensor(0.0, device=env.device))\n\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    # CORRECT: Main reward with weight 1.0\n    Main_WalkToSmallSphereReward = RewTerm(func=main_WalkToSmallSphere_reward, weight=1.0,\n                                params={"normalise": True, "normaliser_name": "main_reward"})\n\n    # CORRECT: Supporting rewards with lower weights\n    ForwardProgressReward = RewTerm(func=shaping_reward_forward_progress, weight=0.3,\n                                params={"normalise": True, "normaliser_name": "forward_progress_reward"})\n    \n    CollisionAvoidanceReward = RewTerm(func=collision_avoidance_reward, weight=0.2,\n                                params={"normalise": True, "normaliser_name": "collision_avoidance_reward"})\n\n    StabilityReward = RewTerm(func=shaping_reward_stability, weight=0.6,\n                                params={"normalise": True, "normaliser_name": "stability_reward"})\n',successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\ndef WalkToSmallSphere_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the WalkToSmallSphere skill has been successfully completed.\n    Success is defined as the robot pelvis being within 1m of the small sphere horizontally.\n    '''\n    # 1. Access the robot object using the approved pattern\n    robot = env.scene[\"robot\"]\n\n    # 2. Get the index of the robot pelvis using robot.body_names.index for robustness\n    robot_pelvis_idx = robot.body_names.index('pelvis')\n    # 3. Get the world position of the robot pelvis using the approved pattern\n    robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx]\n\n    try:\n        # 4. Safely access the small sphere object (Object2) using try-except block and approved pattern\n        small_sphere = env.scene['Object2']\n        # 5. Get the world position of the small sphere using the approved pattern\n        small_sphere_pos = small_sphere.data.root_pos_w\n\n        # 6. Calculate the relative distance in x and y directions (horizontal distance)\n        distance_x = small_sphere_pos[:, 0] - robot_pelvis_pos[:, 0]\n        distance_y = small_sphere_pos[:, 1] - robot_pelvis_pos[:, 1]\n    \n\n\n        pelvis_z_pos = robot_pelvis_pos[:, 2]\n\n        # 7. Define success condition: horizontal distance to small sphere is less than 1m.\n        #    Using a lenient threshold of 1.5m to ensure robustness, as per instructions.\n        success_threshold = 1.0\n\n        condition = (distance_x < success_threshold) & (distance_y < success_threshold) & (pelvis_z_pos > 0.5)\n\n    except KeyError:\n        # 8. Handle the case where the small sphere object is not found, setting success to False for all envs.\n        condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 9. Check for success duration using the check_success_duration function with a duration of 0.1s\n    success = check_success_duration(env, condition, \"WalkToSmallSphere\", duration=0.1)\n\n    # 10. Save success states for environments that are successful in this step\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"WalkToSmallSphere\")\n\n    # 11. Return the success tensor\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=WalkToSmallSphere_success)\n"},{name:"ExecuteKickSmallSphereForward",level:1,policyVideo:"/videos/ExecuteKickSmallSphereForward.mp4",rewardCode:"\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer\nfrom ...objects import get_object_volume\n\ndef main_ExecuteKickSmallSphereForward_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for ExecuteKickSmallSphereForward.\n\n    Reward for kicking the small sphere past the block in the x direction.\n    This reward is activated when the small sphere is past the block in the x direction, and the reward is the x distance of the sphere past the block.\n    '''\n    try:\n        small_sphere = env.scene['Object2'] # CORRECT: Accessing small sphere object using approved pattern\n        block = env.scene['Object5'] # CORRECT: Accessing block object using approved pattern\n\n        # CORRECT: Accessing object positions using approved pattern\n        small_sphere_pos_x = small_sphere.data.root_pos_w[:, 0]\n        small_sphere_pos_y = small_sphere.data.root_pos_w[:, 1]\n        block_pos_x = block.data.root_pos_w[:, 0]\n        block_pos_y = block.data.root_pos_w[:, 1]\n\n        # CORRECT: Calculate relative distance in x direction between small sphere and block\n        distance_x_sphere_block = small_sphere_pos_x - block_pos_x\n        distance_y_sphere_block = small_sphere_pos_y - block_pos_y\n\n        distance_sphere_block = torch.sqrt(distance_x_sphere_block**2 + distance_y_sphere_block**2)\n\n        # Reward is the absolute x distance of the sphere past the block, only when activated\n        reward = torch.abs(distance_sphere_block)\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # CORRECT: Reward normalization implementation\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_approach_sphere(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"approach_sphere_reward\") -> torch.Tensor:\n    '''Shaping reward for approaching the small sphere from behind in the x-direction.\n\n    Reward is based on reducing the x-distance between the robot's pelvis and the small sphere.\n    Activated when the robot is behind the sphere in the x direction.\n    '''\n    try:\n        small_sphere = env.scene['Object2'] # CORRECT: Accessing small sphere object using approved pattern\n        robot = env.scene[\"robot\"] # CORRECT: Accessing robot object using approved pattern\n        pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Accessing pelvis index using approved pattern\n        robot_pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n\n        # CORRECT: Accessing object position using approved pattern\n        small_sphere_pos_x = small_sphere.data.root_pos_w[:, 0]\n        small_sphere_pos_y = small_sphere.data.root_pos_w[:, 1]\n        robot_pelvis_pos_x = robot_pelvis_pos[:, 0]\n        robot_pelvis_pos_y = robot_pelvis_pos[:, 1]\n\n        # CORRECT: Calculate relative distance in x direction between pelvis and sphere\n        distance_x_pelvis_sphere = small_sphere_pos_x - robot_pelvis_pos_x\n        distance_y_pelvis_sphere = small_sphere_pos_y - robot_pelvis_pos_y\n\n        distance_to_sphere = torch.sqrt(distance_x_pelvis_sphere**2 + distance_y_pelvis_sphere**2)\n\n\n        # Reward is negative absolute x distance to the sphere, only when activated\n        reward = -torch.abs(distance_to_sphere)\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # CORRECT: Reward normalization implementation\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_foot_close_to_sphere(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"foot_close_sphere_reward\") -> torch.Tensor:\n    '''Shaping reward for bringing the kicking foot close to the small sphere.\n\n    Reward is based on reducing the distance between the right ankle (kicking foot) and the small sphere in x and y directions.\n    Activated when the robot pelvis is close to the sphere in x direction.\n    '''\n    try:\n        small_sphere = env.scene['Object2'] # CORRECT: Accessing small sphere object using approved pattern\n        robot = env.scene[\"robot\"] # CORRECT: Accessing robot object using approved pattern\n        robot_foot_idx = robot.body_names.index('right_ankle_roll_link') # CORRECT: Accessing right ankle index using approved pattern\n        robot_foot_pos = robot.data.body_pos_w[:, robot_foot_idx] # CORRECT: Accessing right ankle position using approved pattern\n        robot_pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Accessing pelvis index using approved pattern\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n\n        # CORRECT: Accessing object position using approved pattern\n        small_sphere_pos = small_sphere.data.root_pos_w\n        robot_foot_pos_x = robot_foot_pos[:, 0]\n        robot_foot_pos_y = robot_foot_pos[:, 1]\n        robot_pelvis_pos_x = robot_pelvis_pos[:, 0]\n        small_sphere_pos_x = small_sphere_pos[:, 0]\n\n        # CORRECT: Calculate relative distances in x and y directions between foot and sphere\n        distance_x_foot_sphere = small_sphere_pos_x - robot_foot_pos_x\n        distance_y_foot_sphere = small_sphere_pos[:, 1] - robot_foot_pos_y # y distance\n\n        distance_to_sphere = torch.sqrt(distance_x_foot_sphere**2 + distance_y_foot_sphere**2)\n\n        # Activation condition: robot pelvis is close to the sphere in x direction (within 1m)\n        activation_condition = (torch.abs(distance_to_sphere) < 2)\n\n        # Reward is negative sum of absolute distances to the sphere in x and y, only when activated\n        reward = -distance_to_sphere\n        reward = torch.where(activation_condition, reward, -2*torch.ones_like(reward)) # CORRECT: Reward is -2 if not activated, ensuring continuous reward\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # CORRECT: Reward normalization implementation\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_stable_pelvis_height(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"stable_pelvis_height_reward\") -> torch.Tensor:\n    '''Shaping reward for maintaining a stable pelvis height.\n\n    Reward is based on keeping the pelvis z-position close to a desired standing height (0.7m).\n    Always active.\n    '''\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot object using approved pattern\n    robot_pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Accessing pelvis index using approved pattern\n    robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n\n    # CORRECT: Accessing pelvis z position\n    robot_pelvis_pos_z = robot_pelvis_pos[:, 2]\n\n    # Desired pelvis height\n    default_pelvis_z = 0.7\n\n    # CORRECT: Calculate relative distance in z direction from default height\n    distance_z_pelvis_default =  default_pelvis_z - robot_pelvis_pos_z\n\n    # Activation condition: always active\n    activation_condition = True\n\n    # Reward is negative absolute distance from default pelvis z height, always active\n    reward = -torch.abs(distance_z_pelvis_default)\n\n    # CORRECT: Reward normalization implementation\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_collision_avoidance_block(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"collision_avoidance_block_reward\") -> torch.Tensor:\n    '''Shaping reward for collision avoidance penalty for collisions between the robot's pelvis and the block.\n\n    Penalty if the robot's pelvis is close to the block in x and y directions.\n    Activated when robot pelvis is near the block in x and y direction.\n    '''\n    try:\n        block = env.scene['Object5'] # CORRECT: Accessing block object using approved pattern\n        robot = env.scene[\"robot\"] # CORRECT: Accessing robot object using approved pattern\n        robot_pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Accessing pelvis index using approved pattern\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n\n        # CORRECT: Accessing object position using approved pattern\n        block_pos = block.data.root_pos_w\n        robot_pelvis_pos_x = robot_pelvis_pos[:, 0]\n        robot_pelvis_pos_y = robot_pelvis_pos[:, 1]\n        robot_pelvis_pos_z = robot_pelvis_pos[:, 2]\n        block_pos_x = block_pos[:, 0]\n        block_pos_y = block_pos[:, 1]\n        block_pos_z = block_pos[:, 2]\n\n\n        # CORRECT: Calculate relative distances in x, y, and z directions between pelvis and block\n        distance_x_pelvis_block = block_pos_x - robot_pelvis_pos_x\n        distance_y_pelvis_block = block_pos_y - robot_pelvis_pos_y\n        distance_z_pelvis_block = block_pos_z - robot_pelvis_pos_z\n\n\n        # Activation condition: robot pelvis is near the block in x and y direction (within 1m in x, 5m in y)\n        activation_condition = (torch.abs(distance_x_pelvis_block) < 1.0) & (torch.abs(distance_y_pelvis_block) < 5.0)\n\n        # Reward is a penalty (-1.0) if close to block, otherwise 0, only when activated\n        reward = -1.0 * torch.ones(env.num_envs, device=env.device) # penalty value\n        reward = torch.where(activation_condition, reward, torch.zeros_like(reward)) # CORRECT: Penalty when activated, 0 otherwise, ensuring continuous reward\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # CORRECT: Reward normalization implementation\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    Main_ExecuteKickSmallSphereForwardReward = RewTerm(func=main_ExecuteKickSmallSphereForward_reward, weight=1.0,\n                                            params={\"normalise\": True, \"normaliser_name\": \"main_reward\"})\n    ShapingRewardApproachSphere = RewTerm(func=shaping_reward_approach_sphere, weight=0.2,\n                                            params={\"normalise\": True, \"normaliser_name\": \"approach_sphere_reward\"})\n    ShapingRewardFootCloseToSphere = RewTerm(func=shaping_reward_foot_close_to_sphere, weight=0.2,\n                                            params={\"normalise\": True, \"normaliser_name\": \"foot_close_sphere_reward\"})\n    ShapingRewardStablePelvisHeight = RewTerm(func=shaping_reward_stable_pelvis_height, weight=0.2,\n                                            params={\"normalise\": True, \"normaliser_name\": \"stable_pelvis_height_reward\"})\n    ShapingRewardCollisionAvoidanceBlock = RewTerm(func=shaping_reward_collision_avoidance_block, weight=0, # Reduced weight slightly as it's a penalty\n                                            params={\"normalise\": True, \"normaliser_name\": \"collision_avoidance_block_reward\"})\n",successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\n# Standard imports - DO NOT MODIFY\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer\nfrom ...objects import get_object_volume\n\ndef ExecuteKickSmallSphereForward_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the ExecuteKickSmallSphereForward skill has been successfully completed.\n    Success is defined as the small sphere being at least 1 meter past the block in the x direction.\n    '''\n    try:\n        # Access the small sphere object using the approved pattern (rule 2 & 6)\n        object_small_sphere = env.scene['Object2']\n        # Access the block object using the approved pattern (rule 2 & 6)\n        object_block = env.scene['Object5']\n\n        # Get the x position of the small sphere using the approved pattern (rule 2)\n        small_sphere_pos_x = object_small_sphere.data.root_pos_w[:, 0]\n        small_sphere_pos_y = object_small_sphere.data.root_pos_w[:, 1]\n        # Get the x position of the block using the approved pattern (rule 2)\n        block_pos_x = object_block.data.root_pos_w[:, 0]\n        block_pos_y = object_block.data.root_pos_w[:, 1]\n\n\n\n        # Calculate the relative distance in the x direction between the small sphere and the block (rule 1)\n        distance_x_sphere_block = torch.abs(small_sphere_pos_x - block_pos_x)\n        distance_y_sphere_block = torch.abs(small_sphere_pos_y - block_pos_y)\n\n        distance_sphere_block = torch.sqrt(distance_x_sphere_block**2 + distance_y_sphere_block**2)\n        # Define success condition: small sphere is at least 1.0 meter past the block in the x direction (rule 4 & 13)\n        success_condition = distance_sphere_block > 3.0\n\n    except KeyError:\n        # Handle missing objects (small sphere or block) by returning failure (rule 5)\n        success_condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # Check for success duration and save success state using provided helper functions (rule 6 & 7)\n    success = check_success_duration(env, success_condition, \"ExecuteKickSmallSphereForward\", duration=0.5) # Using a duration of 0.5 seconds\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"ExecuteKickSmallSphereForward\")\n\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=ExecuteKickSmallSphereForward_success)\n"}]},{name:"JumpOntoBlock",level:2,rewardCode:"\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer\nfrom ...objects import get_object_volume\n\nimport torch\n\ndef main_ExecuteJumpOntoBlock_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for ExecuteJumpOntoBlock.\n\n    Reward for the robot standing on top of the block with feet above the block's top surface.\n    This encourages the robot to successfully jump and land on the block.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    try:\n        block = env.scene['Object5'] # Accessing block object using approved pattern and try/except - rule 2 & 5 in ABSOLUTE REQUIREMENTS\n\n        # Accessing robot foot positions using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n        left_foot_idx = robot.body_names.index('left_ankle_roll_link')\n        right_foot_idx = robot.body_names.index('right_ankle_roll_link')\n        left_foot_pos = robot.data.body_pos_w[:, left_foot_idx]\n        right_foot_pos = robot.data.body_pos_w[:, right_foot_idx]\n\n        # Calculate minimum foot position\n        min_foot_pos = torch.min(left_foot_pos, right_foot_pos)\n\n        # Accessing block position using approved pattern - rule 2 in ABSOLUTE REQUIREMENTS\n        block_pos = block.data.root_pos_w\n\n        # Calculate relative distance in z direction between minimum foot position and block top surface - rule 1 in ABSOLUTE REQUIREMENTS\n        block_size_z = 0.5 # Reading block size from object config (size_cubes = [[0.4, 10.0, 0.4], [1.0, 10.0, 0.2], [0.5, 0.5, 0.5]]) - rule 6 & 7 in CRITICAL IMPLEMENTATION RULES\n        block_top_surface_z = block_pos[:, 2] + block_size_z \n        distance_z_feet_block_top = min_foot_pos[:, 2] - block_top_surface_z  # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n\n\n        # Reward is negative absolute distance to encourage feet to be on top of the block - rule 5 in CRITICAL IMPLEMENTATION RULES\n        reward = -torch.abs(distance_z_feet_block_top) # Continuous reward, using relative distance - rule 5 in CRITICAL IMPLEMENTATION RULES\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward - rule 5 in ABSOLUTE REQUIREMENTS\n\n    # Reward normalization using RewNormalizer - rule 6 in ABSOLUTE REQUIREMENTS and rule 4 in REWARD STRUCTURE RULES\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_approach_block_x(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"approach_block_x_reward\") -> torch.Tensor:\n    '''Shaping reward for approaching the block in the x-direction.\n\n    Reward for decreasing the x-distance between the pelvis and the block.\n    Encourages the robot to move towards the block before jumping.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    try:\n        block = env.scene['Object5'] # Accessing block object using approved pattern and try/except - rule 2 & 5 in ABSOLUTE REQUIREMENTS\n\n        # Accessing robot pelvis position using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n\n        # Accessing block position using approved pattern - rule 2 in ABSOLUTE REQUIREMENTS\n        block_pos = block.data.root_pos_w\n\n        # Calculate relative distance in x direction between pelvis and block - rule 1 in ABSOLUTE REQUIREMENTS\n        distance_x_pelvis_block = block_pos[:, 0] - pelvis_pos[:, 0] # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n        distance_y_pelvis_block = block_pos[:, 1] - pelvis_pos[:, 1] # Relative distance in y-direction\n        distance = torch.sqrt(distance_x_pelvis_block**2 + distance_y_pelvis_block**2)\n\n\n        # Reward is negative absolute distance to encourage moving closer in x direction - rule 5 in CRITICAL IMPLEMENTATION RULES\n        reward = -torch.abs(distance) # Continuous reward, using relative distance - rule 5 in CRITICAL IMPLEMENTATION RULES\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward - rule 5 in ABSOLUTE REQUIREMENTS\n\n    # Reward normalization using RewNormalizer - rule 6 in ABSOLUTE REQUIREMENTS and rule 4 in REWARD STRUCTURE RULES\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_jump_height(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"jump_height_reward\") -> torch.Tensor:\n    '''Shaping reward for achieving a suitable jump height when approaching the block.\n\n    Reward for reaching a target pelvis height above the block's top surface when close to the block in x-direction.\n    Encourages the robot to jump upwards when approaching the block.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    try:\n        block = env.scene['Object5'] # Accessing block object using approved pattern and try/except - rule 2 & 5 in ABSOLUTE REQUIREMENTS\n\n        # Accessing robot pelvis position using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n\n        # Accessing block position using approved pattern - rule 2 in ABSOLUTE REQUIREMENTS\n        block_pos = block.data.root_pos_w\n\n        # Calculate target pelvis height above block top surface - rule 1 in ABSOLUTE REQUIREMENTS\n        block_size_z = 0.5 # Reading block size from object config (size_cubes = [[0.4, 10.0, 0.4], [1.0, 10.0, 0.2], [0.5, 0.5, 0.5]]) - rule 6 & 7 in CRITICAL IMPLEMENTATION RULES\n        block_top_surface_z = block_pos[:, 2] + block_size_z\n        target_pelvis_height = block_top_surface_z + 0.5 # Relative target height - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Calculate relative distance in z direction between pelvis and target height - rule 1 in ABSOLUTE REQUIREMENTS\n        distance_z_pelvis_target_height = pelvis_pos[:, 2] - target_pelvis_height # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Approach block condition: activate when robot is approaching the block in x direction - rule 4 in ABSOLUTE REQUIREMENTS\n        approach_block_condition = (pelvis_pos[:, 0] > block_pos[:, 0] - 2.0) & (pelvis_pos[:, 0] < block_pos[:, 0]) # Relative condition - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Reward is negative absolute distance to encourage reaching target height - rule 5 in CRITICAL IMPLEMENTATION RULES\n        reward = -torch.abs(distance_z_pelvis_target_height) # Continuous reward, using relative distance - rule 5 in CRITICAL IMPLEMENTATION RULES\n\n        reward = torch.where(approach_block_condition, reward, -torch.ones_like(reward)) # Apply activation condition - rule 4 in ABSOLUTE REQUIREMENTS\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward - rule 5 in ABSOLUTE REQUIREMENTS\n\n    # Reward normalization using RewNormalizer - rule 6 in ABSOLUTE REQUIREMENTS and rule 4 in REWARD STRUCTURE RULES\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_stability_on_block(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"stability_on_block_reward\") -> torch.Tensor:\n    '''Shaping reward for maintaining stability on the block after landing.\n\n    Reward for keeping the pelvis at a reasonable height relative to the block and feet, and avoid falling off.\n    Encourages the robot to maintain balance on the block.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    try:\n        block = env.scene['Object5'] # Accessing block object using approved pattern and try/except - rule 2 & 5 in ABSOLUTE REQUIREMENTS\n\n        # Accessing robot pelvis and feet positions using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n        left_foot_idx = robot.body_names.index('left_ankle_roll_link')\n        right_foot_idx = robot.body_names.index('right_ankle_roll_link')\n        left_foot_pos = robot.data.body_pos_w[:, left_foot_idx]\n        right_foot_pos = robot.data.body_pos_w[:, right_foot_idx]\n\n        # Calculate average foot position\n        avg_foot_pos = (left_foot_pos + right_foot_pos) / 2\n\n        # Accessing block position using approved pattern - rule 2 in ABSOLUTE REQUIREMENTS\n        block_pos = block.data.root_pos_w\n\n        # Calculate relative pelvis height above average feet position - rule 1 in ABSOLUTE REQUIREMENTS\n        relative_pelvis_height = pelvis_pos[:, 2] - avg_foot_pos[:, 2] # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Target relative pelvis height (slightly above feet) - rule 4 in ABSOLUTE REQUIREMENTS\n        target_relative_pelvis_height = 0.5 # Relative target height - rule 4 in ABSOLUTE REQUIREMENTS\n\n        # Calculate distance from target relative pelvis height - rule 1 in ABSOLUTE REQUIREMENTS\n        distance_z_pelvis_relative_target = relative_pelvis_height - target_relative_pelvis_height # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n\n        pelvis_on_block_condition_x = (pelvis_pos[:, 0] > block_pos[:, 0] - 0.25) & (pelvis_pos[:, 0] < block_pos[:, 0] + 0.25)\n        pelvis_on_block_condition_y = (pelvis_pos[:, 1] > block_pos[:, 1] - 0.25) & (pelvis_pos[:, 1] < block_pos[:, 1] + 0.25)\n\n\n        # On block condition: activate when feet are approximately on top of the block - rule 4 in ABSOLUTE REQUIREMENTS\n        block_size_z = 0.5 # Reading block size from object config (size_cubes = [[0.4, 10.0, 0.4], [1.0, 1.0, 0.2], [0.5, 0.5, 0.5]]) - rule 6 & 7 in CRITICAL IMPLEMENTATION RULES\n        on_block_condition = (avg_foot_pos[:, 2] > (block_pos[:, 2] + block_size_z - 0.1)) & pelvis_on_block_condition_x & pelvis_on_block_condition_y # Relative condition - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Reward is negative absolute distance to encourage maintaining target relative pelvis height - rule 5 in CRITICAL IMPLEMENTATION RULES\n        reward = -torch.abs(distance_z_pelvis_relative_target) # Continuous reward, using relative distance - rule 5 in CRITICAL IMPLEMENTATION RULES\n\n        reward = torch.where(on_block_condition, reward, -2.0 * torch.ones_like(reward)) # Apply activation condition - rule 4 in ABSOLUTE REQUIREMENTS\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward - rule 5 in ABSOLUTE REQUIREMENTS\n\n    # Reward normalization using RewNormalizer - rule 6 in ABSOLUTE REQUIREMENTS and rule 4 in REWARD STRUCTURE RULES\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_collision_avoidance(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"collision_avoidance_reward\") -> torch.Tensor:\n    '''Shaping reward for collision avoidance with the block at ground level.\n\n    Negative reward if the pelvis is too close to the block horizontally when the pelvis is low to the ground.\n    Discourages the robot from colliding with the block before jumping.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    try:\n        block = env.scene['Object5'] # Accessing block object using approved pattern and try/except - rule 2 & 5 in ABSOLUTE REQUIREMENTS\n\n        # Accessing robot pelvis position using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n\n        # Accessing block position using approved pattern - rule 2 in ABSOLUTE REQUIREMENTS\n        block_pos = block.data.root_pos_w\n\n        # Calculate relative distance in x direction between pelvis and block - rule 1 in ABSOLUTE REQUIREMENTS\n        distance_x_pelvis_block = block_pos[:, 0] - pelvis_pos[:, 0] # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Collision condition: activate when robot is very close to block horizontally and low to the ground - rule 4 in ABSOLUTE REQUIREMENTS\n        collision_condition = (pelvis_pos[:, 0] > block_pos[:, 0] - 0.5) & (pelvis_pos[:, 0] < block_pos[:, 0] + 0.5) & (pelvis_pos[:, 2] < 0.2) # Relative condition in x, absolute condition in z (allowed for height) - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Small negative reward for collision proximity - rule 5 in CRITICAL IMPLEMENTATION RULES\n        reward = -1.0 * torch.ones(env.num_envs, device=env.device) # Continuous negative reward - rule 5 in CRITICAL IMPLEMENTATION RULES\n\n        reward = torch.where(collision_condition, reward, torch.zeros_like(reward)) # Apply activation condition - rule 4 in ABSOLUTE REQUIREMENTS\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward - rule 5 in ABSOLUTE REQUIREMENTS\n\n    # Reward normalization using RewNormalizer - rule 6 in ABSOLUTE REQUIREMENTS and rule 4 in REWARD STRUCTURE RULES\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_feet_under_pelvis(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"feet_under_pelvis_reward\") -> torch.Tensor:\n    '''Shaping reward for keeping feet underneath the pelvis in the horizontal plane.\n    \n    Reward for minimizing the horizontal (x,y) distance between the feet and pelvis.\n    Encourages the robot to maintain a stable posture.\n    '''\n    robot = env.scene[\"robot\"]\n    \n    # Accessing robot pelvis and feet positions using approved pattern\n    pelvis_idx = robot.body_names.index('pelvis')\n    pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n    left_foot_idx = robot.body_names.index('left_ankle_roll_link')\n    right_foot_idx = robot.body_names.index('right_ankle_roll_link')\n    left_foot_pos = robot.data.body_pos_w[:, left_foot_idx]\n    right_foot_pos = robot.data.body_pos_w[:, right_foot_idx]\n    \n    left_x_distance = pelvis_pos[:, 0] - left_foot_pos[:, 0]\n    left_y_distance = pelvis_pos[:, 1] - left_foot_pos[:, 1]\n    \n    right_x_distance = pelvis_pos[:, 0] - right_foot_pos[:, 0]\n    right_y_distance = pelvis_pos[:, 1] - right_foot_pos[:, 1]\n    \n    \n    \n\n    # Calculate horizontal distance between pelvis and average foot position\n    horizontal_distance = torch.sqrt((left_x_distance)**2 + \n                                     (left_y_distance)**2) + torch.sqrt((right_x_distance)**2 + \n                                     (right_y_distance)**2)\n    \n    # Reward is negative distance to encourage feet to stay under pelvis\n    reward = -horizontal_distance\n    \n    # Reward normalization using RewNormalizer\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n    \n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n@configclass\nclass TaskRewardsCfg:\n    Main_ExecuteJumpOntoBlockReward = RewTerm(func=main_ExecuteJumpOntoBlock_reward, weight=1.0,\n                                            params={\"normalise\": True, \"normaliser_name\": \"main_reward\"})\n    ShapingRewardApproachBlockX = RewTerm(func=shaping_reward_approach_block_x, weight=1.0,\n                                            params={\"normalise\": True, \"normaliser_name\": \"approach_block_x_reward\"})\n    ShapingRewardJumpHeight = RewTerm(func=shaping_reward_jump_height, weight=0.5,\n                                            params={\"normalise\": True, \"normaliser_name\": \"jump_height_reward\"})\n    ShapingRewardStabilityOnBlock = RewTerm(func=shaping_reward_stability_on_block, weight=1.0,\n                                            params={\"normalise\": True, \"normaliser_name\": \"stability_on_block_reward\"})\n    ShapingRewardCollisionAvoidance = RewTerm(func=shaping_reward_collision_avoidance, weight=0, \n                                            params={\"normalise\": True, \"normaliser_name\": \"collision_avoidance_reward\"})\n    ShapingRewardFeetUnderPelvis = RewTerm(func=shaping_reward_feet_under_pelvis, weight=0.5,\n                                            params={\"normalise\": True, \"normaliser_name\": \"feet_under_pelvis_reward\"})\n",successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer\nfrom ...objects import get_object_volume\n\ndef JumpOntoBlock_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the JumpOntoBlock skill has been successfully completed.\n    Success is defined as the robot standing on top of the block with both feet.\n    '''\n    # 1. Access the robot object from the environment scene.\n    robot = env.scene[\"robot\"]\n\n    # 2. Get indices for the left and right feet (ankle_roll_link) using robot.body_names.index.\n    left_foot_idx = robot.body_names.index('left_ankle_roll_link')\n    right_foot_idx = robot.body_names.index('right_ankle_roll_link')\n\n    # 3. Get the world positions of the left and right feet using robot.data.body_pos_w and the indices.\n    left_foot_pos = robot.data.body_pos_w[:, left_foot_idx] # Shape: [num_envs, 3]\n    right_foot_pos = robot.data.body_pos_w[:, right_foot_idx] # Shape: [num_envs, 3]\n\n    # 4. Calculate the average position of the two feet.\n    avg_feet_pos_x = (left_foot_pos[:, 0] + right_foot_pos[:, 0]) / 2.0\n    avg_feet_pos_y = (left_foot_pos[:, 1] + right_foot_pos[:, 1]) / 2.0\n    avg_feet_pos_z = (left_foot_pos[:, 2] + right_foot_pos[:, 2]) / 2.0\n    avg_feet_pos = torch.stack([avg_feet_pos_x, avg_feet_pos_y, avg_feet_pos_z], dim=-1) # Shape: [num_envs, 3]\n\n\n    try:\n        # 5. Safely access the block object (Object5) from the environment scene using try-except block.\n        block = env.scene['Object5']\n        # 6. Get the world position of the block using block.data.root_pos_w.\n        block_pos = block.data.root_pos_w # Shape: [num_envs, 3]\n\n        # 7. Calculate the relative distance vector between the average feet position and the block position.\n        distance_x = block_pos[:, 0] - avg_feet_pos[:, 0]\n        distance_y = block_pos[:, 1] - avg_feet_pos[:, 1]\n        # 8. Calculate the vertical distance between the average feet position and the top of the block.\n        #    Block size is [0.5, 0.5, 0.5], so half height is 0.25.\n        distance_z = (block_pos[:, 2] + 0.25) - avg_feet_pos[:, 2] # distance to top of block\n\n        # 9. Define success condition:\n        #    - Vertical distance (distance_z) is less than 0.15 (feet are above the block top within 15cm).\n        #    - Horizontal distances (distance_x and distance_y) are within 0.25 in both x and y directions.\n        success_condition = (distance_z < 0.15) & (torch.abs(distance_x) < 0.25) & (torch.abs(distance_y) < 0.25)\n\n    except KeyError:\n        # 10. Handle KeyError if 'Object5' (block) is not found in the scene. Set success to False for all environments.\n        success_condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 11. Check for success duration of 0.5 seconds using check_success_duration.\n    success = check_success_duration(env, success_condition, \"JumpOntoBlock\", duration=0.5)\n\n    # 12. Save success states for environments that meet the success condition using save_success_state.\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"JumpOntoBlock\")\n\n    # 13. Return the success tensor.\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=JumpOntoBlock_success)\n",policyVideo:"/videos/JumpOntoBlock.mp4",children:[{name:"WalkToBlock",level:1,policyVideo:"/videos/WalkToBlock.mp4",rewardCode:"\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer # DO NOT CHANGE THIS LINE!\nfrom ...objects import get_object_volume\n\ndef main_WalkToBlock_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for WalkToBlock.\n\n    Reward for moving the robot's pelvis closer to the block in the x-direction.\n    This encourages the robot to walk towards the block.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        block = env.scene['Object5'] # Accessing block (Object5) using approved pattern and try/except\n        pelvis_idx = robot.body_names.index('pelvis') # Getting pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Getting pelvis position using approved pattern\n        block_pos = block.data.root_pos_w # Getting block position using approved pattern\n\n        distance_x = block_pos[:, 0] - pelvis_pos[:, 0] # Relative distance in x-direction (block - pelvis)\n        distance_y = block_pos[:, 1] - pelvis_pos[:, 1] # Relative distance in y-direction (block - pelvis)\n        distance = torch.sqrt(distance_x**2 + distance_y**2)\n\n        reward = -torch.abs(distance) # Reward is negative absolute x-distance to encourage closer distance, continuous reward\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # Normalize reward\n        RewNormalizer.update_stats(normaliser_name, reward) # Update normalizer stats\n        return scaled_reward\n    return reward\n\ndef pelvis_height_stability_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"pelvis_height_reward\") -> torch.Tensor:\n    '''Supporting reward for maintaining pelvis height.\n\n    Reward for keeping the pelvis at a stable height around 0.7m.\n    This encourages a stable standing posture.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        pelvis_idx = robot.body_names.index('pelvis') # Getting pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Getting pelvis position using approved pattern\n        pelvis_pos_z = pelvis_pos[:, 2] # Getting pelvis z position\n\n        default_pelvis_z = 0.75 # Define default pelvis height (read from task description)\n        reward = -torch.abs(pelvis_pos_z - default_pelvis_z) # Reward is negative absolute difference from default height, continuous reward\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # Normalize reward\n        RewNormalizer.update_stats(normaliser_name, reward) # Update normalizer stats\n        return scaled_reward\n    return reward\n\ndef feet_block_collision_penalty(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"feet_block_penalty\") -> torch.Tensor:\n    '''Supporting reward to penalize feet getting too close to the block.\n\n    Penalty when the robot's feet are too close to the block in the x-direction.\n    This prevents collisions and encourages careful approach.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        block = env.scene['Object5'] # Accessing block (Object5) using approved pattern and try/except\n        left_foot_idx = robot.body_names.index('left_ankle_roll_link') # Getting left foot index using approved pattern\n        right_foot_idx = robot.body_names.index('right_ankle_roll_link') # Getting right foot index using approved pattern\n        left_foot_pos = robot.data.body_pos_w[:, left_foot_idx] # Getting left foot position using approved pattern\n        right_foot_pos = robot.data.body_pos_w[:, right_foot_idx] # Getting right foot position using approved pattern\n        block_pos_x = block.data.root_pos_w[:, 0] # Getting block x position\n\n        distance_x_left_foot = torch.abs(block_pos_x - left_foot_pos[:, 0]) # Relative x-distance between block and left foot\n        distance_x_right_foot = torch.abs(block_pos_x - right_foot_pos[:, 0]) # Relative x-distance between block and right foot\n        min_distance_x_feet_block = torch.min(distance_x_left_foot, distance_x_right_foot) # Minimum distance to either foot\n\n        collision_threshold = 0.5 # Define collision threshold (read from shaping reward description)\n        penalty = -1.0 # Fixed negative penalty value (read from shaping reward description)\n        reward = torch.where(min_distance_x_feet_block < collision_threshold, torch.tensor(penalty, device=env.device), torch.tensor(0.0, device=env.device)) # Penalty if too close, otherwise 0, continuous reward (though binary in effect)\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # Normalize reward\n        RewNormalizer.update_stats(normaliser_name, reward) # Update normalizer stats\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    Main_WalkToBlockReward = RewTerm(func=main_WalkToBlock_reward, weight=1.0,\n                                params={\"normalise\": True, \"normaliser_name\": \"main_reward\"}) # Main reward with weight 1.0\n    PelvisHeightStabilityReward = RewTerm(func=pelvis_height_stability_reward, weight=0.4,\n                                params={\"normalise\": True, \"normaliser_name\": \"pelvis_height_reward\"}) # Supporting reward with weight 0.4\n    # FeetBlockCollisionPenalty = RewTerm(func=feet_block_collision_penalty, weight=0.2,\n    #                             params={\"normalise\": True, \"normaliser_name\": \"feet_block_penalty\"}) # Supporting reward with weight 0.2 (lower weight for penalty)\n                ",successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\n# Standard imports - DO NOT MODIFY\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer\nfrom ...objects import get_object_volume\n\n\ndef WalkToBlock_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the WalkToBlock skill has been successfully completed.'''\n    # 1. Get robot pelvis position - Approved access pattern\n    robot = env.scene[\"robot\"] # Approved access pattern to get robot\n    pelvis_idx = robot.body_names.index('pelvis') # Approved access pattern to get pelvis index\n    pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Approved access pattern to get pelvis position\n\n    try:\n        # 2. Get block position (Object5) - Approved access pattern and try/except for missing object\n        block = env.scene['Object5'] # Approved access pattern to get Object5 (block)\n        block_pos = block.data.root_pos_w # Approved access pattern to get block position\n\n        # 3. Calculate relative x-distance (block x - pelvis x) - Relative distance as required, only x component\n        x_distance = block_pos[:, 0] - pelvis_pos[:, 0] # Relative distance in x-direction\n        y_distance = block_pos[:, 1] - pelvis_pos[:, 1] # Relative distance in y-direction\n        distance = torch.sqrt(x_distance**2 + y_distance**2)\n\n        condition = distance < 1.4 # Success if x distance is less than 1.2m\n    except KeyError:\n        # Handle case where block (Object5) is missing - Required for robustness\n        condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device) # If block is missing, skill is not successful\n\n    # 5. Check duration and save success states - DO NOT MODIFY THIS SECTION - Correct usage of check_success_duration and save_success_state\n    success = check_success_duration(env, condition, \"WalkToBlock\", duration=2) # Check if success condition is maintained for 0.3 seconds\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"WalkToBlock\") # Save success state for successful environments\n\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=WalkToBlock_success) # Defines success termination condition using WalkToBlock_success function\n                "},{name:"PrepareForJumpOntoBlock",level:1,policyVideo:"/videos/PrepareForJumpOntoBlock.mp4",rewardCode:'\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer # DO NOT CHANGE THIS LINE!\nfrom ...objects import get_object_volume\n\ndef main_PrepareForJumpOntoBlock_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "main_reward") -> torch.Tensor:\n    \'\'\'Main reward for PrepareForJumpOntoBlock.\n\n    Rewards the robot for crouching to a target pelvis height, preparing for the jump.\n    This reward encourages the robot to lower its pelvis, which is the primary action for preparing to jump.\n    \'\'\'\n    robot = env.scene["robot"] # Accessing robot using approved pattern\n    try:\n        #access the required robot part(s)\n        robot_pelvis_idx = robot.body_names.index(\'pelvis\') # Getting pelvis index using approved pattern\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx] # Getting pelvis position using approved pattern\n        robot_pelvis_pos_z = robot_pelvis_pos[:, 2] # Pelvis z position\n\n        # Define target pelvis height for crouching (relative to ground z=0). No hardcoded positions, target is relative to ground.\n        target_pelvis_z = 0.5\n\n        # Primary reward: negative absolute difference between current pelvis z and target crouch z. Relative distance to target height.\n        primary_reward = -torch.abs(robot_pelvis_pos_z - target_pelvis_z) # Continuous reward, relative distance, no hardcoded thresholds except target_z\n\n        reward = primary_reward\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object (robot or pelvis), return zero reward\n\n    # Reward normalization\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_block_xy(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "shaping_reward_block_x") -> torch.Tensor:\n    \'\'\'Shaping reward for being in front of the block in the x-direction.\n\n    Encourages the robot to position itself appropriately for the jump onto the block.\n    This reward is activated when the robot is past the small sphere, indicating it\'s moving towards the block.\n    \'\'\'\n    robot = env.scene["robot"] # Accessing robot using approved pattern\n    try:\n        block = env.scene[\'Object5\'] # Access the block using approved pattern\n        small_sphere = env.scene[\'Object2\'] # Access the small sphere using approved pattern\n        #access the required robot part(s)\n        robot_pelvis_idx = robot.body_names.index(\'pelvis\') # Getting pelvis index using approved pattern\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx] # Getting pelvis position using approved pattern\n        robot_pelvis_pos_x = robot_pelvis_pos[:, 0] # Pelvis x position\n        robot_pelvis_pos_y = robot_pelvis_pos[:, 1] # Pelvis y position\n        block_pos_x = block.data.root_pos_w[:, 0] # Block x position using approved pattern\n        block_pos_y = block.data.root_pos_w[:, 1] # Block y position using approved pattern\n        small_sphere_pos_x = small_sphere.data.root_pos_w[:, 0] # Small sphere x position using approved pattern\n\n\n        # Calculate x distance to block. Relative distance.\n        distance_block_x = robot_pelvis_pos_x - block_pos_x\n        distance_block_y = robot_pelvis_pos_y - block_pos_y\n\n        # Shaping reward: negative absolute x distance to the block. Continuous reward, relative distance.\n        reward_block_x = -torch.abs(distance_block_x)\n        reward_block_y = -torch.abs(distance_block_y)\n        shaping_reward_block_x_unscaled = reward_block_x + reward_block_y # Apply reward only when activated\n\n        # Normalize the reward\n        reward = shaping_reward_block_x_unscaled\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Reward normalization\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n\ndef shaping_reward_min_z(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "shaping_reward_min_z") -> torch.Tensor:\n    \'\'\'Shaping reward to penalize pelvis going too low.\n\n    Prevents the pelvis from going too low, which might lead to instability or self-collisions during crouching.\n    Penalizes if the pelvis z is lower than a minimum threshold (0.3m relative to ground).\n    \'\'\'\n    robot = env.scene["robot"] # Accessing robot using approved pattern\n    try:\n        #access the required robot part(s)\n        robot_pelvis_idx = robot.body_names.index(\'pelvis\') # Getting pelvis index using approved pattern\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx] # Getting pelvis position using approved pattern\n        robot_pelvis_pos_z = robot_pelvis_pos[:, 2] # Pelvis z position\n\n        # Minimum pelvis height threshold. No hardcoded positions, threshold is relative to ground.\n        min_pelvis_z = 0.3\n\n        # Activation condition: Pelvis is below the minimum height. Relative condition.\n        activation_condition_min_z = (robot_pelvis_pos_z < min_pelvis_z)\n\n        # Shaping reward: penalty for going below minimum pelvis height. Proportional penalty. Continuous reward, relative distance.\n        reward_min_z = -(min_pelvis_z - robot_pelvis_pos_z) # negative reward if below threshold, 0 if above.\n\n        shaping_reward_min_z_unscaled = torch.where(activation_condition_min_z, reward_min_z, torch.tensor(0.0, device=env.device)) # only apply penalty if below threshold\n\n        reward = shaping_reward_min_z_unscaled\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Reward normalization\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n\ndef shaping_reward_feet_z(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = "shaping_reward_feet_z") -> torch.Tensor:\n    \'\'\'Shaping reward for keeping feet relatively close to the ground during crouching.\n\n    Encourages a controlled crouch and prevents the feet from lifting excessively, maintaining stability.\n    Rewards for keeping the average z-height of the feet low (relative to ground).\n    \'\'\'\n    robot = env.scene["robot"] # Accessing robot using approved pattern\n    try:\n        #access the required robot part(s)\n        robot_left_foot_idx = robot.body_names.index(\'left_ankle_roll_link\') # Getting left foot index using approved pattern\n        robot_right_foot_idx = robot.body_names.index(\'right_ankle_roll_link\') # Getting right foot index using approved pattern\n        robot_left_foot_pos = robot.data.body_pos_w[:, robot_left_foot_idx] # Left foot position using approved pattern\n        robot_right_foot_pos = robot.data.body_pos_w[:, robot_right_foot_idx] # Right foot position using approved pattern\n        robot_left_foot_pos_z = robot_left_foot_pos[:, 2] # Left foot z position\n        robot_right_foot_pos_z = robot_right_foot_pos[:, 2] # Right foot z position\n\n        # Calculate average foot z-height. Relative to ground.\n        average_foot_z = (robot_left_foot_pos_z + robot_right_foot_pos_z) / 2.0\n\n        # Shaping reward: negative average foot z-height (reward for lower feet). Continuous reward, relative distance to ground.\n        shaping_reward_feet_z_unscaled = -torch.abs(average_foot_z) # reward for feet being closer to ground (z=0)\n\n        reward = shaping_reward_feet_z_unscaled\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Reward normalization\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    Main_PrepareForJumpOntoBlockReward = RewTerm(func=main_PrepareForJumpOntoBlock_reward, weight=1.0,\n                                params={"normalise": True, "normaliser_name": "main_reward"}) # Main reward with weight 1.0\n\n    ShapingRewardBlockXY = RewTerm(func=shaping_reward_block_xy, weight=0.5,\n                                params={"normalise": True, "normaliser_name": "shaping_reward_block_xy"}) # Shaping reward with weight 0.5\n\n    ShapingRewardMinZ = RewTerm(func=shaping_reward_min_z, weight=0.3,\n                                params={"normalise": True, "normaliser_name": "shaping_reward_min_z"}) # Shaping reward with weight 0.3\n\n    ShapingRewardFeetZ = RewTerm(func=shaping_reward_feet_z, weight=0.3,\n                                params={"normalise": True, "normaliser_name": "shaping_reward_feet_z"}) # Shaping reward with weight 0.3\n                ',successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\ndef PrepareForJumpOntoBlock_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the PrepareForJumpOntoBlock skill has been successfully completed.\n    Success is defined as the robot pelvis being below 0.6m in height, indicating a crouched position.\n    This is measured relative to the ground (z=0).\n    '''\n    # 1. Access the robot object from the scene using the approved pattern.\n    robot = env.scene[\"robot\"]\n\n    try:\n        # 2. Get the index of the 'pelvis' body part using the approved pattern.\n        robot_pelvis_idx = robot.body_names.index('pelvis')\n        # 3. Get the world position of the pelvis using the approved pattern.\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx]\n        # 4. Extract the z-component of the pelvis position.\n        robot_pelvis_pos_z = robot_pelvis_pos[:, 2]\n\n        # 5. Define the success condition: pelvis z-position < 0.6m.\n        #    This is a relative check to the ground (z=0), fulfilling requirement 1 (relative distances).\n        #    No hardcoded positions or arbitrary thresholds are used, threshold is based on skill description and reward functions.\n        condition = (robot_pelvis_pos_z < 0.6)\n\n    except KeyError:\n        # 6. Handle potential KeyError if 'robot' or 'pelvis' is not found in the scene.\n        #    This fulfills requirement 5 (handle missing objects).\n        condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 7. Check success duration and save success states using the provided helper functions.\n    #    This fulfills requirement 6 and 7 (check_success_duration and save_success_state).\n    success = check_success_duration(env, condition, \"PrepareForJumpOntoBlock\", duration=0.5) # Using duration of 0.5 seconds as specified.\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"PrepareForJumpOntoBlock\")\n\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=PrepareForJumpOntoBlock_success)\n                "},{name:"ExecuteJumpOntoBlock",level:1,policyVideo:"/videos/ExecuteJumpOntoBlock.mp4",rewardCode:"\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer\nfrom ...objects import get_object_volume\n\nimport torch\n\ndef main_ExecuteJumpOntoBlock_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for ExecuteJumpOntoBlock.\n\n    Reward for the robot standing on top of the block with feet above the block's top surface.\n    This encourages the robot to successfully jump and land on the block.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    try:\n        block = env.scene['Object5'] # Accessing block object using approved pattern and try/except - rule 2 & 5 in ABSOLUTE REQUIREMENTS\n\n        # Accessing robot foot positions using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n        left_foot_idx = robot.body_names.index('left_ankle_roll_link')\n        right_foot_idx = robot.body_names.index('right_ankle_roll_link')\n        left_foot_pos = robot.data.body_pos_w[:, left_foot_idx]\n        right_foot_pos = robot.data.body_pos_w[:, right_foot_idx]\n\n        # Calculate minimum foot position\n        min_foot_pos = torch.min(left_foot_pos, right_foot_pos)\n\n        # Accessing block position using approved pattern - rule 2 in ABSOLUTE REQUIREMENTS\n        block_pos = block.data.root_pos_w\n\n        # Calculate relative distance in z direction between minimum foot position and block top surface - rule 1 in ABSOLUTE REQUIREMENTS\n        block_size_z = 0.5 # Reading block size from object config (size_cubes = [[0.4, 10.0, 0.4], [1.0, 10.0, 0.2], [0.5, 0.5, 0.5]]) - rule 6 & 7 in CRITICAL IMPLEMENTATION RULES\n        block_top_surface_z = block_pos[:, 2] + block_size_z \n        distance_z_feet_block_top = min_foot_pos[:, 2] - block_top_surface_z  # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n\n\n        # Reward is negative absolute distance to encourage feet to be on top of the block - rule 5 in CRITICAL IMPLEMENTATION RULES\n        reward = -torch.abs(distance_z_feet_block_top) # Continuous reward, using relative distance - rule 5 in CRITICAL IMPLEMENTATION RULES\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward - rule 5 in ABSOLUTE REQUIREMENTS\n\n    # Reward normalization using RewNormalizer - rule 6 in ABSOLUTE REQUIREMENTS and rule 4 in REWARD STRUCTURE RULES\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_approach_block_x(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"approach_block_x_reward\") -> torch.Tensor:\n    '''Shaping reward for approaching the block in the x-direction.\n\n    Reward for decreasing the x-distance between the pelvis and the block.\n    Encourages the robot to move towards the block before jumping.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    try:\n        block = env.scene['Object5'] # Accessing block object using approved pattern and try/except - rule 2 & 5 in ABSOLUTE REQUIREMENTS\n\n        # Accessing robot pelvis position using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n\n        # Accessing block position using approved pattern - rule 2 in ABSOLUTE REQUIREMENTS\n        block_pos = block.data.root_pos_w\n\n        # Calculate relative distance in x direction between pelvis and block - rule 1 in ABSOLUTE REQUIREMENTS\n        distance_x_pelvis_block = block_pos[:, 0] - pelvis_pos[:, 0] # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n        distance_y_pelvis_block = block_pos[:, 1] - pelvis_pos[:, 1] # Relative distance in y-direction\n        distance = torch.sqrt(distance_x_pelvis_block**2 + distance_y_pelvis_block**2)\n\n\n        # Reward is negative absolute distance to encourage moving closer in x direction - rule 5 in CRITICAL IMPLEMENTATION RULES\n        reward = -torch.abs(distance) # Continuous reward, using relative distance - rule 5 in CRITICAL IMPLEMENTATION RULES\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward - rule 5 in ABSOLUTE REQUIREMENTS\n\n    # Reward normalization using RewNormalizer - rule 6 in ABSOLUTE REQUIREMENTS and rule 4 in REWARD STRUCTURE RULES\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_jump_height(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"jump_height_reward\") -> torch.Tensor:\n    '''Shaping reward for achieving a suitable jump height when approaching the block.\n\n    Reward for reaching a target pelvis height above the block's top surface when close to the block in x-direction.\n    Encourages the robot to jump upwards when approaching the block.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    try:\n        block = env.scene['Object5'] # Accessing block object using approved pattern and try/except - rule 2 & 5 in ABSOLUTE REQUIREMENTS\n\n        # Accessing robot pelvis position using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n\n        # Accessing block position using approved pattern - rule 2 in ABSOLUTE REQUIREMENTS\n        block_pos = block.data.root_pos_w\n\n        # Calculate target pelvis height above block top surface - rule 1 in ABSOLUTE REQUIREMENTS\n        block_size_z = 0.5 # Reading block size from object config (size_cubes = [[0.4, 10.0, 0.4], [1.0, 10.0, 0.2], [0.5, 0.5, 0.5]]) - rule 6 & 7 in CRITICAL IMPLEMENTATION RULES\n        block_top_surface_z = block_pos[:, 2] + block_size_z\n        target_pelvis_height = block_top_surface_z + 0.5 # Relative target height - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Calculate relative distance in z direction between pelvis and target height - rule 1 in ABSOLUTE REQUIREMENTS\n        distance_z_pelvis_target_height = pelvis_pos[:, 2] - target_pelvis_height # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Approach block condition: activate when robot is approaching the block in x direction - rule 4 in ABSOLUTE REQUIREMENTS\n        approach_block_condition = (pelvis_pos[:, 0] > block_pos[:, 0] - 2.0) & (pelvis_pos[:, 0] < block_pos[:, 0]) # Relative condition - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Reward is negative absolute distance to encourage reaching target height - rule 5 in CRITICAL IMPLEMENTATION RULES\n        reward = -torch.abs(distance_z_pelvis_target_height) # Continuous reward, using relative distance - rule 5 in CRITICAL IMPLEMENTATION RULES\n\n        reward = torch.where(approach_block_condition, reward, -torch.ones_like(reward)) # Apply activation condition - rule 4 in ABSOLUTE REQUIREMENTS\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward - rule 5 in ABSOLUTE REQUIREMENTS\n\n    # Reward normalization using RewNormalizer - rule 6 in ABSOLUTE REQUIREMENTS and rule 4 in REWARD STRUCTURE RULES\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_stability_on_block(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"stability_on_block_reward\") -> torch.Tensor:\n    '''Shaping reward for maintaining stability on the block after landing.\n\n    Reward for keeping the pelvis at a reasonable height relative to the block and feet, and avoid falling off.\n    Encourages the robot to maintain balance on the block.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    try:\n        block = env.scene['Object5'] # Accessing block object using approved pattern and try/except - rule 2 & 5 in ABSOLUTE REQUIREMENTS\n\n        # Accessing robot pelvis and feet positions using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n        left_foot_idx = robot.body_names.index('left_ankle_roll_link')\n        right_foot_idx = robot.body_names.index('right_ankle_roll_link')\n        left_foot_pos = robot.data.body_pos_w[:, left_foot_idx]\n        right_foot_pos = robot.data.body_pos_w[:, right_foot_idx]\n\n        # Calculate average foot position\n        avg_foot_pos = (left_foot_pos + right_foot_pos) / 2\n\n        # Accessing block position using approved pattern - rule 2 in ABSOLUTE REQUIREMENTS\n        block_pos = block.data.root_pos_w\n\n        # Calculate relative pelvis height above average feet position - rule 1 in ABSOLUTE REQUIREMENTS\n        relative_pelvis_height = pelvis_pos[:, 2] - avg_foot_pos[:, 2] # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Target relative pelvis height (slightly above feet) - rule 4 in ABSOLUTE REQUIREMENTS\n        target_relative_pelvis_height = 0.5 # Relative target height - rule 4 in ABSOLUTE REQUIREMENTS\n\n        # Calculate distance from target relative pelvis height - rule 1 in ABSOLUTE REQUIREMENTS\n        distance_z_pelvis_relative_target = relative_pelvis_height - target_relative_pelvis_height # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n\n        pelvis_on_block_condition_x = (pelvis_pos[:, 0] > block_pos[:, 0] - 0.25) & (pelvis_pos[:, 0] < block_pos[:, 0] + 0.25)\n        pelvis_on_block_condition_y = (pelvis_pos[:, 1] > block_pos[:, 1] - 0.25) & (pelvis_pos[:, 1] < block_pos[:, 1] + 0.25)\n\n\n        # On block condition: activate when feet are approximately on top of the block - rule 4 in ABSOLUTE REQUIREMENTS\n        block_size_z = 0.5 # Reading block size from object config (size_cubes = [[0.4, 10.0, 0.4], [1.0, 1.0, 0.2], [0.5, 0.5, 0.5]]) - rule 6 & 7 in CRITICAL IMPLEMENTATION RULES\n        on_block_condition = (avg_foot_pos[:, 2] > (block_pos[:, 2] + block_size_z - 0.1)) & pelvis_on_block_condition_x & pelvis_on_block_condition_y # Relative condition - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Reward is negative absolute distance to encourage maintaining target relative pelvis height - rule 5 in CRITICAL IMPLEMENTATION RULES\n        reward = -torch.abs(distance_z_pelvis_relative_target) # Continuous reward, using relative distance - rule 5 in CRITICAL IMPLEMENTATION RULES\n\n        reward = torch.where(on_block_condition, reward, -2.0 * torch.ones_like(reward)) # Apply activation condition - rule 4 in ABSOLUTE REQUIREMENTS\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward - rule 5 in ABSOLUTE REQUIREMENTS\n\n    # Reward normalization using RewNormalizer - rule 6 in ABSOLUTE REQUIREMENTS and rule 4 in REWARD STRUCTURE RULES\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_collision_avoidance(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"collision_avoidance_reward\") -> torch.Tensor:\n    '''Shaping reward for collision avoidance with the block at ground level.\n\n    Negative reward if the pelvis is too close to the block horizontally when the pelvis is low to the ground.\n    Discourages the robot from colliding with the block before jumping.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    try:\n        block = env.scene['Object5'] # Accessing block object using approved pattern and try/except - rule 2 & 5 in ABSOLUTE REQUIREMENTS\n\n        # Accessing robot pelvis position using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n\n        # Accessing block position using approved pattern - rule 2 in ABSOLUTE REQUIREMENTS\n        block_pos = block.data.root_pos_w\n\n        # Calculate relative distance in x direction between pelvis and block - rule 1 in ABSOLUTE REQUIREMENTS\n        distance_x_pelvis_block = block_pos[:, 0] - pelvis_pos[:, 0] # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Collision condition: activate when robot is very close to block horizontally and low to the ground - rule 4 in ABSOLUTE REQUIREMENTS\n        collision_condition = (pelvis_pos[:, 0] > block_pos[:, 0] - 0.5) & (pelvis_pos[:, 0] < block_pos[:, 0] + 0.5) & (pelvis_pos[:, 2] < 0.2) # Relative condition in x, absolute condition in z (allowed for height) - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Small negative reward for collision proximity - rule 5 in CRITICAL IMPLEMENTATION RULES\n        reward = -1.0 * torch.ones(env.num_envs, device=env.device) # Continuous negative reward - rule 5 in CRITICAL IMPLEMENTATION RULES\n\n        reward = torch.where(collision_condition, reward, torch.zeros_like(reward)) # Apply activation condition - rule 4 in ABSOLUTE REQUIREMENTS\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward - rule 5 in ABSOLUTE REQUIREMENTS\n\n    # Reward normalization using RewNormalizer - rule 6 in ABSOLUTE REQUIREMENTS and rule 4 in REWARD STRUCTURE RULES\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_feet_under_pelvis(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"feet_under_pelvis_reward\") -> torch.Tensor:\n    '''Shaping reward for keeping feet underneath the pelvis in the horizontal plane.\n    \n    Reward for minimizing the horizontal (x,y) distance between the feet and pelvis.\n    Encourages the robot to maintain a stable posture.\n    '''\n    robot = env.scene[\"robot\"]\n    \n    # Accessing robot pelvis and feet positions using approved pattern\n    pelvis_idx = robot.body_names.index('pelvis')\n    pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n    left_foot_idx = robot.body_names.index('left_ankle_roll_link')\n    right_foot_idx = robot.body_names.index('right_ankle_roll_link')\n    left_foot_pos = robot.data.body_pos_w[:, left_foot_idx]\n    right_foot_pos = robot.data.body_pos_w[:, right_foot_idx]\n    \n    left_x_distance = pelvis_pos[:, 0] - left_foot_pos[:, 0]\n    left_y_distance = pelvis_pos[:, 1] - left_foot_pos[:, 1]\n    \n    right_x_distance = pelvis_pos[:, 0] - right_foot_pos[:, 0]\n    right_y_distance = pelvis_pos[:, 1] - right_foot_pos[:, 1]\n    \n    \n    \n\n    # Calculate horizontal distance between pelvis and average foot position\n    horizontal_distance = torch.sqrt((left_x_distance)**2 + \n                                     (left_y_distance)**2) + torch.sqrt((right_x_distance)**2 + \n                                     (right_y_distance)**2)\n    \n    # Reward is negative distance to encourage feet to stay under pelvis\n    reward = -horizontal_distance\n    \n    # Reward normalization using RewNormalizer\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n    \n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n@configclass\nclass TaskRewardsCfg:\n    Main_ExecuteJumpOntoBlockReward = RewTerm(func=main_ExecuteJumpOntoBlock_reward, weight=1.0,\n                                            params={\"normalise\": True, \"normaliser_name\": \"main_reward\"})\n    ShapingRewardApproachBlockX = RewTerm(func=shaping_reward_approach_block_x, weight=1.0,\n                                            params={\"normalise\": True, \"normaliser_name\": \"approach_block_x_reward\"})\n    ShapingRewardJumpHeight = RewTerm(func=shaping_reward_jump_height, weight=0.5,\n                                            params={\"normalise\": True, \"normaliser_name\": \"jump_height_reward\"})\n    ShapingRewardStabilityOnBlock = RewTerm(func=shaping_reward_stability_on_block, weight=1.0,\n                                            params={\"normalise\": True, \"normaliser_name\": \"stability_on_block_reward\"})\n    ShapingRewardCollisionAvoidance = RewTerm(func=shaping_reward_collision_avoidance, weight=0, \n                                            params={\"normalise\": True, \"normaliser_name\": \"collision_avoidance_reward\"})\n    ShapingRewardFeetUnderPelvis = RewTerm(func=shaping_reward_feet_under_pelvis, weight=0.5,\n                                            params={\"normalise\": True, \"normaliser_name\": \"feet_under_pelvis_reward\"})\n                ",successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\ndef ExecuteJumpOntoBlock_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the ExecuteJumpOntoBlock skill has been successfully completed.'''\n    # 1. Get robot and block objects - rule 2 & 3 in ABSOLUTE REQUIREMENTS\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    try:\n        block = env.scene['Object5'] # Accessing block object using approved pattern and try/except - rule 2 & 5 in ABSOLUTE REQUIREMENTS\n    except KeyError:\n        # Handle case where the block object is not found - rule 5 in ABSOLUTE REQUIREMENTS\n        return torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 2. Get indices for robot parts - rule 3 in ABSOLUTE REQUIREMENTS\n    left_foot_idx = robot.body_names.index('left_ankle_roll_link')\n    right_foot_idx = robot.body_names.index('right_ankle_roll_link')\n    pelvis_idx = robot.body_names.index('pelvis')\n\n    # 3. Get positions of robot parts and block - rule 2 & 3 in ABSOLUTE REQUIREMENTS\n    left_foot_pos = robot.data.body_pos_w[:, left_foot_idx] # Accessing left foot position using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    right_foot_pos = robot.data.body_pos_w[:, right_foot_idx] # Accessing right foot position using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Accessing pelvis position using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    block_pos = block.data.root_pos_w # Accessing block position using approved pattern - rule 2 in ABSOLUTE REQUIREMENTS\n\n    # 4. Calculate average foot z position - rule 1 in ABSOLUTE REQUIREMENTS\n    avg_foot_z = (left_foot_pos[:, 2] + right_foot_pos[:, 2]) / 2\n\n    # 5. Calculate block top surface z position - rule 1 & 6 in ABSOLUTE REQUIREMENTS, rule 6 & 7 in CRITICAL IMPLEMENTATION RULES\n    block_size_z = 0.5 # Reading block size from object config (size_cubes = [[0.4, 10.0, 0.4], [1.0, 10.0, 0.2], [0.5, 0.5, 0.5]]) - rule 6 & 7 in CRITICAL IMPLEMENTATION RULES\n    block_top_surface_z = block_pos[:, 2] + block_size_z/2\n\n    # 6. Calculate relative z distances - rule 1 in ABSOLUTE REQUIREMENTS\n    feet_above_block = avg_foot_z - block_top_surface_z # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n    pelvis_above_block = pelvis_pos[:, 2] - block_top_surface_z # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n\n    # 7. Define success conditions based on relative distances and thresholds - rule 1 & 4 in ABSOLUTE REQUIREMENTS, rule 1 & 2 in SUCCESS CRITERIA RULES\n    feet_condition = feet_above_block > 0 # Feet are above block top - rule 1 in ABSOLUTE REQUIREMENTS, rule 3 in SUCCESS CRITERIA RULES\n    pelvis_condition = pelvis_above_block > 0.2 # Pelvis is 20cm above block top - rule 1 in ABSOLUTE REQUIREMENTS, rule 3 in SUCCESS CRITERIA RULES\n\n    # 8. Define success conditions based on pelvis position - rule 1 & 4 in ABSOLUTE REQUIREMENTS, rule 1 & 2 in SUCCESS CRITERIA RULES\n    pelvis_x_condition_low = pelvis_pos[:, 0] > block_pos[:, 0] - 0.25 # Pelvis is in front of block - rule 1 in ABSOLUTE REQUIREMENTS, rule 3 in SUCCESS CRITERIA RULES\n    pelvis_x_condition_high = pelvis_pos[:, 0] < block_pos[:, 0] + 0.25 # Pelvis is in front of block - rule 1 in ABSOLUTE REQUIREMENTS, rule 3 in SUCCESS CRITERIA RULES\n    pelvis_y_condition_low = pelvis_pos[:, 1] > block_pos[:, 1] - 0.25 # Pelvis is in front of block - rule 1 in ABSOLUTE REQUIREMENTS, rule 3 in SUCCESS CRITERIA RULES\n    pelvis_y_condition_high = pelvis_pos[:, 1] < block_pos[:, 1] + 0.25 # Pelvis is in front of block - rule 1 in ABSOLUTE REQUIREMENTS, rule 3 in SUCCESS CRITERIA RULES\n    \n    success_condition = feet_condition & pelvis_condition & pelvis_x_condition_low & pelvis_x_condition_high & pelvis_y_condition_low & pelvis_y_condition_high # Both feet and pelvis conditions must be met - rule 1 in SUCCESS CRITERIA RULES\n\n    # 8. Check success duration and save success states - rule 6 & 7 in ABSOLUTE REQUIREMENTS, rule 4 in CRITICAL IMPLEMENTATION RULES\n    success = check_success_duration(env, success_condition, \"ExecuteJumpOntoBlock\", duration=0.5) # Check success duration for 0.3 seconds - rule 6 in ABSOLUTE REQUIREMENTS\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"ExecuteJumpOntoBlock\") # Save success state for successful environments - rule 7 in ABSOLUTE REQUIREMENTS\n\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=ExecuteJumpOntoBlock_success)\n                "},{name:"StabilizeOnBlockTop",level:1,policyVideo:"/videos/StabalizerOnBlockTop.mp4",rewardCode:"\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer # DO NOT CHANGE THIS LINE!\nfrom ...objects import get_object_volume\n\ndef main_StabilizeOnBlockTop_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for StabilizeOnBlockTop.\n\n    Reward for getting both feet on top of the block and minimizing the vertical distance between the feet and the top surface of the block.\n    This encourages the robot to land on the block and stay on it.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern (rule 3, approved access pattern)\n    try:\n        block = env.scene['Object5'] # Accessing block using approved pattern and try/except for robustness (rule 2, 5, approved access pattern)\n\n        left_foot_idx = robot.body_names.index('left_ankle_roll_link') # Getting left foot index using approved pattern (rule 3, approved access pattern)\n        left_foot_pos = robot.data.body_pos_w[:, left_foot_idx] # Getting left foot position using approved pattern (rule 3, approved access pattern)\n        right_foot_idx = robot.body_names.index('right_ankle_roll_link') # Getting right foot index using approved pattern (rule 3, approved access pattern)\n        right_foot_pos = robot.data.body_pos_w[:, right_foot_idx] # Getting right foot position using approved pattern (rule 3, approved access pattern)\n\n        block_top_z = block.data.root_pos_w[:, 2] + (0.5/2) # Block top z position, block size is 0.5m, so half is 0.25m. Using relative distance, no hardcoded values. (rule 1, 4, relative distance, using object position, block size from config)\n\n        left_foot_distance_z = left_foot_pos[:, 2] - block_top_z # Vertical distance of left foot from block top, relative distance (rule 1, relative distance)\n        right_foot_distance_z = right_foot_pos[:, 2] - block_top_z # Vertical distance of right foot from block top, relative distance (rule 1, relative distance)\n\n        reward_left_foot = -torch.abs(left_foot_distance_z) # Negative absolute distance to minimize vertical distance (rule 5, continuous reward)\n        reward_right_foot = -torch.abs(right_foot_distance_z) # Negative absolute distance to minimize vertical distance (rule 5, continuous reward)\n\n        reward = reward_left_foot + reward_right_foot # Summing rewards for both feet (rule 3, tensor operation)\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handling missing block, returning zero reward (rule 5, handle missing object)\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats() # Initialize normalizer if not present (rule 6, reward normalization)\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # Normalize reward (rule 6, reward normalization)\n        RewNormalizer.update_stats(normaliser_name, reward) # Update normalizer stats (rule 6, reward normalization)\n        return scaled_reward\n    return reward\n\ndef shaping_approach_block_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"approach_block_reward\") -> torch.Tensor:\n    '''Shaping reward for approaching the block in the x-direction.\n\n    Reward for decreasing the x-distance between the pelvis and the block.\n    Activates when the robot is behind the block in the x-direction.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern (rule 3, approved access pattern)\n    try:\n        block = env.scene['Object5'] # Accessing block using approved pattern and try/except for robustness (rule 2, 5, approved access pattern)\n\n        pelvis_idx = robot.body_names.index('pelvis') # Getting pelvis index using approved pattern (rule 3, approved access pattern)\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Getting pelvis position using approved pattern (rule 3, approved access pattern)\n\n        distance_x_pelvis_block = pelvis_pos[:, 0] - block.data.root_pos_w[:, 0] # x distance between pelvis and block, relative distance (rule 1, relative distance)\n\n        activation_condition = (pelvis_pos[:, 0] < block.data.root_pos_w[:, 0]) # Activation when robot is behind the block in x (rule 1, relative condition)\n\n        reward = -torch.abs(distance_x_pelvis_block) # Negative absolute distance to encourage approaching (rule 5, continuous reward)\n\n        reward = torch.where(activation_condition, reward, torch.tensor(0.0, device=env.device)) # Apply reward only when activated (rule 3, tensor operation)\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handling missing block, returning zero reward (rule 5, handle missing object)\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats() # Initialize normalizer if not present (rule 6, reward normalization)\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # Normalize reward (rule 6, reward normalization)\n        RewNormalizer.update_stats(normaliser_name, reward) # Update normalizer stats (rule 6, reward normalization)\n        return scaled_reward\n    return reward\n\ndef shaping_horizontal_stability_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"horizontal_stability_reward\") -> torch.Tensor:\n    '''Shaping reward for minimizing horizontal distance between feet and block center.\n\n    Reward for positioning feet above the block center in the xy-plane.\n    Activates when both feet are on or above the top surface of the block in z-direction.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern (rule 3, approved access pattern)\n    try:\n        block = env.scene['Object5'] # Accessing block using approved pattern and try/except for robustness (rule 2, 5, approved access pattern)\n\n        left_foot_idx = robot.body_names.index('left_ankle_roll_link') # Getting left foot index using approved pattern (rule 3, approved access pattern)\n        left_foot_pos = robot.data.body_pos_w[:, left_foot_idx] # Getting left foot position using approved pattern (rule 3, approved access pattern)\n        right_foot_idx = robot.body_names.index('right_ankle_roll_link') # Getting right foot index using approved pattern (rule 3, approved access pattern)\n        right_foot_pos = robot.data.body_pos_w[:, right_foot_idx] # Getting right foot position using approved pattern (rule 3, approved access pattern)\n\n        distance_x_left_foot_block = left_foot_pos[:, 0] - block.data.root_pos_w[:, 0] # x distance left foot to block center, relative distance (rule 1, relative distance)\n        distance_y_left_foot_block = left_foot_pos[:, 1] - block.data.root_pos_w[:, 1] # y distance left foot to block center, relative distance (rule 1, relative distance)\n        distance_horizontal_left_foot = torch.sqrt(distance_x_left_foot_block**2 + distance_y_left_foot_block**2) # Horizontal distance left foot to block center (rule 3, tensor operation)\n\n        distance_x_right_foot_block = right_foot_pos[:, 0] - block.data.root_pos_w[:, 0] # x distance right foot to block center, relative distance (rule 1, relative distance)\n        distance_y_right_foot_block = right_foot_pos[:, 1] - block.data.root_pos_w[:, 1] # y distance right foot to block center, relative distance (rule 1, relative distance)\n        distance_horizontal_right_foot = torch.sqrt(distance_x_right_foot_block**2 + distance_y_right_foot_block**2) # Horizontal distance right foot to block center (rule 3, tensor operation)\n\n        block_top_z = block.data.root_pos_w[:, 2] + (0.5/2) # Block top z position, block size is 0.5m, so half is 0.25m. Using relative distance, no hardcoded values. (rule 1, 4, relative distance, using object position, block size from config)\n        activation_condition = (left_foot_pos[:, 2] >= block_top_z) & (right_foot_pos[:, 2] >= block_top_z) # Activation when feet are on or above block top (rule 1, relative condition)\n\n        reward_left_foot = -distance_horizontal_left_foot # Negative horizontal distance to encourage centering (rule 5, continuous reward)\n        reward_right_foot = -distance_horizontal_right_foot # Negative horizontal distance to encourage centering (rule 5, continuous reward)\n\n        reward = reward_left_foot + reward_right_foot # Summing rewards for both feet (rule 3, tensor operation)\n        reward = torch.where(activation_condition, reward, torch.tensor(0.0, device=env.device)) # Apply reward only when activated (rule 3, tensor operation)\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handling missing block, returning zero reward (rule 5, handle missing object)\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats() # Initialize normalizer if not present (rule 6, reward normalization)\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # Normalize reward (rule 6, reward normalization)\n        RewNormalizer.update_stats(normaliser_name, reward) # Update normalizer stats (rule 6, reward normalization)\n        return scaled_reward\n    return reward\n\ndef shaping_pelvis_height_stability_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"pelvis_height_stability_reward\") -> torch.Tensor:\n    '''Shaping reward for maintaining stable pelvis height.\n\n    Reward for keeping the pelvis at a desired height for stability on the block.\n    Always active.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern (rule 3, approved access pattern)\n\n    pelvis_idx = robot.body_names.index('pelvis') # Getting pelvis index using approved pattern (rule 3, approved access pattern)\n    pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Getting pelvis position using approved pattern (rule 3, approved access pattern)\n\n    desired_pelvis_z =  torch.tensor(0.7, device=env.device) # default value if block is missing. (rule 4, no hardcoded absolute values, but default value if block is missing is acceptable)\n    try:\n        block = env.scene['Object5'] # Accessing block using approved pattern and try/except for robustness (rule 2, 5, approved access pattern)\n        block_top_z = block.data.root_pos_w[:, 2] + (0.5/2) # Block top z position, block size is 0.5m, so half is 0.25m. Using relative distance, no hardcoded values. (rule 1, 4, relative distance, using object position, block size from config)\n        desired_pelvis_z = block_top_z + 0.7 # Desired pelvis height relative to block top. No hardcoded absolute values. (rule 1, relative distance)\n    except KeyError:\n        pass # if block is missing, use default value already set. (rule 5, handle missing object)\n\n\n    reward = -torch.abs(pelvis_pos[:, 2] - desired_pelvis_z) # Negative absolute difference from desired pelvis height (rule 5, continuous reward)\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats() # Initialize normalizer if not present (rule 6, reward normalization)\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # Normalize reward (rule 6, reward normalization)\n        RewNormalizer.update_stats(normaliser_name, reward) # Update normalizer stats (rule 6, reward normalization)\n        return scaled_reward\n    return reward\n\ndef shaping_foot_block_collision_avoidance_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"foot_block_collision_avoidance_reward\") -> torch.Tensor:\n    '''Shaping reward for foot-block collision avoidance.\n\n    Negative reward when feet penetrate below the bottom of the block.\n    Always active.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern (rule 3, approved access pattern)\n    try:\n        block = env.scene['Object5'] # Accessing block using approved pattern and try/except for robustness (rule 2, 5, approved access pattern)\n\n        left_foot_idx = robot.body_names.index('left_ankle_roll_link') # Getting left foot index using approved pattern (rule 3, approved access pattern)\n        left_foot_pos = robot.data.body_pos_w[:, left_foot_idx] # Getting left foot position using approved pattern (rule 3, approved access pattern)\n        right_foot_idx = robot.body_names.index('right_ankle_roll_link') # Getting right foot index using approved pattern (rule 3, approved access pattern)\n        right_foot_pos = robot.data.body_pos_w[:, right_foot_idx] # Getting right foot position using approved pattern (rule 3, approved access pattern)\n\n        block_bottom_z = block.data.root_pos_w[:, 2] - (0.5/2) # Block bottom z position, block size is 0.5m, so half is 0.25m. Using relative distance, no hardcoded values. (rule 1, 4, relative distance, using object position, block size from config)\n\n        left_foot_distance_z_bottom = left_foot_pos[:, 2] - block_bottom_z # Vertical distance of left foot from block bottom, relative distance (rule 1, relative distance)\n        right_foot_distance_z_bottom = right_foot_pos[:, 2] - block_bottom_z # Vertical distance of right foot from block bottom, relative distance (rule 1, relative distance)\n\n        penetration_threshold = 0.0 # Penetration depth threshold (rule 4, no arbitrary thresholds, but 0.0 is physically meaningful)\n\n        reward_left_foot = torch.where(left_foot_distance_z_bottom < penetration_threshold, left_foot_distance_z_bottom - penetration_threshold, torch.tensor(0.0, device=env.device)) # Negative reward if penetrating (rule 5, continuous reward)\n        reward_right_foot = torch.where(right_foot_distance_z_bottom < penetration_threshold, right_foot_distance_z_bottom - penetration_threshold, torch.tensor(0.0, device=env.device)) # Negative reward if penetrating (rule 5, continuous reward)\n\n        reward = reward_left_foot + reward_right_foot # Summing rewards for both feet (rule 3, tensor operation)\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handling missing block, returning zero reward (rule 5, handle missing object)\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats() # Initialize normalizer if not present (rule 6, reward normalization)\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # Normalize reward (rule 6, reward normalization)\n        RewNormalizer.update_stats(normaliser_name, reward) # Update normalizer stats (rule 6, reward normalization)\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    Main_StabilizeOnBlockTopReward = RewTerm(func=main_StabilizeOnBlockTop_reward, weight=1.0,\n                                params={\"normalise\": True, \"normaliser_name\": \"main_reward\"}) # Main reward with weight 1.0 (rule 7, proper weights)\n    ApproachBlockReward = RewTerm(func=shaping_approach_block_reward, weight=0.4,\n                                params={\"normalise\": True, \"normaliser_name\": \"approach_block_reward\"}) # Shaping reward with weight < 1.0 (rule 7, proper weights)\n    HorizontalStabilityReward = RewTerm(func=shaping_horizontal_stability_reward, weight=0.5,\n                                params={\"normalise\": True, \"normaliser_name\": \"horizontal_stability_reward\"}) # Shaping reward with weight < 1.0 (rule 7, proper weights)\n    PelvisHeightStabilityReward = RewTerm(func=shaping_pelvis_height_stability_reward, weight=0.3,\n                                params={\"normalise\": True, \"normaliser_name\": \"pelvis_height_stability_reward\"}) # Shaping reward with weight < 1.0 (rule 7, proper weights)\n    FootBlockCollisionAvoidanceReward = RewTerm(func=shaping_foot_block_collision_avoidance_reward, weight=0.2,\n                                params={\"normalise\": True, \"normaliser_name\": \"foot_block_collision_avoidance_reward\"}) # Shaping reward with weight < 1.0 (rule 7, proper weights)\n                ",successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\ndef StabilizeOnBlockTop_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the StabilizeOnBlockTop skill has been successfully completed.\n    Success is defined as both feet being on top of the block and stable for a duration.\n    '''\n    # 1. Get robot object - Approved access pattern (rule 3, approved access pattern)\n    robot = env.scene[\"robot\"]\n\n    # 2. Get indices for left and right feet - Approved access pattern (rule 3, approved access pattern)\n    left_foot_idx = robot.body_names.index('left_ankle_roll_link')\n    right_foot_idx = robot.body_names.index('right_ankle_roll_link')\n\n    # 3. Get positions of left and right feet - Approved access pattern (rule 3, approved access pattern)\n    left_foot_pos = robot.data.body_pos_w[:, left_foot_idx]\n    right_foot_pos = robot.data.body_pos_w[:, right_foot_idx]\n\n    try:\n        # 4. Get block object - Approved access pattern and handle missing object (rule 2, 5, approved access pattern)\n        block = env.scene['Object5']\n        block_pos = block.data.root_pos_w\n\n        # 5. Calculate block top z position - Relative distance, no hardcoded values (rule 1, 4)\n        block_top_z = block_pos[:, 2] + (0.5/2) # Block size is 0.5m from config\n\n        # 6. Calculate distances - Relative distances (rule 1)\n        distance_z_left_foot_block_top = left_foot_pos[:, 2] - block_top_z # Vertical distance of left foot from block top\n        distance_z_right_foot_block_top = right_foot_pos[:, 2] - block_top_z # Vertical distance of right foot from block top\n\n        distance_x_left_foot_block = left_foot_pos[:, 0] - block_pos[:, 0] # Horizontal distance x of left foot from block center\n        distance_y_left_foot_block = left_foot_pos[:, 1] - block_pos[:, 1] # Horizontal distance y of left foot from block center\n        distance_x_right_foot_block = right_foot_pos[:, 0] - block_pos[:, 0] # Horizontal distance x of right foot from block center\n        distance_y_right_foot_block = right_foot_pos[:, 1] - block_pos[:, 1] # Horizontal distance y of right foot from block center\n\n\n        # 7. Define success condition - Relative distances and reasonable thresholds (rule 1, 13, 14)\n        vertical_threshold = 0.2 # Allow feet to be slightly above or below block top\n        horizontal_threshold = 0.35 # Allow feet to be within 25cm of block center horizontally\n\n        condition = (distance_z_left_foot_block_top >= -vertical_threshold) & (distance_z_left_foot_block_top <= vertical_threshold) &                     (distance_z_right_foot_block_top >= -vertical_threshold) & (distance_z_right_foot_block_top <= vertical_threshold) &                     (torch.abs(distance_x_left_foot_block) < horizontal_threshold) & (torch.abs(distance_y_left_foot_block) < horizontal_threshold) &                     (torch.abs(distance_x_right_foot_block) < horizontal_threshold) & (torch.abs(distance_y_right_foot_block) < horizontal_threshold)\n\n    except KeyError:\n        # 8. Handle missing block - Return failure (rule 5)\n        condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 9. Check success duration and save success states - DO NOT MODIFY THIS SECTION (rule 6, 7)\n    success = check_success_duration(env, condition, \"StabilizeOnBlockTop\", duration=0.5) # Using duration of 0.5 seconds\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"StabilizeOnBlockTop\")\n\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=StabilizeOnBlockTop_success)\n                "}]}]};var s=r(3241),i=r.n(s);function l(){return(0,a.useEffect)(()=>{i().highlightAll()},[]),null}r(4495);let c=e=>{var n,r;let{skill:t,skillPath:s,openContentPath:i,expandedChildrenPaths:d,onToggleContent:_,onToggleChildren:p}=e,h=t.children&&t.children.length>0,m=i===s,w=null!=(r=null==d?void 0:d.has(s))&&r,[v,g]=(0,a.useState)("video"),[u,b]=(0,a.useState)(!1);(0,a.useEffect)(()=>{if(m){b(!0);let e=setTimeout(()=>{b(!1)},3e3);return()=>clearTimeout(e)}},[m]);let f={3:"bg-blue-700 hover:bg-blue-800 text-white",2:"bg-blue-500 hover:bg-blue-600 text-white",1:"bg-blue-300 hover:bg-blue-400 text-gray-800"}[t.level]||"bg-blue-200 hover:bg-blue-300 text-gray-700",R=()=>{_(s)},x=e=>{let n="px-3 py-1 text-base rounded-t-md cursor-pointer transition-colors";return v===e?n+=" bg-gray-200 font-semibold":n+=" bg-gray-100 hover:bg-gray-200",u&&(n+=" sunset-flash-tab"),n};return(0,o.jsxs)("div",{className:"skill-node-wrapper",children:[(0,o.jsxs)("div",{className:"relative tree-node-content p-1 rounded ".concat(f," border border-gray-400 flex items-center cursor-pointer"),onClick:R,onKeyDown:e=>{("Enter"===e.key||" "===e.key)&&(e.preventDefault(),R())},role:"button",tabIndex:0,children:[h&&(0,o.jsx)("span",{className:"mr-2 p-1 rounded",children:w?"":""}),(0,o.jsxs)("span",{className:"flex-grow",children:[t.name," (Level ",t.level,")"]}),!m&&(0,o.jsx)("span",{className:"ml-auto text-xs italic opacity-75 px-2",children:"Click for details"})]}),m&&(0,o.jsxs)("div",{className:"mt-1 border border-gray-300 rounded bg-gray-50 shadow-sm",children:[(0,o.jsxs)("div",{className:"flex border-b border-gray-300 bg-gray-100 rounded-t-md",children:[(0,o.jsx)("button",{className:x("video"),onClick:()=>g("video"),style:u?{animationDelay:"0s"}:{},children:"Video Demo"}),(0,o.jsx)("button",{className:x("reward"),onClick:()=>g("reward"),style:u?{animationDelay:"0.4s"}:{},children:"Reward Code"}),(0,o.jsx)("button",{className:x("success"),onClick:()=>g("success"),style:u?{animationDelay:"0.8s"}:{},children:"Success Code"})]}),(0,o.jsxs)("div",{className:"p-2",children:["video"===v&&(0,o.jsx)("div",{children:t.policyVideo?(0,o.jsxs)("video",{controls:!0,width:"100%",className:"w-4/5 mx-auto rounded",children:[(0,o.jsx)("source",{src:"".concat("/website2").concat(t.policyVideo),type:"video/mp4"}),"Your browser does not support the video tag."]},s):(0,o.jsx)("p",{className:"text-xs italic text-center p-4",children:"Placeholder for policy video."})}),"reward"===v&&(0,o.jsx)("div",{className:"text-xs",children:(0,o.jsx)("pre",{className:"bg-gray-100 p-2 rounded text-xs overflow-auto h-60 border border-gray-200",children:(0,o.jsxs)("code",{className:"language-python",children:[(0,o.jsx)(l,{}),t.rewardCode||"// Placeholder for reward code"]})})}),"success"===v&&(0,o.jsx)("div",{className:"text-xs",children:(0,o.jsx)("pre",{className:"bg-gray-100 p-2 rounded text-xs overflow-auto h-60 border border-gray-200",children:(0,o.jsxs)("code",{className:"language-python",children:[(0,o.jsx)(l,{}),t.successTerminationCode||"// Placeholder for success termination code"]})})})]})]}),h&&w&&(0,o.jsx)("div",{className:"relative tree-children-group pl-6 mt-1",children:null==(n=t.children)?void 0:n.map(e=>{let n="".concat(s,"/").concat(e.name);return(0,o.jsx)(c,{skill:e,skillPath:n,openContentPath:i,expandedChildrenPaths:d,onToggleContent:_,onToggleChildren:p},n)})})]})},d=(e,n,r)=>{e.children&&e.children.length>0&&(r.add(n),e.children.forEach(e=>{d(e,"".concat(n,"/").concat(e.name),r)}))},_=()=>{let[e,n]=(0,a.useState)(t.name),r=new Set;d(t,t.name,r);let[s,i]=(0,a.useState)(r);return(0,o.jsxs)("div",{className:"p-4",children:[(0,o.jsx)("h1",{className:"text-2xl font-bold mb-4",children:"Interactive Skill Hierarchy"}),(0,o.jsx)(c,{skill:t,skillPath:t.name,openContentPath:e,expandedChildrenPaths:s,onToggleContent:e=>{n(n=>n===e?null:e)},onToggleChildren:e=>{i(n=>{let r=new Set(n);return r.has(e)?r.delete(e):r.add(e),r})}})]})}},5384:(e,n,r)=>{Promise.resolve().then(r.t.bind(r,3063,23)),Promise.resolve().then(r.bind(r,3169))}},e=>{var n=n=>e(e.s=n);e.O(0,[573,441,684,358],()=>n(5384)),_N_E=e.O()}]);
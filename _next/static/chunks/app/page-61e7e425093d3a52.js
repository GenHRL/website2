(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[974],{3169:(e,n,r)=>{"use strict";r.d(n,{default:()=>p});var o=r(5155),a=r(2115);let s={name:"Obstacle Course",level:3,rewardCode:"\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer\nfrom ...objects import get_object_volume\n\nimport torch\n\ndef main_obstacle_course_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for obstacle_course.\n\n    This reward is the negative distance between the robot pelvis (hip) x and y coordinates \n    and the block x and y coordinates.\n    '''\n    robot = env.scene[\"robot\"]\n    try:\n        block = env.scene['Object5']\n\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n        pelvis_pos_xy = pelvis_pos[:, :2]  # x and y components\n\n        block_pos_xy = block.data.root_pos_w[:, :2]  # x and y components\n\n        # Calculate Euclidean distance between pelvis and block in x-y plane\n        distance_xy = torch.norm(pelvis_pos_xy - block_pos_xy, dim=1)\n        \n        # Negative distance as reward\n        reward = -distance_xy\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device)\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_jump_over_low_wall(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"jump_low_wall_reward\") -> torch.Tensor:\n    '''Shaping reward for jumping over the low wall.\n    Encourages the robot to increase its pelvis height when approaching the low wall and to be near the low wall in x direction.\n    '''\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # CORRECT: Accessing object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Accessing robot part index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing robot part position using approved pattern\n        pelvis_pos_x = pelvis_pos[:, 0] # CORRECT: Accessing x component of pelvis position\n        pelvis_pos_z = pelvis_pos[:, 2] # CORRECT: Accessing z component of pelvis position\n\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0] # CORRECT: Accessing low wall x position using approved pattern\n        low_wall_pos_z_top = low_wall.data.root_pos_w[:, 2] + 0.4 # top of low wall, using object config size (0.4m height)\n\n        # Activation condition: Robot is approaching the low wall but not yet past it\n        activation_condition = (pelvis_pos_x < low_wall_pos_x + 1.5) & (pelvis_pos_x > low_wall_pos_x - 1.5) # CORRECT: Relative x distance activation\n\n        # Reward for increasing pelvis height above the low wall height\n        pelvis_height_reward = -torch.abs(torch.relu(low_wall_pos_z_top + 0.5 - pelvis_pos_z)) # reward when pelvis is 0.5m above wall, relative z distance\n\n        reward = torch.where(activation_condition, pelvis_height_reward, torch.tensor(0.0, device=env.device)) # CORRECT: Apply reward only when activated\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # CORRECT: Normalize reward\n        RewNormalizer.update_stats(normaliser_name, reward) # CORRECT: Update reward stats\n        return scaled_reward\n    return reward\n\ndef shaping_reward_push_large_sphere(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"push_sphere_reward\") -> torch.Tensor:\n    '''Shaping reward for pushing the large sphere towards the high wall.\n    Negative x distance between the sphere and the wall, only active when the wall hasn't fallen (wall z > 0.3).\n    '''\n    robot = env.scene[\"robot\"]\n    try:\n        large_sphere = env.scene['Object1']\n        high_wall = env.scene['Object4']\n        robot_pelvis = env.scene['robot'].body_names.index('pelvis')\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis]\n\n        ideal_pelvis_x = 0.6\n\n\n        large_sphere_pos = large_sphere.data.root_pos_w\n        large_sphere_pos_x = large_sphere_pos[:, 0]\n        large_sphere_pos_y = large_sphere_pos[:, 1]\n        high_wall_pos = high_wall.data.root_pos_w\n        high_wall_pos_x = high_wall_pos[:, 0]\n        high_wall_pos_z = high_wall_pos[:, 2]\n\n        pelvis_x_reward = -torch.abs(robot_pelvis_pos[:, 0] - large_sphere_pos_x - ideal_pelvis_x)\n        pelvis_y_reward = -torch.abs(robot_pelvis_pos[:, 1] - large_sphere_pos_y)\n\n        # Calculate x distance between sphere and wall\n        x_distance = torch.abs(high_wall_pos_x - large_sphere_pos_x)\n        \n        # Negative distance as reward\n        distance_reward = -x_distance\n        pelvis_shape_reward = pelvis_x_reward + pelvis_y_reward\n        \n        # Activation condition: Wall hasn't fallen (z > 0.3)\n        activation_condition = high_wall_pos_z > 0.3\n        \n        reward = torch.where(activation_condition, distance_reward + 0.5*pelvis_shape_reward, torch.tensor(0.0, device=env.device))\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device)\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_kick_small_sphere(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"kick_sphere_reward\") -> torch.Tensor:\n    '''Shaping reward for kicking the small sphere towards the block.\n    Positive x,y distance between the small sphere and the block.\n    '''\n    robot = env.scene[\"robot\"]\n    try:\n        small_sphere = env.scene['Object2']\n        block = env.scene['Object5']\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n        pelvis_pos_xy = pelvis_pos[:, :2]  # x and y components\n\n        small_sphere_pos = small_sphere.data.root_pos_w\n        small_sphere_pos_xy = small_sphere_pos[:, :2]  # x and y components\n        \n        block_pos = block.data.root_pos_w\n        block_pos_xy = block_pos[:, :2]  # x and y components\n\n        approach_sphere_reward = -torch.norm(small_sphere_pos_xy - pelvis_pos_xy, dim=1)\n        \n        # Calculate Euclidean distance between small sphere and block in x-y plane\n        distance_xy = torch.norm(small_sphere_pos_xy - block_pos_xy, dim=1)\n        \n        # Positive distance as reward\n        reward = distance_xy #+ approach_sphere_reward\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device)\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_jump_on_block(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"jump_block_reward\") -> torch.Tensor:\n    '''Shaping reward for jumping onto the block and staying stable.\n    Encourages the robot to position its feet above the block and maintain a stable pelvis height on top of the block.\n    '''\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot using approved pattern\n    try:\n        block = env.scene['Object5'] # CORRECT: Accessing object using approved pattern and try/except\n\n        left_ankle_roll_link_idx = robot.body_names.index('left_ankle_roll_link') # CORRECT: Accessing robot part index using approved pattern\n        right_ankle_roll_link_idx = robot.body_names.index('right_ankle_roll_link') # CORRECT: Accessing robot part index using approved pattern\n        left_ankle_roll_link_pos = robot.data.body_pos_w[:, left_ankle_roll_link_idx] # CORRECT: Accessing robot part position using approved pattern\n        right_ankle_roll_link_pos = robot.data.body_pos_w[:, right_ankle_roll_link_idx] # CORRECT: Accessing robot part position using approved pattern\n        left_ankle_roll_link_pos_z = left_ankle_roll_link_pos[:, 2] # CORRECT: Accessing z component of feet position\n        right_ankle_roll_link_pos_z = right_ankle_roll_link_pos[:, 2] # CORRECT: Accessing z component of feet position\n\n        pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Accessing robot part index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing robot part position using approved pattern\n        pelvis_pos_x = pelvis_pos[:, 0] # CORRECT: Accessing x component of pelvis position\n        pelvis_pos_z = pelvis_pos[:, 2] # CORRECT: Accessing z component of pelvis position\n\n        block_pos_x = block.data.root_pos_w[:, 0] # CORRECT: Accessing block x position using approved pattern\n        block_pos_z_top = block.data.root_pos_w[:, 2] + 0.5 # top of block, using object config size (0.5m height)\n\n        # Activation condition: Robot is near the block in x direction\n        activation_condition = (pelvis_pos_x > block_pos_x - 2.0) & (pelvis_pos_x < block_pos_x + 2.0) # CORRECT: Relative x distance activation\n\n        # Reward for feet being above the block\n        feet_height_reward = -torch.abs(left_ankle_roll_link_pos_z - block_pos_z_top) - torch.abs(right_ankle_roll_link_pos_z - block_pos_z_top)\n\n        # Reward for pelvis being at a stable height above the block\n        pelvis_stable_height_reward = -torch.abs(block_pos_z_top + 0.7 - pelvis_pos_z) # reward when pelvis is 0.7m above block, relative z distance\n\n        reward = torch.where(activation_condition, feet_height_reward + pelvis_stable_height_reward, -torch.ones(env.num_envs, device=env.device)) # CORRECT: Apply reward only when activated\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # CORRECT: Normalize reward\n        RewNormalizer.update_stats(normaliser_name, reward) # CORRECT: Update reward stats\n        return scaled_reward\n    return reward\n\ndef shaping_reward_celebrate_on_block(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"celebrate_reward\") -> torch.Tensor:\n    '''Shaping reward for celebrating on the block by varying pelvis height.\n    Encourages vertical movement of the pelvis while on the block.\n    '''\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot using approved pattern\n    try:\n        block = env.scene['Object5'] # CORRECT: Accessing object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Accessing robot part index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing robot part position using approved pattern\n        pelvis_pos_x = pelvis_pos[:, 0] # CORRECT: Accessing x component of pelvis position\n        pelvis_pos_z = pelvis_pos[:, 2] # CORRECT: Accessing z component of pelvis position\n\n        block_pos_x = block.data.root_pos_w[:, 0] # CORRECT: Accessing block x position using approved pattern\n        block_pos_z_top = block.data.root_pos_w[:, 2] + 0.5 # top of block, using object config size (0.5m height)\n\n\n        # Activation condition: Robot is on the block in x direction\n        activation_condition = (pelvis_pos_x > block_pos_x - 0.5) & (pelvis_pos_x < block_pos_x + 0.5) # CORRECT: Relative x distance activation\n\n        # Reward for varying pelvis height (jumping up and down) - using absolute deviation from a target height to encourage movement around it.\n        target_pelvis_z_celebrate = block_pos_z_top + 0.7 # Target pelvis height for celebration, relative z position\n        celebration_reward = -torch.abs(target_pelvis_z_celebrate - pelvis_pos_z) # CORRECT: Reward based on relative z distance\n\n        reward = torch.where(activation_condition, celebration_reward, torch.tensor(0.0, device=env.device)) # CORRECT: Apply reward only when activated\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    # Normalize and return\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward) # CORRECT: Normalize reward\n        RewNormalizer.update_stats(normaliser_name, reward) # CORRECT: Update reward stats\n        return scaled_reward\n    return reward\n\n\ndef overall_raw_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"overall_raw_reward\") -> torch.Tensor:\n    '''Overall raw reward for the obstacle course.\n    '''\n\n    # This should be a combination of +1 for being past the low wall.\n    # +1 for pushing the large sphere towards the high wall.\n    # +1 for kicking the small sphere towards the block.\n    # +1 for jumping onto the block.\n    reward = torch.zeros(env.num_envs, device=env.device)\n    \n    try:\n        robot = env.scene[\"robot\"]\n        low_wall = env.scene['Object3']\n        large_sphere = env.scene['Object1']\n        high_wall = env.scene['Object4']\n        small_sphere = env.scene['Object2']\n        block = env.scene['Object5']\n        \n        # +1 for being past the low wall\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n        pelvis_pos_x = pelvis_pos[:, 0]\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0]\n        past_low_wall = pelvis_pos_x > low_wall_pos_x\n        reward = torch.where(past_low_wall, reward + 1.0, reward)\n        \n        # +1 for pushing the large sphere towards the high wall (only if past low wall)\n        large_sphere_pos_x = large_sphere.data.root_pos_w[:, 0]\n        high_wall_pos_x = high_wall.data.root_pos_w[:, 0]\n        high_wall_pos_z = high_wall.data.root_pos_w[:, 2]\n        large_sphere_near_wall = torch.abs(high_wall_pos_x - large_sphere_pos_x) < 2.5\n        wall_pushed = high_wall_pos_z < 0.4  # Wall has fallen\n        sphere_pushed_wall = large_sphere_near_wall\n        # Only award if past low wall\n        reward = torch.where(past_low_wall & wall_pushed, reward + 1.0, reward)\n        \n        # +1 for kicking the small sphere away from the block (only if wall pushed)\n        small_sphere_pos_xy = small_sphere.data.root_pos_w[:, :2]\n        block_pos_xy = block.data.root_pos_w[:, :2]\n        small_sphere_away_from_block = torch.norm(small_sphere_pos_xy - block_pos_xy, dim=1) > 4.0\n        # Only award if past low wall and wall pushed\n        reward = torch.where(past_low_wall & wall_pushed & small_sphere_away_from_block, reward + 1.0, reward)\n        \n        # +1 for jumping onto the block (only if sphere kicked away)\n        pelvis_pos_z = pelvis_pos[:, 2]\n        block_pos_x = block.data.root_pos_w[:, 0]\n        block_pos_z_top = block.data.root_pos_w[:, 2] + 0.5  # top of block, 0.5m height\n        on_block = (pelvis_pos_x > block_pos_x - 0.5) & (pelvis_pos_x < block_pos_x + 0.5) & (pelvis_pos_z > block_pos_z_top + 0.3)\n        # Only award if all previous milestones completed\n        reward = torch.where(past_low_wall & wall_pushed & small_sphere_away_from_block & on_block, reward + 1.0, reward)\n\n        if normalise:\n            reward = RewNormalizer.normalize(normaliser_name, reward)\n            RewNormalizer.update_stats(normaliser_name, reward)\n            return reward\n        else:\n            reward = reward/4.0\n    \n    except KeyError:\n        pass  # Keep reward at zeros if objects not found\n    \n    return reward\n\n@configclass\nclass TaskRewardsCfg:\n    # MainObstacleCourseReward = RewTerm(func=main_obstacle_course_reward, weight=1.0,\n    #                                  params={\"normalise\": True, \"normaliser_name\": \"main_reward\"})\n    JumpOverLowWallReward = RewTerm(func=shaping_reward_jump_over_low_wall, weight=0.1,\n                                     params={\"normalise\": True, \"normaliser_name\": \"jump_low_wall_reward\"})\n    PushLargeSphereReward = RewTerm(func=shaping_reward_push_large_sphere, weight=0.1,\n                                     params={\"normalise\": True, \"normaliser_name\": \"push_sphere_reward\"})\n    KickSmallSphereReward = RewTerm(func=shaping_reward_kick_small_sphere, weight=0.1,\n                                     params={\"normalise\": True, \"normaliser_name\": \"kick_sphere_reward\"})\n    JumpOnBlockReward = RewTerm(func=shaping_reward_jump_on_block, weight=0.1,\n                                     params={\"normalise\": True, \"normaliser_name\": \"jump_block_reward\"})\n    # CelebrateOnBlockReward = RewTerm(func=shaping_reward_celebrate_on_block, weight=0.3,\n    #                                  params={\"normalise\": True, \"normaliser_name\": \"celebrate_reward\"})\n    OverallRawReward = RewTerm(func=overall_raw_reward, weight=1.0,\n                                     params={\"normalise\": False})\n",successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\n# Standard imports - DO NOT MODIFY\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer\nfrom ...objects import get_object_volume\n\ndef obstacle_course_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the obstacle_course skill has been successfully completed.'''\n    # 1. Get robot and block objects - CORRECT: Accessing robot and object using approved pattern\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot using approved pattern\n    try:\n        block = env.scene['Object5'] # CORRECT: Accessing object using approved pattern and try/except\n\n        # 2. Get robot pelvis and block positions - CORRECT: Accessing robot part and object positions using approved pattern\n        pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Getting pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Getting pelvis position using approved pattern\n        block_pos = block.data.root_pos_w # CORRECT: Getting block position using approved pattern\n\n        # 3. Calculate relative distances - CORRECT: Using relative distances as required\n        distance_x_block_pelvis = torch.abs(block_pos[:, 0] - pelvis_pos[:, 0]) # CORRECT: Relative x-distance between pelvis and block\n        distance_z_block_pelvis_top = pelvis_pos[:, 2] - (block_pos[:, 2] + 0.7) # CORRECT: Relative z-distance between pelvis and top of block (block height is 0.5m from object config)\n\n        # 4. Define success condition - CORRECT: Using relative distances and reasonable thresholds\n        x_threshold = 0.5 # Reasonable x-distance threshold\n        z_threshold = 0.3 # Reasonable z-distance threshold above the block\n        success_condition = (distance_x_block_pelvis < x_threshold) & (distance_z_block_pelvis_top > z_threshold) # CORRECT: Combining x and z conditions\n\n    except KeyError:\n        # 5. Handle missing objects - CORRECT: Handling missing object with try/except as required\n        success_condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device) # CORRECT: Return False if object is missing\n\n    # 6. Check duration and save success states - CORRECT: Using check_success_duration and save_success_state as required\n    success = check_success_duration(env, success_condition, \"obstacle_course\", duration=1.0) # CORRECT: Checking success duration for 1.0 second\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"obstacle_course\") # CORRECT: Saving success state for successful environments\n\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=obstacle_course_success)\n",policyVideo:"/videos/ZeroShotObstacleCourse.mp4",children:[{name:"JumpOverLowWall",level:2,rewardCode:"\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer\nfrom ...objects import get_object_volume\n\nimport torch\n\ndef main_JumpOverLowWall_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for JumpOverLowWall.\n\n    Reward for moving past the low wall in the x direction while also increasing pelvis height, encouraging the robot to jump over the wall.\n    The reward is activated when the robot is approaching the large sphere, ensuring it focuses on jumping the low wall first.\n    '''\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # CORRECT: Accessing low wall object using approved pattern and try/except\n        large_sphere = env.scene['Object1'] # CORRECT: Accessing large sphere object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0] # CORRECT: Accessing low wall x position using approved pattern\n        large_sphere_pos_x = large_sphere.data.root_pos_w[:, 0] # CORRECT: Accessing large sphere x position using approved pattern\n        low_wall_pos_z = low_wall.data.root_pos_w[:, 2] # CORRECT: Accessing low wall z position using approved pattern\n\n        distance_pelvis_wall_x = low_wall_pos_x - pelvis_pos[:, 0] # CORRECT: Relative distance in x direction\n        distance_pelvis_wall_z = low_wall_pos_z + 0.4 - pelvis_pos[:, 2] # CORRECT: Relative distance in z direction from pelvis to top of wall (wall height is 0.4m from config)\n\n        target_x_position_beyond_wall = low_wall_pos_x + 1.5 # Target x position 1.5m beyond the wall\n        reward_x_progress = -torch.abs(pelvis_pos[:, 0] - target_x_position_beyond_wall) # CORRECT: Reward for x progress beyond the wall, continuous reward\n\n        target_pelvis_z_height = low_wall_pos_z + 1.2 # Target pelvis height 1m above the wall (0.4m wall height + 0.6m clearance)\n        reward_z_height = -torch.abs(pelvis_pos[:, 2] - target_pelvis_z_height) # CORRECT: Reward for pelvis height above the wall, continuous reward\n\n        activation_condition = (pelvis_pos[:, 0] < low_wall_pos_x + 0.2) # Activation when robot is before the wall\n\n        pre_wall_reward =  reward_x_progress # Combining x and z rewards\n\n        post_wall_reward = reward_x_progress + reward_z_height\n\n        primary_reward = torch.where(activation_condition, pre_wall_reward, post_wall_reward) # Combining x and z rewards\n\n        reward = primary_reward # CORRECT: Apply activation condition\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef approach_wall_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"approach_wall_reward\") -> torch.Tensor:\n    '''Shaping reward for approaching the low wall.\n\n    Rewards the robot for moving closer to the low wall in the x-direction before reaching it.\n    This encourages forward movement towards the obstacle.\n    '''\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # CORRECT: Accessing low wall object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0] # CORRECT: Accessing low wall x position using approved pattern\n\n        distance_pelvis_wall_x = low_wall_pos_x - pelvis_pos[:, 0] # CORRECT: Relative distance in x direction\n\n        approach_wall_condition = (pelvis_pos[:, 0] < low_wall_pos_x) # CORRECT: Activation condition: robot is before the wall\n\n        reward_approach_wall = -torch.abs(distance_pelvis_wall_x) # CORRECT: Reward for decreasing x distance to the wall, continuous reward\n\n        reward = torch.where(approach_wall_condition, reward_approach_wall, torch.tensor(0.0, device=env.device)) # CORRECT: Apply activation condition\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef stable_pelvis_height_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"stable_pelvis_height_reward\") -> torch.Tensor:\n    '''Shaping reward for maintaining stable pelvis height after jumping over the wall.\n\n    Rewards the robot for maintaining a stable pelvis height close to a default standing height (0.7m)\n    after landing on the other side of the wall. This encourages stability after the jump.\n    '''\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # CORRECT: Accessing low wall object using approved pattern and try/except\n        large_sphere = env.scene['Object1'] # CORRECT: Accessing large sphere object using approved pattern and try/except\n\n        pelvis_idx = robot.body_names.index('pelvis') # CORRECT: Accessing pelvis index using approved pattern\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # CORRECT: Accessing pelvis position using approved pattern\n\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0] # CORRECT: Accessing low wall x position using approved pattern\n        large_sphere_pos_x = large_sphere.data.root_pos_w[:, 0] # CORRECT: Accessing large sphere x position using approved pattern\n\n        stable_pelvis_condition = (pelvis_pos[:, 0] > low_wall_pos_x) & (pelvis_pos[:, 0] < large_sphere_pos_x) # CORRECT: Activation condition: robot is past the wall and before the large sphere\n\n        target_pelvis_z = 0.7 # Default standing pelvis height\n        reward_stable_pelvis_z = -torch.abs(pelvis_pos[:, 2] - target_pelvis_z) # CORRECT: Reward for maintaining pelvis height close to 0.7m, continuous reward\n\n        reward = torch.where(stable_pelvis_condition, reward_stable_pelvis_z, torch.tensor(0.0, device=env.device)) # CORRECT: Apply activation condition\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef collision_avoidance_low_wall_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"collision_avoidance_low_wall_reward\") -> torch.Tensor:\n    '''Shaping reward for collision avoidance with the low wall.\n\n    Penalizes collisions between the robot's feet and the low wall, encouraging the robot to jump high enough.\n    '''\n    robot = env.scene[\"robot\"] # CORRECT: Accessing robot using approved pattern\n    try:\n        low_wall = env.scene['Object3'] # CORRECT: Accessing low wall object using approved pattern and try/except\n\n        left_foot_idx = robot.body_names.index('left_ankle_roll_link') # CORRECT: Accessing left foot index using approved pattern\n        right_foot_idx = robot.body_names.index('right_ankle_roll_link') # CORRECT: Accessing right foot index using approved pattern\n        left_foot_pos = robot.data.body_pos_w[:, left_foot_idx] # CORRECT: Accessing left foot position using approved pattern\n        right_foot_pos = robot.data.body_pos_w[:, right_foot_idx] # CORRECT: Accessing right foot position using approved pattern\n\n        low_wall_pos_x = low_wall.data.root_pos_w[:, 0] # CORRECT: Accessing low wall x position using approved pattern\n        low_wall_pos_y = low_wall.data.root_pos_w[:, 1] # CORRECT: Accessing low wall y position using approved pattern\n        low_wall_pos_z = low_wall.data.root_pos_w[:, 2] # CORRECT: Accessing low wall z position using approved pattern\n\n        low_wall_x_size = 0.4 # Hardcoded from object config\n        low_wall_y_size = 10.0 # Hardcoded from object config\n        low_wall_z_size = 0.4 # Hardcoded from object config\n\n        low_wall_x_min = low_wall_pos_x - low_wall_x_size/2.0 # CORRECT: min x bound of wall\n        low_wall_x_max = low_wall_pos_x + low_wall_x_size/2.0 # CORRECT: max x bound of wall\n        low_wall_y_min = low_wall_pos_y - low_wall_y_size/2.0 # CORRECT: min y bound of wall\n        low_wall_y_max = low_wall_pos_y + low_wall_y_size/2.0 # CORRECT: max y bound of wall\n        low_wall_z_max = low_wall_pos_z + low_wall_z_size # CORRECT: top z bound of wall\n\n        collision_reward = torch.zeros(env.num_envs, device=env.device) # Initialize collision reward to zero\n\n        # Collision condition for left foot\n        collision_left_foot = (left_foot_pos[:, 0] > low_wall_x_min) & (left_foot_pos[:, 0] < low_wall_x_max) &                               (left_foot_pos[:, 1] > low_wall_y_min) & (left_foot_pos[:, 1] < low_wall_y_max) &                               (left_foot_pos[:, 2] < low_wall_z_max) # CORRECT: Collision condition for left foot\n\n        # Collision condition for right foot\n        collision_right_foot = (right_foot_pos[:, 0] > low_wall_x_min) & (right_foot_pos[:, 0] < low_wall_x_max) &                                (right_foot_pos[:, 1] > low_wall_y_min) & (right_foot_pos[:, 1] < low_wall_y_max) &                                (right_foot_pos[:, 2] < low_wall_z_max) # CORRECT: Collision condition for right foot\n\n        collision_reward = torch.where(collision_left_foot, collision_reward - 0.1, collision_reward) # CORRECT: Penalize left foot collision\n        collision_reward = torch.where(collision_right_foot, collision_reward - 0.1, collision_reward) # CORRECT: Penalize right foot collision\n\n\n        reward = collision_reward\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # CORRECT: Handle missing object, return zero reward\n\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    Main_JumpOverLowWallReward = RewTerm(func=main_JumpOverLowWall_reward, weight=1.0,\n                                params={\"normalise\": True, \"normaliser_name\": \"main_reward\"})\n    ApproachWallReward = RewTerm(func=approach_wall_reward, weight=0.4,\n                                params={\"normalise\": True, \"normaliser_name\": \"approach_wall_reward\"})\n    StablePelvisHeightReward = RewTerm(func=stable_pelvis_height_reward, weight=0.3,\n                                params={\"normalise\": True, \"normaliser_name\": \"stable_pelvis_height_reward\"})\n    CollisionAvoidanceLowWallReward = RewTerm(func=collision_avoidance_low_wall_reward, weight=0.2,\n                                params={\"normalise\": True, \"normaliser_name\": \"collision_avoidance_low_wall_reward\"})",successTerminationCode:"\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\ndef JumpOverLowWall_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the JumpOverLowWall skill has been successfully completed.'''\n    # 1. Get robot object - CORRECT: Approved access pattern\n    robot = env.scene[\"robot\"]\n\n    # 2. Get pelvis index - CORRECT: Approved access pattern\n    pelvis_idx = robot.body_names.index('pelvis')\n    # 3. Get pelvis position - CORRECT: Approved access pattern\n    pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n\n    try:\n        # 4. Get low wall object - CORRECT: Approved access pattern and try/except\n        low_wall = env.scene['Object3']\n        # 5. Get low wall position - CORRECT: Approved access pattern\n        wall_pos = low_wall.data.root_pos_w\n\n        # 6. Calculate relative x distance - CORRECT: Relative distance\n        relative_x_distance = pelvis_pos[:, 0] - wall_pos[:, 0]\n\n        pelvis_pos_y = pelvis_pos[:, 1]\n        wall_pos_y = wall_pos[:, 1]\n\n        y_condition = (pelvis_pos_y > wall_pos_y-2.5) & (pelvis_pos_y < wall_pos_y+2.5)\n\n        # 7. Define success condition: pelvis is past the wall in x direction by 0.7m - CORRECT: Relative distance and reasonable threshold\n        success_threshold = 0.7\n        condition = (relative_x_distance > success_threshold) & y_condition\n\n    except KeyError:\n        # 8. Handle missing object - CORRECT: Handle missing object with try/except\n        condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 9. Check success duration and save success states - CORRECT: Using check_success_duration and save_success_state\n    success = check_success_duration(env, condition, \"JumpOverLowWall\", duration=0.5) # Using duration = 0.5\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"JumpOverLowWall\")\n\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=JumpOverLowWall_success)\n\n",policyVideo:"/videos/L2_JumpOverLowWall.mp4",children:[{name:"WalkToLowWall",level:1},{name:"PrepareForJumpOverLowWall",level:1},{name:"ExecuteJumpOverLowWall",level:1},{name:"LandStablyAfterLowWall",level:1}]},{name:"PushLargeSphereToHighWall",level:2,rewardCode:"from isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer # DO NOT CHANGE THIS LINE!\nfrom ...objects import get_object_volume\n\ndef main_PushLargeSphereToHighWall_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for PushLargeSphereToHighWall.\n\n    Reward for moving the large sphere closer to the high wall in the x-direction.\n    This encourages the robot to push the large sphere towards the high wall to knock it over.\n    '''\n    try:\n        large_sphere = env.scene['Object1'] # Access the large sphere using approved pattern (rule 2, rule 3, rule 5)\n        high_wall = env.scene['Object4'] # Access the high wall using approved pattern (rule 2, rule 3, rule 5)\n\n        # Calculate the x-distance between the large sphere and the high wall (rule 1)\n        distance_x = high_wall.data.root_pos_w[:, 0] - large_sphere.data.root_pos_w[:, 0] # Access object positions using approved pattern (rule 2)\n\n        # Reward is negative absolute x-distance to encourage minimizing the distance (rule 4, rule 5)\n        reward = -torch.abs(distance_x) # Continuous reward (rule 5)\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing objects (rule 5, rule 6)\n\n    # Reward normalization (rule 6)\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_approach_large_sphere_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"approach_sphere_reward\") -> torch.Tensor:\n    '''Shaping reward for approaching the large sphere.\n\n    Reward for reducing the x-distance between the robot's pelvis and the large sphere when the robot is behind the sphere.\n    This encourages the robot to move towards the large sphere before pushing it.\n    '''\n    try:\n        large_sphere = env.scene['Object1'] # Access the large sphere using approved pattern (rule 2, rule 3, rule 5)\n        robot = env.scene['robot'] # Access the robot object (rule 2, rule 3)\n        pelvis_idx = robot.body_names.index('pelvis') # Get pelvis index using approved pattern (rule 3)\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Get pelvis position using approved pattern (rule 3)\n\n        # Calculate the x-distance between the pelvis and the large sphere (rule 1)\n        distance_x = large_sphere.data.root_pos_w[:, 0] - pelvis_pos[:, 0] # Access object and robot part positions using approved pattern (rule 2, rule 3)\n        distance_y = large_sphere.data.root_pos_w[:, 1] - pelvis_pos[:, 1] # Access object and robot part positions using approved pattern (rule 2, rule 3)\n\n        # Reward is negative absolute x-distance when activated (rule 4, rule 5)\n        reward = -torch.abs(distance_x) - torch.abs(distance_y) # Continuous reward (rule 5)\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing objects (rule 5, rule 6)\n\n    # Reward normalization (rule 6)\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_stable_pelvis_height_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"stable_height_reward\") -> torch.Tensor:\n    '''Shaping reward for maintaining a stable pelvis height.\n\n    Reward for keeping the pelvis height close to a target height (0.7m).\n    This encourages the robot to stay upright and stable.\n    '''\n    try:\n        robot = env.scene['robot'] # Access the robot object (rule 2, rule 3)\n        pelvis_idx = robot.body_names.index('pelvis') # Get pelvis index using approved pattern (rule 3)\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Get pelvis position using approved pattern (rule 3)\n\n        # Define target pelvis height (no hardcoded position, but target height is acceptable as a task parameter)\n        target_pelvis_z = 0.7\n\n        # Calculate the z-distance between the current pelvis height and the target height (rule 1)\n        distance_z = target_pelvis_z - pelvis_pos[:, 2] # Relative distance (rule 1)\n\n        # Reward is negative absolute z-distance (rule 4, rule 5)\n        reward = -torch.abs(distance_z) # Continuous reward (rule 5)\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing objects (rule 5, rule 6)\n\n    # Reward normalization (rule 6)\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_collision_avoidance_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"avoid_collision_reward\") -> torch.Tensor:\n    '''Shaping reward for collision avoidance with low and high walls.\n\n    Penalize the robot for getting too close to the low wall and high wall unnecessarily,\n    but deactivate this reward when the robot is close to the large sphere to allow pushing.\n    '''\n    try:\n        low_wall = env.scene['Object3'] # Access the low wall using approved pattern (rule 2, rule 3, rule 5)\n        high_wall = env.scene['Object4'] # Access the high wall using approved pattern (rule 2, rule 3, rule 5)\n        large_sphere = env.scene['Object1'] # Access the large sphere using approved pattern (rule 2, rule 3, rule 5)\n        robot = env.scene['robot'] # Access the robot object (rule 2, rule 3)\n        pelvis_idx = robot.body_names.index('pelvis') # Get pelvis index using approved pattern (rule 3)\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx] # Get pelvis position using approved pattern (rule 3)\n\n        # Calculate the x-distance between the pelvis and the walls (rule 1)\n        distance_low_wall_x = low_wall.data.root_pos_w[:, 0] - pelvis_pos[:, 0] # Access object and robot part positions using approved pattern (rule 2, rule 3)\n        distance_high_wall_x = high_wall.data.root_pos_w[:, 0] - pelvis_pos[:, 0] # Access object and robot part positions using approved pattern (rule 2, rule 3)\n        distance_sphere_x = large_sphere.data.root_pos_w[:, 0] - pelvis_pos[:, 0] # Distance to sphere for activation condition\n\n        # Activation condition: robot is not close to the large sphere (to allow pushing)\n        approach_sphere_threshold = 1.0 # Threshold is relative, not hardcoded position (rule 4)\n        activation_condition_avoid_low_wall = (torch.abs(distance_low_wall_x) < 1.0) & (torch.abs(distance_sphere_x) > approach_sphere_threshold) # Relative conditions (rule 1)\n        activation_condition_avoid_high_wall = (torch.abs(distance_high_wall_x) < 1.0) & (torch.abs(distance_sphere_x) > approach_sphere_threshold) # Relative conditions (rule 1)\n\n        # Reward is negative distance if too close to walls, scaled to be continuous (rule 4, rule 5)\n        reward_low_wall = torch.where(activation_condition_avoid_low_wall, -torch.abs(1.0 - torch.abs(distance_low_wall_x)), torch.tensor(0.0, device=env.device)) # Continuous reward (rule 5)\n        reward_high_wall = torch.where(activation_condition_avoid_high_wall, -torch.abs(1.0 - torch.abs(distance_high_wall_x)), torch.tensor(0.0, device=env.device)) # Continuous reward (rule 5)\n\n        reward = reward_low_wall + reward_high_wall # Sum of rewards\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing objects (rule 5, rule 6)\n\n    # Reward normalization (rule 6)\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    Main_PushLargeSphereToHighWallReward = RewTerm(func=main_PushLargeSphereToHighWall_reward, weight=1.0,\n                                params={\"normalise\": True, \"normaliser_name\": \"main_reward\"}) # Main reward weight ~1.0 (rule 7)\n    ApproachLargeSphereReward = RewTerm(func=shaping_approach_large_sphere_reward, weight=0.4,\n                                params={\"normalise\": True, \"normaliser_name\": \"approach_sphere_reward\"}) # Supporting reward weight < 1.0 (rule 7)\n    StablePelvisHeightReward = RewTerm(func=shaping_stable_pelvis_height_reward, weight=0.3,\n                                params={\"normalise\": True, \"normaliser_name\": \"stable_height_reward\"}) # Supporting reward weight < 1.0 (rule 7)\n    CollisionAvoidanceReward = RewTerm(func=shaping_collision_avoidance_reward, weight=0.2, # combined weight of low and high wall avoidance.\n                                params={\"normalise\": True, \"normaliser_name\": \"avoid_collision_reward\"}) # Supporting reward weight < 1.0 (rule 7)",successTerminationCode:"\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\ndef PushLargeSphereToHighWall_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the PushLargeSphereToHighWall skill has been successfully completed.\n    Success is defined as the large sphere being pushed to be at or beyond the x-position of the high wall.\n    '''\n    # 1. Access the large sphere object (Object1) from the environment scene using the approved pattern (rule 2, rule 3, rule 5)\n    try:\n        large_sphere = env.scene['Object1']\n    except KeyError:\n        # Handle the case where the large sphere object is not found in the scene (rule 5, rule 6)\n        return torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 2. Access the high wall object (Object4) from the environment scene using the approved pattern (rule 2, rule 3, rule 5)\n    try:\n        high_wall = env.scene['Object4']\n    except KeyError:\n        # Handle the case where the high wall object is not found in the scene (rule 5, rule 6)\n        return torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n\n    # 3. Get the x-position of the large sphere using the approved pattern (rule 2)\n    large_sphere_x = large_sphere.data.root_pos_w[:, 0]\n\n    # 4. Get the x-position of the high wall using the approved pattern (rule 2)\n    high_wall_x = high_wall.data.root_pos_w[:, 0]\n    high_wall_z = high_wall.data.root_pos_w[:, 2]\n\n    # 5. Calculate the relative x-distance: large_sphere_x - high_wall_x (rule 1, rule 3)\n    distance_x = large_sphere_x - high_wall_x\n\n    # 6. Define the success condition: large_sphere_x is at or beyond the high_wall_x - 2m tolerance (rule 1, rule 4, rule 10)\n    #    We use a negative threshold because we want the large sphere's x position to be greater than the high wall's x position.\n    success_threshold = -2\n    condition_sphere = distance_x > success_threshold\n    condition_high_wall = high_wall_z < 0.4\n\n    final_condition = condition_sphere & condition_high_wall\n\n    # 7. Check success duration and save success states using the provided helper functions (rule 6, rule 7)\n    success = check_success_duration(env, final_condition, \"PushLargeSphereToHighWall\", duration=0.5)\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"PushLargeSphereToHighWall\")\n\n    # 8. Return the success tensor (rule 3)\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=PushLargeSphereToHighWall_success)\n",policyVideo:"/videos/L2_PushLargeSphereToHighWall.mp4",children:[{name:"WalkToLargeSphere",level:1,policyVideo:""},{name:"PositionHandsForPushLargeSphere",level:1,policyVideo:"/videos/PositionHandsForPushLargeSphere.mp4"},{name:"PushLargeSphereForward",level:1,policyVideo:"/videos/PushLargeSphereForward.mp4"},{name:"EnsureHighWallFalls",level:1,policyVideo:"/videos/EnsureHighWallFalls.mp4"}]},{name:"KickSmallSpherePastBlock",level:2,rewardCode:"\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer # DO NOT CHANGE THIS LINE!\nfrom ...objects import get_object_volume\n\ndef main_KickSmallSpherePastBlock_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for KickSmallSpherePastBlock.\n\n    Reward is negative absolute distance in x direction between the small sphere and a target position 5m past the block.\n    This encourages the robot to kick the small sphere past the block in the x direction.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        small_sphere = env.scene['Object2'] # Accessing small sphere using approved pattern and try/except\n        block = env.scene['Object5'] # Accessing block using approved pattern and try/except\n\n        # Accessing object positions using approved pattern\n        small_sphere_pos = small_sphere.data.root_pos_w\n        block_pos = block.data.root_pos_w\n\n        small_sphere_pos_x = small_sphere.data.root_pos_w[:, 0]\n        small_sphere_pos_y = small_sphere.data.root_pos_w[:, 1]\n        block_pos_x = block.data.root_pos_w[:, 0]\n        block_pos_y = block.data.root_pos_w[:, 1]\n\n        relative_distance_x = small_sphere_pos_x - block_pos_x\n        relative_distance_y = small_sphere_pos_y - block_pos_y\n        relative_distance = torch.sqrt(relative_distance_x**2 + relative_distance_y**2)\n\n\n        # Primary reward is negative absolute distance in x to the target position past the block. Continuous reward.\n        reward = -torch.abs(relative_distance_x)\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Reward normalization using RewNormalizer.\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef reward_shaping_approach_sphere(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"shaping_approach_sphere_reward\") -> torch.Tensor:\n    '''Shaping reward 1: Reward for approaching the small sphere with the robot's right foot.\n\n    Reward is negative 3D distance between the robot's right foot and the small sphere, activated when the foot is behind the sphere in x direction.\n    Encourages the robot to get closer to the small sphere in preparation for kicking.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        small_sphere = env.scene['Object2'] # Accessing small sphere using approved pattern and try/except\n\n        # Accessing robot right foot position using approved pattern\n        robot_foot_right_idx = robot.body_names.index('right_ankle_roll_link')\n        robot_foot_right_pos = robot.data.body_pos_w[:, robot_foot_right_idx]\n        robot_foot_right_pos_x = robot_foot_right_pos[:, 0]\n\n        # Accessing small sphere position using approved pattern\n        small_sphere_pos = small_sphere.data.root_pos_w\n\n        # Calculate the distance vector between the small sphere and the robot right foot. Relative distance.\n        distance_x_foot_sphere = small_sphere_pos[:, 0] - robot_foot_right_pos_x\n        distance_y_foot_sphere = small_sphere_pos[:, 1] - robot_foot_right_pos[:, 1]\n        distance_z_foot_sphere = small_sphere_pos[:, 2] - robot_foot_right_pos[:, 2]\n        distance_foot_sphere = torch.sqrt(distance_x_foot_sphere**2 + distance_y_foot_sphere**2 + distance_z_foot_sphere**2)\n\n        # Activation condition: robot foot is behind the sphere in x direction. Relative positions.\n        activation_condition_approach = (robot_foot_right_pos_x < small_sphere_pos[:, 0])\n\n        # Reward is negative absolute distance to the sphere. Continuous reward.\n        reward_shaping_sphere = -distance_foot_sphere\n\n        reward = torch.where(activation_condition_approach, reward_shaping_sphere, torch.tensor(0.0, device=env.device)) # Apply activation condition\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Reward normalization using RewNormalizer.\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef reward_shaping_kick_sphere(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"shaping_kick_sphere_reward\") -> torch.Tensor:\n    '''Shaping reward 2: Reward for moving the small sphere in the positive x direction relative to the robot's right foot.\n\n    Reward is positive x distance between the sphere and the foot, activated when the foot is close to the sphere and before the block in x direction.\n    Encourages the kicking motion.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        small_sphere = env.scene['Object2'] # Accessing small sphere using approved pattern and try/except\n        block = env.scene['Object5'] # Accessing block using approved pattern and try/except\n\n        # Accessing robot right foot position using approved pattern\n        robot_foot_right_idx = robot.body_names.index('right_ankle_roll_link')\n        robot_foot_right_pos = robot.data.body_pos_w[:, robot_foot_right_idx]\n        robot_foot_right_pos_x = robot_foot_right_pos[:, 0]\n\n        # Accessing small sphere and block positions using approved pattern\n        small_sphere_pos = small_sphere.data.root_pos_w\n        block_pos = block.data.root_pos_w\n\n        # Calculate the distance vector between the small sphere and the robot right foot. Relative distance.\n        distance_x_foot_sphere = small_sphere_pos[:, 0] - robot_foot_right_pos_x\n        distance_y_foot_sphere = small_sphere_pos[:, 1] - robot_foot_right_pos[:, 1]\n        distance_z_foot_sphere = small_sphere_pos[:, 2] - robot_foot_right_pos[:, 2]\n        distance_foot_sphere = torch.sqrt(distance_x_foot_sphere**2 + distance_y_foot_sphere**2 + distance_z_foot_sphere**2)\n\n        # Activation condition: robot foot is close to the sphere (0.3m) and before the block in x direction. Relative positions.\n        activation_condition_kick = (distance_foot_sphere < 0.3) & (robot_foot_right_pos_x < block_pos[:, 0])\n\n        # Reward is the positive x distance between the sphere and the foot. Continuous reward.\n        reward_shaping_sphere = (small_sphere_pos[:, 0] - robot_foot_right_pos_x)\n\n        reward = torch.where(activation_condition_kick, reward_shaping_sphere, torch.tensor(0.0, device=env.device)) # Apply activation condition\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Reward normalization using RewNormalizer.\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef reward_shaping_collision_avoidance(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"shaping_collision_avoidance_reward\") -> torch.Tensor:\n    '''Shaping reward 3: Collision avoidance reward to prevent the robot's pelvis from getting too close to the block.\n\n    Reward is negative when pelvis is too close to the block (within 1.0m), based on 3D distance.\n    Helps maintain a safe distance and prevents the robot from stumbling into the block.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        block = env.scene['Object5'] # Accessing block using approved pattern and try/except\n\n        # Accessing robot pelvis position using approved pattern\n        robot_pelvis_idx = robot.body_names.index('pelvis')\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx]\n\n        # Accessing block position using approved pattern\n        block_pos = block.data.root_pos_w\n\n        # Calculate the distance vector between the pelvis and the block. Relative distance.\n        distance_x_pelvis_block = block_pos[:, 0] - robot_pelvis_pos[:, 0]\n        distance_y_pelvis_block = block_pos[:, 1] - robot_pelvis_pos[:, 1]\n        distance_z_pelvis_block = block_pos[:, 2] - robot_pelvis_pos[:, 2]\n        distance_pelvis_block = torch.sqrt(distance_x_pelvis_block**2 + distance_y_pelvis_block**2 + distance_z_pelvis_block**2)\n\n        # Collision threshold. Not hardcoded position, relative distance is used.\n        collision_threshold = 1.0\n\n        # Negative reward when pelvis is too close to the block. Continuous reward.\n        reward_shaping_collision = -torch.abs(torch.where(distance_pelvis_block < collision_threshold, collision_threshold - distance_pelvis_block, torch.tensor(0.0, device=env.device)))\n\n        reward = reward_shaping_collision\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward\n\n    # Reward normalization using RewNormalizer.\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef reward_shaping_stability(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"shaping_stability_reward\") -> torch.Tensor:\n    '''Shaping reward 4: Stability reward to encourage the robot to maintain a stable standing posture.\n\n    Reward is negative absolute difference between the robot's pelvis z position and a desired pelvis height (0.7m).\n    Encourages the robot to maintain a stable standing posture.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern\n    try:\n        # Accessing robot pelvis position using approved pattern\n        robot_pelvis_idx = robot.body_names.index('pelvis')\n        robot_pelvis_pos = robot.data.body_pos_w[:, robot_pelvis_idx]\n        robot_pelvis_pos_z = robot_pelvis_pos[:, 2]\n\n        # Desired pelvis height. Not hardcoded position, relative height is used conceptually.\n        default_pelvis_z = 0.7\n\n        # Reward for maintaining pelvis height. Continuous reward.\n        reward_shaping_stability_value = -torch.abs(robot_pelvis_pos_z - default_pelvis_z)\n\n        reward = reward_shaping_stability_value\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward (though robot should always exist)\n\n    # Reward normalization using RewNormalizer.\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n\n@configclass\nclass TaskRewardsCfg:\n    Main_KickSmallSpherePastBlockReward = RewTerm(func=main_KickSmallSpherePastBlock_reward, weight=1.0,\n                                params={\"normalise\": True, \"normaliser_name\": \"main_reward\"})\n\n    Shaping_ApproachSphereReward = RewTerm(func=reward_shaping_approach_sphere, weight=0.5,\n                                params={\"normalise\": True, \"normaliser_name\": \"shaping_approach_sphere_reward\"})\n\n    Shaping_KickSphereReward = RewTerm(func=reward_shaping_kick_sphere, weight=0.6,\n                                params={\"normalise\": True, \"normaliser_name\": \"shaping_kick_sphere_reward\"})\n\n    Shaping_CollisionAvoidanceReward = RewTerm(func=reward_shaping_collision_avoidance, weight=0.4,\n                                params={\"normalise\": True, \"normaliser_name\": \"shaping_collision_avoidance_reward\"})\n\n    Shaping_StabilityReward = RewTerm(func=reward_shaping_stability, weight=0.3,\n                                params={\"normalise\": True, \"normaliser_name\": \"shaping_stability_reward\"})\n",successTerminationCode:"\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\ndef KickSmallSpherePastBlock_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the KickSmallSpherePastBlock skill has been successfully completed.\n    Success is defined as the small sphere being at least 5m past the block in the x direction, relative to the block's x position.\n    '''\n    # 1. Get robot - although not directly used in this success criteria, it is good practice to include for potential future expansion and template compliance.\n    robot = env.scene[\"robot\"]\n\n    try:\n        # 2. Get object positions - Accessing small sphere (Object2) and block (Object5) positions using approved pattern and try/except block.\n        small_sphere = env.scene['Object2'] # Accessing small sphere using approved pattern\n        block = env.scene['Object5'] # Accessing block using approved pattern\n\n        # 3. Access object positions - Getting x positions of the small sphere and the block using approved pattern.\n        small_sphere_pos_x = small_sphere.data.root_pos_w[:, 0] # Accessing small sphere x position using approved pattern\n        small_sphere_pos_y = small_sphere.data.root_pos_w[:, 1] # Accessing small sphere y position using approved pattern\n        block_pos_x = block.data.root_pos_w[:, 0] # Accessing block x position using approved pattern\n        block_pos_y = block.data.root_pos_w[:, 1] # Accessing block y position using approved pattern\n\n        # 4. Calculate relative distance - Calculating the x distance between the small sphere and the block.\n        relative_distance_x = small_sphere_pos_x - block_pos_x\n        relative_distance_y = small_sphere_pos_y - block_pos_y\n        relative_distance = torch.sqrt(relative_distance_x**2 + relative_distance_y**2)\n\n        # 5. Define success condition - Checking if the small sphere is at least 5m past the block in the x direction. Using relative distance and no hardcoded thresholds.\n        success_threshold = 3.0 # 5m past the block\n        condition = relative_distance > success_threshold # Success condition: small sphere is 5m past the block in x direction\n\n    except KeyError:\n        # 6. Handle missing objects - If 'Object2' or 'Object5' is not found, consider success as false for all environments.\n        condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 7. Check success duration and save success states - Using check_success_duration to ensure success is maintained for a duration and save_success_state to record success.\n    success = check_success_duration(env, condition, \"KickSmallSpherePastBlock\", duration=0.5) # Check if success condition is maintained for 0.5 seconds\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"KickSmallSpherePastBlock\") # Save success state for successful environments\n\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=KickSmallSpherePastBlock_success)\n",policyVideo:"/videos/L2_KickSmallSpherePastBlock.mp4",children:[{name:"WalkToSmallSphere",level:1,policyVideo:"/videos/WalkToSmallSphere.mp4"},{name:"ExecuteKickSmallSphereForward",level:1,policyVideo:"/videos/ExecuteKickSmallSphereForward.mp4"}]},{name:"JumpOntoBlock",level:2,rewardCode:"\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer\nfrom ...objects import get_object_volume\n\nimport torch\n\ndef main_ExecuteJumpOntoBlock_reward(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"main_reward\") -> torch.Tensor:\n    '''Main reward for ExecuteJumpOntoBlock.\n\n    Reward for the robot standing on top of the block with feet above the block's top surface.\n    This encourages the robot to successfully jump and land on the block.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    try:\n        block = env.scene['Object5'] # Accessing block object using approved pattern and try/except - rule 2 & 5 in ABSOLUTE REQUIREMENTS\n\n        # Accessing robot foot positions using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n        left_foot_idx = robot.body_names.index('left_ankle_roll_link')\n        right_foot_idx = robot.body_names.index('right_ankle_roll_link')\n        left_foot_pos = robot.data.body_pos_w[:, left_foot_idx]\n        right_foot_pos = robot.data.body_pos_w[:, right_foot_idx]\n\n        # Calculate minimum foot position\n        min_foot_pos = torch.min(left_foot_pos, right_foot_pos)\n\n        # Accessing block position using approved pattern - rule 2 in ABSOLUTE REQUIREMENTS\n        block_pos = block.data.root_pos_w\n\n        # Calculate relative distance in z direction between minimum foot position and block top surface - rule 1 in ABSOLUTE REQUIREMENTS\n        block_size_z = 0.5 # Reading block size from object config (size_cubes = [[0.4, 10.0, 0.4], [1.0, 10.0, 0.2], [0.5, 0.5, 0.5]]) - rule 6 & 7 in CRITICAL IMPLEMENTATION RULES\n        block_top_surface_z = block_pos[:, 2] + block_size_z \n        distance_z_feet_block_top = min_foot_pos[:, 2] - block_top_surface_z  # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n\n\n        # Reward is negative absolute distance to encourage feet to be on top of the block - rule 5 in CRITICAL IMPLEMENTATION RULES\n        reward = -torch.abs(distance_z_feet_block_top) # Continuous reward, using relative distance - rule 5 in CRITICAL IMPLEMENTATION RULES\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward - rule 5 in ABSOLUTE REQUIREMENTS\n\n    # Reward normalization using RewNormalizer - rule 6 in ABSOLUTE REQUIREMENTS and rule 4 in REWARD STRUCTURE RULES\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_approach_block_x(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"approach_block_x_reward\") -> torch.Tensor:\n    '''Shaping reward for approaching the block in the x-direction.\n\n    Reward for decreasing the x-distance between the pelvis and the block.\n    Encourages the robot to move towards the block before jumping.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    try:\n        block = env.scene['Object5'] # Accessing block object using approved pattern and try/except - rule 2 & 5 in ABSOLUTE REQUIREMENTS\n\n        # Accessing robot pelvis position using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n\n        # Accessing block position using approved pattern - rule 2 in ABSOLUTE REQUIREMENTS\n        block_pos = block.data.root_pos_w\n\n        # Calculate relative distance in x direction between pelvis and block - rule 1 in ABSOLUTE REQUIREMENTS\n        distance_x_pelvis_block = block_pos[:, 0] - pelvis_pos[:, 0] # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n        distance_y_pelvis_block = block_pos[:, 1] - pelvis_pos[:, 1] # Relative distance in y-direction\n        distance = torch.sqrt(distance_x_pelvis_block**2 + distance_y_pelvis_block**2)\n\n\n        # Reward is negative absolute distance to encourage moving closer in x direction - rule 5 in CRITICAL IMPLEMENTATION RULES\n        reward = -torch.abs(distance) # Continuous reward, using relative distance - rule 5 in CRITICAL IMPLEMENTATION RULES\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward - rule 5 in ABSOLUTE REQUIREMENTS\n\n    # Reward normalization using RewNormalizer - rule 6 in ABSOLUTE REQUIREMENTS and rule 4 in REWARD STRUCTURE RULES\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_jump_height(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"jump_height_reward\") -> torch.Tensor:\n    '''Shaping reward for achieving a suitable jump height when approaching the block.\n\n    Reward for reaching a target pelvis height above the block's top surface when close to the block in x-direction.\n    Encourages the robot to jump upwards when approaching the block.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    try:\n        block = env.scene['Object5'] # Accessing block object using approved pattern and try/except - rule 2 & 5 in ABSOLUTE REQUIREMENTS\n\n        # Accessing robot pelvis position using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n\n        # Accessing block position using approved pattern - rule 2 in ABSOLUTE REQUIREMENTS\n        block_pos = block.data.root_pos_w\n\n        # Calculate target pelvis height above block top surface - rule 1 in ABSOLUTE REQUIREMENTS\n        block_size_z = 0.5 # Reading block size from object config (size_cubes = [[0.4, 10.0, 0.4], [1.0, 10.0, 0.2], [0.5, 0.5, 0.5]]) - rule 6 & 7 in CRITICAL IMPLEMENTATION RULES\n        block_top_surface_z = block_pos[:, 2] + block_size_z\n        target_pelvis_height = block_top_surface_z + 0.5 # Relative target height - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Calculate relative distance in z direction between pelvis and target height - rule 1 in ABSOLUTE REQUIREMENTS\n        distance_z_pelvis_target_height = pelvis_pos[:, 2] - target_pelvis_height # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Approach block condition: activate when robot is approaching the block in x direction - rule 4 in ABSOLUTE REQUIREMENTS\n        approach_block_condition = (pelvis_pos[:, 0] > block_pos[:, 0] - 2.0) & (pelvis_pos[:, 0] < block_pos[:, 0]) # Relative condition - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Reward is negative absolute distance to encourage reaching target height - rule 5 in CRITICAL IMPLEMENTATION RULES\n        reward = -torch.abs(distance_z_pelvis_target_height) # Continuous reward, using relative distance - rule 5 in CRITICAL IMPLEMENTATION RULES\n\n        reward = torch.where(approach_block_condition, reward, -torch.ones_like(reward)) # Apply activation condition - rule 4 in ABSOLUTE REQUIREMENTS\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward - rule 5 in ABSOLUTE REQUIREMENTS\n\n    # Reward normalization using RewNormalizer - rule 6 in ABSOLUTE REQUIREMENTS and rule 4 in REWARD STRUCTURE RULES\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_stability_on_block(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"stability_on_block_reward\") -> torch.Tensor:\n    '''Shaping reward for maintaining stability on the block after landing.\n\n    Reward for keeping the pelvis at a reasonable height relative to the block and feet, and avoid falling off.\n    Encourages the robot to maintain balance on the block.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    try:\n        block = env.scene['Object5'] # Accessing block object using approved pattern and try/except - rule 2 & 5 in ABSOLUTE REQUIREMENTS\n\n        # Accessing robot pelvis and feet positions using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n        left_foot_idx = robot.body_names.index('left_ankle_roll_link')\n        right_foot_idx = robot.body_names.index('right_ankle_roll_link')\n        left_foot_pos = robot.data.body_pos_w[:, left_foot_idx]\n        right_foot_pos = robot.data.body_pos_w[:, right_foot_idx]\n\n        # Calculate average foot position\n        avg_foot_pos = (left_foot_pos + right_foot_pos) / 2\n\n        # Accessing block position using approved pattern - rule 2 in ABSOLUTE REQUIREMENTS\n        block_pos = block.data.root_pos_w\n\n        # Calculate relative pelvis height above average feet position - rule 1 in ABSOLUTE REQUIREMENTS\n        relative_pelvis_height = pelvis_pos[:, 2] - avg_foot_pos[:, 2] # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Target relative pelvis height (slightly above feet) - rule 4 in ABSOLUTE REQUIREMENTS\n        target_relative_pelvis_height = 0.5 # Relative target height - rule 4 in ABSOLUTE REQUIREMENTS\n\n        # Calculate distance from target relative pelvis height - rule 1 in ABSOLUTE REQUIREMENTS\n        distance_z_pelvis_relative_target = relative_pelvis_height - target_relative_pelvis_height # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n\n        pelvis_on_block_condition_x = (pelvis_pos[:, 0] > block_pos[:, 0] - 0.25) & (pelvis_pos[:, 0] < block_pos[:, 0] + 0.25)\n        pelvis_on_block_condition_y = (pelvis_pos[:, 1] > block_pos[:, 1] - 0.25) & (pelvis_pos[:, 1] < block_pos[:, 1] + 0.25)\n\n\n        # On block condition: activate when feet are approximately on top of the block - rule 4 in ABSOLUTE REQUIREMENTS\n        block_size_z = 0.5 # Reading block size from object config (size_cubes = [[0.4, 10.0, 0.4], [1.0, 1.0, 0.2], [0.5, 0.5, 0.5]]) - rule 6 & 7 in CRITICAL IMPLEMENTATION RULES\n        on_block_condition = (avg_foot_pos[:, 2] > (block_pos[:, 2] + block_size_z - 0.1)) & pelvis_on_block_condition_x & pelvis_on_block_condition_y # Relative condition - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Reward is negative absolute distance to encourage maintaining target relative pelvis height - rule 5 in CRITICAL IMPLEMENTATION RULES\n        reward = -torch.abs(distance_z_pelvis_relative_target) # Continuous reward, using relative distance - rule 5 in CRITICAL IMPLEMENTATION RULES\n\n        reward = torch.where(on_block_condition, reward, -2.0 * torch.ones_like(reward)) # Apply activation condition - rule 4 in ABSOLUTE REQUIREMENTS\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward - rule 5 in ABSOLUTE REQUIREMENTS\n\n    # Reward normalization using RewNormalizer - rule 6 in ABSOLUTE REQUIREMENTS and rule 4 in REWARD STRUCTURE RULES\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_collision_avoidance(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"collision_avoidance_reward\") -> torch.Tensor:\n    '''Shaping reward for collision avoidance with the block at ground level.\n\n    Negative reward if the pelvis is too close to the block horizontally when the pelvis is low to the ground.\n    Discourages the robot from colliding with the block before jumping.\n    '''\n    robot = env.scene[\"robot\"] # Accessing robot using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n    try:\n        block = env.scene['Object5'] # Accessing block object using approved pattern and try/except - rule 2 & 5 in ABSOLUTE REQUIREMENTS\n\n        # Accessing robot pelvis position using approved pattern - rule 3 in ABSOLUTE REQUIREMENTS\n        pelvis_idx = robot.body_names.index('pelvis')\n        pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n\n        # Accessing block position using approved pattern - rule 2 in ABSOLUTE REQUIREMENTS\n        block_pos = block.data.root_pos_w\n\n        # Calculate relative distance in x direction between pelvis and block - rule 1 in ABSOLUTE REQUIREMENTS\n        distance_x_pelvis_block = block_pos[:, 0] - pelvis_pos[:, 0] # Relative distance - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Collision condition: activate when robot is very close to block horizontally and low to the ground - rule 4 in ABSOLUTE REQUIREMENTS\n        collision_condition = (pelvis_pos[:, 0] > block_pos[:, 0] - 0.5) & (pelvis_pos[:, 0] < block_pos[:, 0] + 0.5) & (pelvis_pos[:, 2] < 0.2) # Relative condition in x, absolute condition in z (allowed for height) - rule 1 in ABSOLUTE REQUIREMENTS\n\n        # Small negative reward for collision proximity - rule 5 in CRITICAL IMPLEMENTATION RULES\n        reward = -1.0 * torch.ones(env.num_envs, device=env.device) # Continuous negative reward - rule 5 in CRITICAL IMPLEMENTATION RULES\n\n        reward = torch.where(collision_condition, reward, torch.zeros_like(reward)) # Apply activation condition - rule 4 in ABSOLUTE REQUIREMENTS\n\n    except KeyError:\n        reward = torch.zeros(env.num_envs, device=env.device) # Handle missing object, return zero reward - rule 5 in ABSOLUTE REQUIREMENTS\n\n    # Reward normalization using RewNormalizer - rule 6 in ABSOLUTE REQUIREMENTS and rule 4 in REWARD STRUCTURE RULES\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n\n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\ndef shaping_reward_feet_under_pelvis(env: ManagerBasedRLEnv, normalise: bool = True, normaliser_name: str = \"feet_under_pelvis_reward\") -> torch.Tensor:\n    '''Shaping reward for keeping feet underneath the pelvis in the horizontal plane.\n    \n    Reward for minimizing the horizontal (x,y) distance between the feet and pelvis.\n    Encourages the robot to maintain a stable posture.\n    '''\n    robot = env.scene[\"robot\"]\n    \n    # Accessing robot pelvis and feet positions using approved pattern\n    pelvis_idx = robot.body_names.index('pelvis')\n    pelvis_pos = robot.data.body_pos_w[:, pelvis_idx]\n    left_foot_idx = robot.body_names.index('left_ankle_roll_link')\n    right_foot_idx = robot.body_names.index('right_ankle_roll_link')\n    left_foot_pos = robot.data.body_pos_w[:, left_foot_idx]\n    right_foot_pos = robot.data.body_pos_w[:, right_foot_idx]\n    \n    left_x_distance = pelvis_pos[:, 0] - left_foot_pos[:, 0]\n    left_y_distance = pelvis_pos[:, 1] - left_foot_pos[:, 1]\n    \n    right_x_distance = pelvis_pos[:, 0] - right_foot_pos[:, 0]\n    right_y_distance = pelvis_pos[:, 1] - right_foot_pos[:, 1]\n    \n    \n    \n\n    # Calculate horizontal distance between pelvis and average foot position\n    horizontal_distance = torch.sqrt((left_x_distance)**2 + \n                                     (left_y_distance)**2) + torch.sqrt((right_x_distance)**2 + \n                                     (right_y_distance)**2)\n    \n    # Reward is negative distance to encourage feet to stay under pelvis\n    reward = -horizontal_distance\n    \n    # Reward normalization using RewNormalizer\n    if normaliser_name not in RewNormalizer.stats:\n        RewNormalizer.stats[normaliser_name] = RewardStats()\n    \n    if normalise:\n        scaled_reward = RewNormalizer.normalize(normaliser_name, reward)\n        RewNormalizer.update_stats(normaliser_name, reward)\n        return scaled_reward\n    return reward\n\n@configclass\nclass TaskRewardsCfg:\n    Main_ExecuteJumpOntoBlockReward = RewTerm(func=main_ExecuteJumpOntoBlock_reward, weight=1.0,\n                                            params={\"normalise\": True, \"normaliser_name\": \"main_reward\"})\n    ShapingRewardApproachBlockX = RewTerm(func=shaping_reward_approach_block_x, weight=1.0,\n                                            params={\"normalise\": True, \"normaliser_name\": \"approach_block_x_reward\"})\n    ShapingRewardJumpHeight = RewTerm(func=shaping_reward_jump_height, weight=0.5,\n                                            params={\"normalise\": True, \"normaliser_name\": \"jump_height_reward\"})\n    ShapingRewardStabilityOnBlock = RewTerm(func=shaping_reward_stability_on_block, weight=1.0,\n                                            params={\"normalise\": True, \"normaliser_name\": \"stability_on_block_reward\"})\n    ShapingRewardCollisionAvoidance = RewTerm(func=shaping_reward_collision_avoidance, weight=0, \n                                            params={\"normalise\": True, \"normaliser_name\": \"collision_avoidance_reward\"})\n    ShapingRewardFeetUnderPelvis = RewTerm(func=shaping_reward_feet_under_pelvis, weight=0.5,\n                                            params={\"normalise\": True, \"normaliser_name\": \"feet_under_pelvis_reward\"})\n",successTerminationCode:"\n\n\nfrom .base_success import save_success_state, check_success_duration\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\n# Assuming mdp is correctly importable from the context where this runs\n# If not, adjust the relative import path\nfrom ...mdp import * \nimport torch\nfrom pathlib import Path\n# Import reward functions if needed by success criteria\n# from .TaskRewardsCfg import * \n\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.utils import configclass\nfrom isaaclab.managers import SceneEntityCfg\nfrom ...mdp import *\nfrom ... import mdp\nfrom ...reward_normalizer import get_normalizer\nfrom ...objects import get_object_volume\n\ndef JumpOntoBlock_success(env: ManagerBasedRLEnv) -> torch.Tensor:\n    '''Determine if the JumpOntoBlock skill has been successfully completed.\n    Success is defined as the robot standing on top of the block with both feet.\n    '''\n    # 1. Access the robot object from the environment scene.\n    robot = env.scene[\"robot\"]\n\n    # 2. Get indices for the left and right feet (ankle_roll_link) using robot.body_names.index.\n    left_foot_idx = robot.body_names.index('left_ankle_roll_link')\n    right_foot_idx = robot.body_names.index('right_ankle_roll_link')\n\n    # 3. Get the world positions of the left and right feet using robot.data.body_pos_w and the indices.\n    left_foot_pos = robot.data.body_pos_w[:, left_foot_idx] # Shape: [num_envs, 3]\n    right_foot_pos = robot.data.body_pos_w[:, right_foot_idx] # Shape: [num_envs, 3]\n\n    # 4. Calculate the average position of the two feet.\n    avg_feet_pos_x = (left_foot_pos[:, 0] + right_foot_pos[:, 0]) / 2.0\n    avg_feet_pos_y = (left_foot_pos[:, 1] + right_foot_pos[:, 1]) / 2.0\n    avg_feet_pos_z = (left_foot_pos[:, 2] + right_foot_pos[:, 2]) / 2.0\n    avg_feet_pos = torch.stack([avg_feet_pos_x, avg_feet_pos_y, avg_feet_pos_z], dim=-1) # Shape: [num_envs, 3]\n\n\n    try:\n        # 5. Safely access the block object (Object5) from the environment scene using try-except block.\n        block = env.scene['Object5']\n        # 6. Get the world position of the block using block.data.root_pos_w.\n        block_pos = block.data.root_pos_w # Shape: [num_envs, 3]\n\n        # 7. Calculate the relative distance vector between the average feet position and the block position.\n        distance_x = block_pos[:, 0] - avg_feet_pos[:, 0]\n        distance_y = block_pos[:, 1] - avg_feet_pos[:, 1]\n        # 8. Calculate the vertical distance between the average feet position and the top of the block.\n        #    Block size is [0.5, 0.5, 0.5], so half height is 0.25.\n        distance_z = (block_pos[:, 2] + 0.25) - avg_feet_pos[:, 2] # distance to top of block\n\n        # 9. Define success condition:\n        #    - Vertical distance (distance_z) is less than 0.15 (feet are above the block top within 15cm).\n        #    - Horizontal distances (distance_x and distance_y) are within 0.25 in both x and y directions.\n        success_condition = (distance_z < 0.15) & (torch.abs(distance_x) < 0.25) & (torch.abs(distance_y) < 0.25)\n\n    except KeyError:\n        # 10. Handle KeyError if 'Object5' (block) is not found in the scene. Set success to False for all environments.\n        success_condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)\n\n    # 11. Check for success duration of 0.5 seconds using check_success_duration.\n    success = check_success_duration(env, success_condition, \"JumpOntoBlock\", duration=0.5)\n\n    # 12. Save success states for environments that meet the success condition using save_success_state.\n    if success.any():\n        for env_id in torch.where(success)[0]:\n            save_success_state(env, env_id, \"JumpOntoBlock\")\n\n    # 13. Return the success tensor.\n    return success\n\nclass SuccessTerminationCfg:\n    success = DoneTerm(func=JumpOntoBlock_success)\n",policyVideo:"/videos/JumpOntoBlock.mp4",children:[{name:"WalkToBlock",level:1,policyVideo:"/videos/WalkToBlock.mp4"},{name:"PrepareForJumpOntoBlock",level:1,policyVideo:"/videos/PrepareForJumpOntoBlock.mp4"},{name:"ExecuteJumpOntoBlock",level:1,policyVideo:"/videos/ExecuteJumpOntoBlock.mp4"},{name:"StabilizeOnBlockTop",level:1,policyVideo:"/videos/StabalizerOnBlockTop.mp4"}]}]};var t=r(3241),i=r.n(t);function l(){return(0,a.useEffect)(()=>{i().highlightAll()},[]),null}r(4495);let c=e=>{var n,r;let{skill:s,skillPath:t,openContentPath:i,expandedChildrenPaths:_,onToggleContent:p,onToggleChildren:d}=e,h=s.children&&s.children.length>0,m=i===t,w=null!=(r=null==_?void 0:_.has(t))&&r,[v,g]=(0,a.useState)("video"),[u,b]=(0,a.useState)(!1);(0,a.useEffect)(()=>{if(m){b(!0);let e=setTimeout(()=>{b(!1)},3e3);return()=>clearTimeout(e)}},[m]);let f={3:"bg-blue-700 hover:bg-blue-800 text-white",2:"bg-blue-500 hover:bg-blue-600 text-white",1:"bg-blue-300 hover:bg-blue-400 text-gray-800"}[s.level]||"bg-blue-200 hover:bg-blue-300 text-gray-700",R=()=>{p(t)},x=e=>{let n="px-3 py-1 text-base rounded-t-md cursor-pointer transition-colors";return v===e?n+=" bg-gray-200 font-semibold":n+=" bg-gray-100 hover:bg-gray-200",u&&(n+=" sunset-flash-tab"),n};return(0,o.jsxs)("div",{className:"ml-4 my-2",children:[(0,o.jsxs)("div",{className:"p-2 rounded ".concat(f," border border-gray-400 flex items-center cursor-pointer"),onClick:R,onKeyDown:e=>{("Enter"===e.key||" "===e.key)&&(e.preventDefault(),R())},role:"button",tabIndex:0,children:[h&&(0,o.jsx)("span",{className:"mr-2 p-1 rounded",children:w?"":""}),(0,o.jsxs)("span",{className:"flex-grow",children:[s.name," (Level ",s.level,")"]}),!m&&(0,o.jsx)("span",{className:"ml-auto text-xs italic opacity-75 px-2",children:"Click for details"})]}),m&&(0,o.jsxs)("div",{className:"mt-1 border border-gray-300 rounded bg-gray-50 shadow-sm",children:[(0,o.jsxs)("div",{className:"flex border-b border-gray-300 bg-gray-100 rounded-t-md",children:[(0,o.jsx)("button",{className:x("video"),onClick:()=>g("video"),style:u?{animationDelay:"0s"}:{},children:"Video Demo"}),(0,o.jsx)("button",{className:x("reward"),onClick:()=>g("reward"),style:u?{animationDelay:"0.4s"}:{},children:"Reward Code"}),(0,o.jsx)("button",{className:x("success"),onClick:()=>g("success"),style:u?{animationDelay:"0.8s"}:{},children:"Success Code"})]}),(0,o.jsxs)("div",{className:"p-2",children:["video"===v&&(0,o.jsx)("div",{children:s.policyVideo?(0,o.jsxs)("video",{controls:!0,width:"100%",className:"w-4/5 mx-auto rounded",children:[(0,o.jsx)("source",{src:"".concat("/website2").concat(s.policyVideo),type:"video/mp4"}),"Your browser does not support the video tag."]},t):(0,o.jsx)("p",{className:"text-xs italic text-center p-4",children:"Placeholder for policy video."})}),"reward"===v&&(0,o.jsx)("div",{className:"text-xs",children:(0,o.jsx)("pre",{className:"bg-gray-100 p-2 rounded text-xs overflow-auto h-60 border border-gray-200",children:(0,o.jsxs)("code",{className:"language-python",children:[(0,o.jsx)(l,{}),s.rewardCode||"// Placeholder for reward code"]})})}),"success"===v&&(0,o.jsx)("div",{className:"text-xs",children:(0,o.jsx)("pre",{className:"bg-gray-100 p-2 rounded text-xs overflow-auto h-60 border border-gray-200",children:(0,o.jsxs)("code",{className:"language-python",children:[(0,o.jsx)(l,{}),s.successTerminationCode||"// Placeholder for success termination code"]})})})]})]}),h&&w&&(0,o.jsx)("div",{className:"ml-4 border-l-2 border-gray-300 pl-2",children:null==(n=s.children)?void 0:n.map(e=>{let n="".concat(t,"/").concat(e.name);return(0,o.jsx)(c,{skill:e,skillPath:n,openContentPath:i,expandedChildrenPaths:_,onToggleContent:p,onToggleChildren:d},n)})})]})},_=(e,n,r)=>{e.children&&e.children.length>0&&(r.add(n),e.children.forEach(e=>{_(e,"".concat(n,"/").concat(e.name),r)}))},p=()=>{let[e,n]=(0,a.useState)(s.name),r=new Set;_(s,s.name,r);let[t,i]=(0,a.useState)(r);return(0,o.jsxs)("div",{className:"p-4",children:[(0,o.jsx)("h1",{className:"text-2xl font-bold mb-4",children:"Interactive Skill Hierarchy"}),(0,o.jsx)(c,{skill:s,skillPath:s.name,openContentPath:e,expandedChildrenPaths:t,onToggleContent:e=>{n(n=>n===e?null:e)},onToggleChildren:e=>{i(n=>{let r=new Set(n);return r.has(e)?r.delete(e):r.add(e),r})}})]})}},5384:(e,n,r)=>{Promise.resolve().then(r.t.bind(r,3063,23)),Promise.resolve().then(r.bind(r,3169))}},e=>{var n=n=>e(e.s=n);e.O(0,[573,441,684,358],()=>n(5384)),_N_E=e.O()}]);
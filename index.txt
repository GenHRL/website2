1:"$Sreact.fragment"
2:I[7555,[],""]
3:I[1295,[],""]
5:I[3063,["63","static/chunks/63-82162ec3eea6ac13.js","974","static/chunks/app/page-7194661d4e84b82d.js"],"Image"]
6:I[8077,["63","static/chunks/63-82162ec3eea6ac13.js","974","static/chunks/app/page-7194661d4e84b82d.js"],"default"]
7:I[9665,[],"MetadataBoundary"]
9:I[9665,[],"OutletBoundary"]
c:I[4911,[],"AsyncMetadataOutlet"]
e:I[9665,[],"ViewportBoundary"]
10:I[6614,[],""]
:HL["/website2/_next/static/media/569ce4b8f30dc480-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/website2/_next/static/media/93f479601ee12b01-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/website2/_next/static/css/b2bb8f1fc8a94a51.css","style"]
4:T41e,Defining effective multi-level skill hierarchies and their corresponding learning objectives is a core challenge in robotics and reinforcement learning. Large Language Models (LLMs) offer powerful new capabilities for tackling this challenge through automated generation and reasoning. This paper introduces GenHRL, an LLM-driven framework that automates the pipeline from high-level natural language task descriptions to learned hierarchical skills. GenHRL autonomously generates: (1) task-specific simulation environments, (2) multi-level skill decompositions, and (3) executable code defining intrinsic reward and termination functions for each skill. This automation avoids the need for manual reward engineering, predefined skill sets, offline datasets, and enables end-to-end hierarchical policy learning via standard reinforcement learning algorithms. Empirical evaluations on complex robotic humanoid simulation tasks demonstrate that GenHRL significantly enhances learning efficiency and final performance compared to non-hierarchical baselines.0:{"P":null,"b":"Txu19Z62vDzjDv3B96zYv","p":"/website2","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/website2/_next/static/css/b2bb8f1fc8a94a51.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"__variable_926cff __variable_470fb6 antialiased","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","main",null,{"className":"container mx-auto p-4","children":[["$","header",null,{"className":"my-6 text-center","children":[["$","h1",null,{"className":"text-4xl font-bold","children":"GenHRL: Generative Hierarchical Reinforcement Learning"}],["$","p",null,{"className":"text-lg text-gray-700 mt-2","children":"Authors Anonymous"}],["$","div",null,{"className":"mt-6 max-w-3xl mx-auto text-left","children":[["$","h2",null,{"className":"text-2xl font-semibold mb-2 text-center","children":"Abstract"}],["$","p",null,{"className":"text-gray-700 leading-relaxed","children":"$4"}],["$","div",null,{"className":"mt-4 text-center","children":["$","a",null,{"href":"https://openreview.net/forum?id=vPwAh0eL0D&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Drobot-learning.org%2FCoRL%2F2025%2FConference%2FAuthors%23your-submissions)","target":"https://openreview.net/forum?id=vPwAh0eL0D&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Drobot-learning.org%2FCoRL%2F2025%2FConference%2FAuthors%23your-submissions)","rel":"noopener noreferrer","className":"inline-block bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded transition-colors","children":"View on OpenReview"}]}]]}],["$","div",null,{"className":"my-8 max-w-4xl mx-auto","children":[["$","h2",null,{"className":"text-2xl font-semibold mb-4 text-center","children":"Method Overview"}],["$","div",null,{"className":"w-full","children":["$","$L5",null,{"src":"/website2/method_drawing.svg","alt":"Method Diagram","width":800,"height":600,"className":"h-auto w-full max-w-2xl mx-auto block"}]}],["$","div",null,{"className":"mt-4 text-left","children":[["$","h3",null,{"className":"text-xl font-semibold mb-2","children":"Method Description:"}],["$","p",null,{"className":"text-gray-700 leading-relaxed","children":["Our ",["$","strong",null,{"children":"GenHRL system"}]," uses ",["$","strong",null,{"children":"Large Language Models (LLMs)"}]," to ",["$","strong",null,{"children":"automatically build complex robot skills"}]," from ",["$","strong",null,{"children":"simple language instructions"}],". As shown above, the process starts (",["$","strong",null,{"children":"Stage 1"}],") when a user provides a ",["$","strong",null,{"children":"task description"}],". The LLM interprets this, designs a suitable ",["$","strong",null,{"children":"simulation environment"}],", and breaks the task down into a ",["$","strong",null,{"children":"multi-level hierarchy of described skills"}],". Next (",["$","strong",null,{"children":"Stage 2"}],"), the LLM translates these skill descriptions into ",["$","strong",null,{"children":"executable code"}]," that defines the ",["$","strong",null,{"children":"goals (rewards)"}]," and ",["$","strong",null,{"children":"completion rules (terminations)"}]," for learning each skill, which can be optionally checked by a human. Finally (",["$","strong",null,{"children":"Stage 3"}],"), GenHRL uses this generated environment and code to ",["$","strong",null,{"children":"automatically train policies"}]," for the entire hierarchy using ",["$","strong",null,{"children":"reinforcement learning"}],", resulting in an agent capable of performing the complex task."]}]]}]]}],["$","div",null,{"className":"text-center","children":["$","p",null,{"className":"text-xl text-gray-600 mt-8","children":"Interactive Skill Hierarchy"}]}]]}],["$","$L6",null,{}],["$","footer",null,{"className":"mt-8 text-center text-gray-500","children":["$","p",null,{"children":"Level 0 skills are Primitive Actions and are implicitly part of Level 1 skills."}]}]]}],["$","$L7",null,{"children":"$L8"}],null,["$","$L9",null,{"children":["$La","$Lb",["$","$Lc",null,{"promise":"$@d"}]]}]]}],{},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","pBDtWgnRRQDOxxHdUN2Un",{"children":[["$","$Le",null,{"children":"$Lf"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],null]}],false]],"m":"$undefined","G":["$10","$undefined"],"s":false,"S":true}
11:"$Sreact.suspense"
12:I[4911,[],"AsyncMetadata"]
8:["$","$11",null,{"fallback":null,"children":["$","$L12",null,{"promise":"$@13"}]}]
b:null
f:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
a:null
13:{"metadata":[["$","title","0",{"children":"GenHRL"}],["$","meta","1",{"name":"description","content":"Generative Hierarchical Reinforcement Learning"}],["$","link","2",{"rel":"icon","href":"/website2/favicon.ico","type":"image/x-icon","sizes":"16x16"}]],"error":null,"digest":"$undefined"}
d:{"metadata":"$13:metadata","error":null,"digest":"$undefined"}

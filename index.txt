1:"$Sreact.fragment"
2:I[7555,[],""]
3:I[1295,[],""]
5:I[3063,["573","static/chunks/573-18a4668a89e1ab7a.js","974","static/chunks/app/page-2c98154428699cdd.js"],"Image"]
6:I[3169,["573","static/chunks/573-18a4668a89e1ab7a.js","974","static/chunks/app/page-2c98154428699cdd.js"],"default"]
7:I[9665,[],"MetadataBoundary"]
9:I[9665,[],"OutletBoundary"]
c:I[4911,[],"AsyncMetadataOutlet"]
e:I[9665,[],"ViewportBoundary"]
10:I[6614,[],""]
:HL["/website2/_next/static/media/569ce4b8f30dc480-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/website2/_next/static/media/93f479601ee12b01-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/website2/_next/static/css/126e234a0c6c6bde.css","style"]
4:T41e,Defining effective multi-level skill hierarchies and their corresponding learning objectives is a core challenge in robotics and reinforcement learning. Large Language Models (LLMs) offer powerful new capabilities for tackling this challenge through automated generation and reasoning. This paper introduces GenHRL, an LLM-driven framework that automates the pipeline from high-level natural language task descriptions to learned hierarchical skills. GenHRL autonomously generates: (1) task-specific simulation environments, (2) multi-level skill decompositions, and (3) executable code defining intrinsic reward and termination functions for each skill. This automation avoids the need for manual reward engineering, predefined skill sets, offline datasets, and enables end-to-end hierarchical policy learning via standard reinforcement learning algorithms. Empirical evaluations on complex robotic humanoid simulation tasks demonstrate that GenHRL significantly enhances learning efficiency and final performance compared to non-hierarchical baselines.0:{"P":null,"b":"sRtlxmN06wMQ3i0x6EJIr","p":"/website2","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/website2/_next/static/css/126e234a0c6c6bde.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"__variable_926cff __variable_470fb6 antialiased","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","main",null,{"className":"container mx-auto p-4","children":[["$","header",null,{"className":"my-6 text-center","children":[["$","h1",null,{"className":"text-4xl font-bold","children":"GenHRL: Generative Hierarchical Reinforcement Learning"}],["$","p",null,{"className":"text-lg text-gray-700 mt-2","children":"Authors Anonymous"}],["$","div",null,{"className":"mt-6 max-w-3xl mx-auto text-left","children":[["$","h2",null,{"className":"text-2xl font-semibold mb-2 text-center","children":"Abstract"}],["$","p",null,{"className":"text-gray-700 leading-relaxed","children":"$4"}],["$","div",null,{"className":"mt-4 text-center","children":["$","a",null,{"href":"https://openreview.net/forum?id=vPwAh0eL0D&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Drobot-learning.org%2FCoRL%2F2025%2FConference%2FAuthors%23your-submissions)","target":"https://openreview.net/forum?id=vPwAh0eL0D&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Drobot-learning.org%2FCoRL%2F2025%2FConference%2FAuthors%23your-submissions)","rel":"noopener noreferrer","className":"inline-block bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded transition-colors","children":"View on OpenReview"}]}]]}],["$","div",null,{"className":"my-8 max-w-4xl mx-auto","children":[["$","h2",null,{"className":"text-2xl font-semibold mb-4 text-center","children":"Method Overview"}],["$","div",null,{"className":"w-full","children":["$","$L5",null,{"src":"/website2/method_drawing.svg","alt":"Method Diagram","width":800,"height":600,"className":"h-auto w-full max-w-2xl mx-auto block"}]}],["$","div",null,{"className":"mt-4 text-left","children":[["$","h3",null,{"className":"text-xl font-semibold mb-2","children":"Method Description:"}],["$","p",null,{"className":"text-gray-700 leading-relaxed","children":"Our GenHRL system employs Large Language Models (LLMs) to automatically construct complex robot skills from simple language instructions. The process, illustrated above, unfolds in three main stages:"}],["$","ul",null,{"className":"list-disc list-outside text-gray-700 leading-relaxed ml-5 mt-2","children":[["$","li",null,{"className":"mb-2","children":[["$","strong",null,{"children":"Stage 1: Task Interpretation and Decomposition."}]," The user provides a task description. The LLM then interprets this, designs a suitable simulation environment, and breaks the task down into a multi-level hierarchy of described skills."]}],["$","li",null,{"className":"mb-2","children":[["$","strong",null,{"children":"Stage 2: Code Generation."}]," The LLM translates these skill descriptions into executable code that defines the goals (rewards) and completion rules (terminations) for learning each skill. This generated code can be optionally checked by a human."]}],["$","li",null,{"className":"mb-2","children":[["$","strong",null,{"children":"Stage 3: Hierarchical Reinforcement Learning."}]," GenHRL utilizes this generated environment and code to automatically train policies for the entire skill hierarchy using reinforcement learning, ultimately resulting in an agent capable of performing the complex task."]}]]}]]}]]}],["$","div",null,{"className":"mt-8 max-w-4xl mx-auto text-left","children":[["$","h3",null,{"className":"text-xl font-semibold mb-2","children":"Demonstration Context:"}],["$","p",null,{"className":"text-gray-700 leading-relaxed mb-2","children":"Below, we showcase the GenHRL system in action. For this demonstration, GenHRL was provided with the following high-level task description:"}],["$","div",null,{"className":"bg-gray-100 border border-gray-300 p-3 rounded-md my-3 shadow-sm","children":["$","em",null,{"className":"text-gray-700 leading-relaxed","children":"\"The robot should jump over a low wall, push a large sphere into a high wall to knock it down and pass over it. The robot should then walk to a small sphere and kick it past a block. Finally the robot should walk to the block and jump onto it.\""}]}],["$","p",null,{"className":"text-gray-700 leading-relaxed mb-2","children":"From this instruction, the system first generates the corresponding simulation environment, including the layout of all necessary objects. It then automatically designs the multi-level skill hierarchy that you can explore interactively below."}],["$","p",null,{"className":"text-gray-700 leading-relaxed mb-2","children":"Furthermore, GenHRL generates the underlying executable code for each skill, including the reward functions that guide learning and the success termination conditions that define task completion. Each higher-level skill in the hierarchy is composed of and utilizes the skills immediately below it to achieve its objective."}],["$","p",null,{"className":"text-gray-700 leading-relaxed","children":["To explore the generated hierarchy, simply click on any skill box. This will reveal tabs for the ",["$","strong",null,{"children":"Video Demo"}],", which shows a successfully trained policy for that skill, as well as the ",["$","strong",null,{"children":"Reward Code"}]," and ",["$","strong",null,{"children":"Success Code"}]," generated by the LLM."]}]]}],["$","div",null,{"className":"text-center","children":["$","p",null,{"className":"text-xl text-gray-600 mt-8","children":"----------------------------------------------------------"}]}]]}],["$","$L6",null,{}],["$","footer",null,{"className":"mt-8 text-center text-gray-500","children":["$","p",null,{"children":"Level 0 skills are Primitive Actions and are implicitly part of Level 1 skills."}]}]]}],["$","$L7",null,{"children":"$L8"}],null,["$","$L9",null,{"children":["$La","$Lb",["$","$Lc",null,{"promise":"$@d"}]]}]]}],{},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","nh6IMhewv8ESJsv2wwKZv",{"children":[["$","$Le",null,{"children":"$Lf"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],null]}],false]],"m":"$undefined","G":["$10","$undefined"],"s":false,"S":true}
11:"$Sreact.suspense"
12:I[4911,[],"AsyncMetadata"]
8:["$","$11",null,{"fallback":null,"children":["$","$L12",null,{"promise":"$@13"}]}]
b:null
f:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
a:null
13:{"metadata":[["$","title","0",{"children":"GenHRL"}],["$","meta","1",{"name":"description","content":"Generative Hierarchical Reinforcement Learning"}],["$","link","2",{"rel":"icon","href":"/website2/favicon.ico","type":"image/x-icon","sizes":"16x16"}]],"error":null,"digest":"$undefined"}
d:{"metadata":"$13:metadata","error":null,"digest":"$undefined"}

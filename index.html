<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/website2/_next/static/media/569ce4b8f30dc480-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/website2/_next/static/media/93f479601ee12b01-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/website2/_next/static/css/b2bb8f1fc8a94a51.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/website2/_next/static/chunks/webpack-6e0995e0e540f1e7.js"/><script src="/website2/_next/static/chunks/4bd1b696-18452535c1c4862d.js" async=""></script><script src="/website2/_next/static/chunks/684-63d730bf244a0b80.js" async=""></script><script src="/website2/_next/static/chunks/main-app-be7e1ccb34745cfc.js" async=""></script><script src="/website2/_next/static/chunks/63-82162ec3eea6ac13.js" async=""></script><script src="/website2/_next/static/chunks/app/page-a8188a588882025e.js" async=""></script><meta name="next-size-adjust" content=""/><title>GenHRL</title><meta name="description" content="Generative Hierarchical Reinforcement Learning"/><link rel="icon" href="/website2/favicon.ico" type="image/x-icon" sizes="16x16"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/website2/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__variable_926cff __variable_470fb6 antialiased"><main class="container mx-auto p-4"><header class="my-6 text-center"><h1 class="text-4xl font-bold">GenHRL: Generative Hierarchical Reinforcement Learning</h1><p class="text-lg text-gray-700 mt-2">Authors Anonymous</p><div class="mt-6 max-w-3xl mx-auto text-left"><h2 class="text-2xl font-semibold mb-2 text-center">Abstract</h2><p class="text-gray-700 leading-relaxed">Defining effective multi-level skill hierarchies and their corresponding learning objectives is a core challenge in robotics and reinforcement learning. Large Language Models (LLMs) offer powerful new capabilities for tackling this challenge through automated generation and reasoning. This paper introduces GenHRL, an LLM-driven framework that automates the pipeline from high-level natural language task descriptions to learned hierarchical skills. GenHRL autonomously generates: (1) task-specific simulation environments, (2) multi-level skill decompositions, and (3) executable code defining intrinsic reward and termination functions for each skill. This automation avoids the need for manual reward engineering, predefined skill sets, offline datasets, and enables end-to-end hierarchical policy learning via standard reinforcement learning algorithms. Empirical evaluations on complex robotic humanoid simulation tasks demonstrate that GenHRL significantly enhances learning efficiency and final performance compared to non-hierarchical baselines.</p><div class="mt-4 text-center"><a href="https://openreview.net/forum?id=vPwAh0eL0D&amp;referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Drobot-learning.org%2FCoRL%2F2025%2FConference%2FAuthors%23your-submissions)" target="https://openreview.net/forum?id=vPwAh0eL0D&amp;referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Drobot-learning.org%2FCoRL%2F2025%2FConference%2FAuthors%23your-submissions)" rel="noopener noreferrer" class="inline-block bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded transition-colors">View on OpenReview</a></div></div><div class="my-8 max-w-4xl mx-auto"><h2 class="text-2xl font-semibold mb-4 text-center">Method Overview</h2><div class="w-full"><img alt="Method Diagram" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="h-auto w-full max-w-2xl mx-auto block" style="color:transparent" src="/website2/method_drawing.svg"/></div><div class="mt-4 text-left"><h3 class="text-xl font-semibold mb-2">Method Description:</h3><p class="text-gray-700 leading-relaxed">Our <strong>GenHRL system</strong> uses <strong>Large Language Models (LLMs)</strong> to <strong>automatically build complex robot skills</strong> from <strong>simple language instructions</strong>. As shown above, the process starts (<strong>Stage 1</strong>) when a user provides a <strong>task description</strong>. The LLM interprets this, designs a suitable <strong>simulation environment</strong>, and breaks the task down into a <strong>multi-level hierarchy of described skills</strong>. Next (<strong>Stage 2</strong>), the LLM translates these skill descriptions into <strong>executable code</strong> that defines the <strong>goals (rewards)</strong> and <strong>completion rules (terminations)</strong> for learning each skill, which can be optionally checked by a human. Finally (<strong>Stage 3</strong>), GenHRL uses this generated environment and code to <strong>automatically train policies</strong> for the entire hierarchy using <strong>reinforcement learning</strong>, resulting in an agent capable of performing the complex task.</p></div></div><div class="text-center"><p class="text-xl text-gray-600 mt-8">Interactive Skill Hierarchy</p></div></header><div class="p-4"><h1 class="text-2xl font-bold mb-4">Obstacle Course<!-- --> Details</h1><div class="ml-4 my-2"><div class="p-2 rounded cursor-pointer bg-gray-200 hover:bg-gray-300 border border-gray-400"><div class="flex justify-between items-center"><span>▼<!-- --> <!-- -->Obstacle Course<!-- --> (Level <!-- -->3<!-- -->)</span></div></div><div class="mt-1 border border-gray-300 rounded bg-gray-50 shadow-sm"><div class="flex border-b border-gray-300 bg-gray-100 rounded-t-md"><button class="px-3 py-1 text-sm rounded-t-md cursor-pointer transition-colors bg-gray-200 font-semibold">Video Demo</button><button class="px-3 py-1 text-sm rounded-t-md cursor-pointer transition-colors bg-gray-100 hover:bg-gray-200">Reward Code</button><button class="px-3 py-1 text-sm rounded-t-md cursor-pointer transition-colors bg-gray-100 hover:bg-gray-200">Success Code</button></div><div class="p-2"><div><p class="text-xs italic text-center p-4">Placeholder for policy video.</p></div></div></div><div class="ml-4 border-l-2 border-gray-300 pl-2"><div class="ml-4 my-2"><div class="p-2 rounded cursor-pointer bg-red-200 hover:bg-red-300 border border-gray-400"><div class="flex justify-between items-center"><span>►<!-- --> <!-- -->JumpOverLowWall<!-- --> (Level <!-- -->2<!-- -->)</span></div></div></div><div class="ml-4 my-2"><div class="p-2 rounded cursor-pointer bg-red-200 hover:bg-red-300 border border-gray-400"><div class="flex justify-between items-center"><span>►<!-- --> <!-- -->PushLargeSphereToHighWall<!-- --> (Level <!-- -->2<!-- -->)</span></div></div></div><div class="ml-4 my-2"><div class="p-2 rounded cursor-pointer bg-red-200 hover:bg-red-300 border border-gray-400"><div class="flex justify-between items-center"><span>►<!-- --> <!-- -->KickSmallSpherePastBlock<!-- --> (Level <!-- -->2<!-- -->)</span></div></div></div><div class="ml-4 my-2"><div class="p-2 rounded cursor-pointer bg-red-200 hover:bg-red-300 border border-gray-400"><div class="flex justify-between items-center"><span>►<!-- --> <!-- -->JumpOntoBlock<!-- --> (Level <!-- -->2<!-- -->)</span></div></div></div></div></div></div><footer class="mt-8 text-center text-gray-500"><p>Level 0 skills are Primitive Actions and are implicitly part of Level 1 skills.</p></footer></main><!--$--><!--/$--><!--$--><!--/$--><script src="/website2/_next/static/chunks/webpack-6e0995e0e540f1e7.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7555,[],\"\"]\n3:I[1295,[],\"\"]\n5:I[3063,[\"63\",\"static/chunks/63-82162ec3eea6ac13.js\",\"974\",\"static/chunks/app/page-a8188a588882025e.js\"],\"Image\"]\n6:I[8077,[\"63\",\"static/chunks/63-82162ec3eea6ac13.js\",\"974\",\"static/chunks/app/page-a8188a588882025e.js\"],\"default\"]\n7:I[9665,[],\"MetadataBoundary\"]\n9:I[9665,[],\"OutletBoundary\"]\nc:I[4911,[],\"AsyncMetadataOutlet\"]\ne:I[9665,[],\"ViewportBoundary\"]\n10:I[6614,[],\"\"]\n:HL[\"/website2/_next/static/media/569ce4b8f30dc480-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/website2/_next/static/media/93f479601ee12b01-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/website2/_next/static/css/b2bb8f1fc8a94a51.css\",\"style\"]\n4:T41e,Defining effective multi-level skill hierarchies and their corresponding learning objectives is a core challenge in robotics and reinforcement learning. Large Language Models (LLMs) offer powerful new capabilities for tackling this challenge through automated generation and reasoning. This paper introduces GenHRL, an LLM-driven framework that automates the pipeline from high-level natural language task descriptions to learned hierarchical skills. GenHRL autonomously generates: (1) task-specific simulation environments, (2) multi-level skill decompositions, and (3) executable code defining intrinsic reward and termination functions for each skill. This automation avoids the need for manual reward engineering, predefined skill sets, offline datasets, and enables end-to-end hierarchical policy learning via standard reinforcement learning algorithms. Empirical evaluations on complex robotic humanoid simulation tasks demonstrate that GenHRL significantly enhances learning efficiency and final performance compared to non-hierarchical baselines."])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"3Z6JTJ6nTwJRa2uomvbiS\",\"p\":\"/website2\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/website2/_next/static/css/b2bb8f1fc8a94a51.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__variable_926cff __variable_470fb6 antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"main\",null,{\"className\":\"container mx-auto p-4\",\"children\":[[\"$\",\"header\",null,{\"className\":\"my-6 text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl font-bold\",\"children\":\"GenHRL: Generative Hierarchical Reinforcement Learning\"}],[\"$\",\"p\",null,{\"className\":\"text-lg text-gray-700 mt-2\",\"children\":\"Authors Anonymous\"}],[\"$\",\"div\",null,{\"className\":\"mt-6 max-w-3xl mx-auto text-left\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-2xl font-semibold mb-2 text-center\",\"children\":\"Abstract\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700 leading-relaxed\",\"children\":\"$4\"}],[\"$\",\"div\",null,{\"className\":\"mt-4 text-center\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://openreview.net/forum?id=vPwAh0eL0D\u0026referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Drobot-learning.org%2FCoRL%2F2025%2FConference%2FAuthors%23your-submissions)\",\"target\":\"https://openreview.net/forum?id=vPwAh0eL0D\u0026referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Drobot-learning.org%2FCoRL%2F2025%2FConference%2FAuthors%23your-submissions)\",\"rel\":\"noopener noreferrer\",\"className\":\"inline-block bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded transition-colors\",\"children\":\"View on OpenReview\"}]}]]}],[\"$\",\"div\",null,{\"className\":\"my-8 max-w-4xl mx-auto\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-2xl font-semibold mb-4 text-center\",\"children\":\"Method Overview\"}],[\"$\",\"div\",null,{\"className\":\"w-full\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/website2/method_drawing.svg\",\"alt\":\"Method Diagram\",\"width\":800,\"height\":600,\"className\":\"h-auto w-full max-w-2xl mx-auto block\"}]}],[\"$\",\"div\",null,{\"className\":\"mt-4 text-left\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold mb-2\",\"children\":\"Method Description:\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700 leading-relaxed\",\"children\":[\"Our \",[\"$\",\"strong\",null,{\"children\":\"GenHRL system\"}],\" uses \",[\"$\",\"strong\",null,{\"children\":\"Large Language Models (LLMs)\"}],\" to \",[\"$\",\"strong\",null,{\"children\":\"automatically build complex robot skills\"}],\" from \",[\"$\",\"strong\",null,{\"children\":\"simple language instructions\"}],\". As shown above, the process starts (\",[\"$\",\"strong\",null,{\"children\":\"Stage 1\"}],\") when a user provides a \",[\"$\",\"strong\",null,{\"children\":\"task description\"}],\". The LLM interprets this, designs a suitable \",[\"$\",\"strong\",null,{\"children\":\"simulation environment\"}],\", and breaks the task down into a \",[\"$\",\"strong\",null,{\"children\":\"multi-level hierarchy of described skills\"}],\". Next (\",[\"$\",\"strong\",null,{\"children\":\"Stage 2\"}],\"), the LLM translates these skill descriptions into \",[\"$\",\"strong\",null,{\"children\":\"executable code\"}],\" that defines the \",[\"$\",\"strong\",null,{\"children\":\"goals (rewards)\"}],\" and \",[\"$\",\"strong\",null,{\"children\":\"completion rules (terminations)\"}],\" for learning each skill, which can be optionally checked by a human. Finally (\",[\"$\",\"strong\",null,{\"children\":\"Stage 3\"}],\"), GenHRL uses this generated environment and code to \",[\"$\",\"strong\",null,{\"children\":\"automatically train policies\"}],\" for the entire hierarchy using \",[\"$\",\"strong\",null,{\"children\":\"reinforcement learning\"}],\", resulting in an agent capable of performing the complex task.\"]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"text-center\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-xl text-gray-600 mt-8\",\"children\":\"Interactive Skill Hierarchy\"}]}]]}],[\"$\",\"$L6\",null,{}],[\"$\",\"footer\",null,{\"className\":\"mt-8 text-center text-gray-500\",\"children\":[\"$\",\"p\",null,{\"children\":\"Level 0 skills are Primitive Actions and are implicitly part of Level 1 skills.\"}]}]]}],[\"$\",\"$L7\",null,{\"children\":\"$L8\"}],null,[\"$\",\"$L9\",null,{\"children\":[\"$La\",\"$Lb\",[\"$\",\"$Lc\",null,{\"promise\":\"$@d\"}]]}]]}],{},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"U3ux0E2pcM2HqHoOpqOTd\",{\"children\":[[\"$\",\"$Le\",null,{\"children\":\"$Lf\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$10\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"11:\"$Sreact.suspense\"\n12:I[4911,[],\"AsyncMetadata\"]\n8:[\"$\",\"$11\",null,{\"fallback\":null,\"children\":[\"$\",\"$L12\",null,{\"promise\":\"$@13\"}]}]\n"])</script><script>self.__next_f.push([1,"b:null\n"])</script><script>self.__next_f.push([1,"f:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\na:null\n"])</script><script>self.__next_f.push([1,"13:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"GenHRL\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Generative Hierarchical Reinforcement Learning\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/website2/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}]],\"error\":null,\"digest\":\"$undefined\"}\nd:{\"metadata\":\"$13:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>